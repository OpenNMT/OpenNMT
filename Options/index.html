<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Options - OpenNMT</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Options";
    var mkdocs_page_input_path = "Options.md";
    var mkdocs_page_url = "/Options/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> OpenNMT</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Options</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#preprocess">Preprocess</a></li>
                
            
                <li class="toctree-l3"><a href="#train">Train</a></li>
                
            
                <li class="toctree-l3"><a href="#evaluate">Evaluate</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Code</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../code/eval/">Eval</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../code/onmt/">Onmt</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../code/train/">Train</a>
        
    </li>

        
    </ul>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">OpenNMT</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Options</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p><a name="preprocess"></a></p>
<h2 id="preprocess">Preprocess</h2>
<p><strong>Preprocess Options</strong></p>
<dl>
<dt>config</dt>
<dd>Read options from this file []</dd>
<dt>train_src_file</dt>
<dd>Path to the training source data []</dd>
<dt>train_targ_file</dt>
<dd>Path to the training target data []</dd>
<dt>valid_src_file</dt>
<dd>Path to the validation source data []</dd>
<dt>valid_targ_file</dt>
<dd>Path to the validation target data []</dd>
<dt>output_file</dt>
<dd>Output file for the prepared data []</dd>
<dt>src_vocab_size</dt>
<dd>Size of the source vocabulary [50000]</dd>
<dt>targ_vocab_size</dt>
<dd>Size of the target vocabulary [50000]</dd>
<dt>src_vocab_file</dt>
<dd>Path to an existing source vocabulary []</dd>
<dt>targ_vocab_file</dt>
<dd>Path to an existing target vocabulary []</dd>
<dt>features_vocabs_prefix</dt>
<dd>Path prefix to existing features vocabularies []</dd>
<dt>seq_length</dt>
<dd>Maximum sequence length [50]</dd>
<dt>shuffle</dt>
<dd>Suffle data [1]</dd>
<dt>seed</dt>
<dd>Random seed [3435]</dd>
</dl>
<p><a name="train"></a></p>
<h2 id="train">Train</h2>
<p><strong>train.lua</strong></p>
<dl>
<dt>config</dt>
<dd>Read options from this file []</dd>
</dl>
<p><strong>Data options</strong></p>
<dl>
<dt>data</dt>
<dd>Path to the training *-train.t7 file from preprocess.lua []</dd>
<dt>save_file</dt>
<dd>Savefile name (model will be saved assavefile_epochX_PPL.t7 where X is the X-th epoch and PPL isthe validation perplexity []</dd>
<dt>train_from</dt>
<dd>If training from a checkpoint then this is the path to the pretrained model. []</dd>
<dt>continue</dt>
<dd>If training from a checkpoint, whether to continue the training in the same configuration or not. [false]</dd>
</dl>
<p><strong>Model options</strong></p>
<dl>
<dt>num_layers</dt>
<dd>Number of layers in the LSTM encoder/decoder [2]</dd>
<dt>rnn_size</dt>
<dd>Size of LSTM hidden states [500]</dd>
<dt>word_vec_size</dt>
<dd>Word embedding sizes [500]</dd>
<dt>feat_vec_exponent</dt>
<dd>If the feature takes N values, then theembedding dimension will be set to N^exponent [0.7]</dd>
<dt>input_feed</dt>
<dd>Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder. [1]</dd>
<dt>brnn</dt>
<dd>Use a bidirectional encoder [false]</dd>
<dt>brnn_merge</dt>
<dd>Merge action for the bidirectional hidden states: concat or sum [sum]</dd>
</dl>
<p><strong>Optimization options</strong></p>
<dl>
<dt>max_batch_size</dt>
<dd>Maximum batch size [64]</dd>
<dt>epochs</dt>
<dd>Number of training epochs [13]</dd>
<dt>start_epoch</dt>
<dd>If loading from a checkpoint, the epoch from which to start [1]</dd>
<dt>start_iteration</dt>
<dd>If loading from a checkpoint, the iteration from which to start [1]</dd>
<dt>param_init</dt>
<dd>Parameters are initialized over uniform distribution with support (-param_init, param_init) [0.1]</dd>
<dt>optim</dt>
<dd>Optimization method. Possible options are: sgd, adagrad, adadelta, adam [sgd]</dd>
<dt>learning_rate</dt>
<dd>Starting learning rate. If adagrad/adadelta/adam is used,then this is the global learning rate. Recommended settings: sgd =1,adagrad = 0.1, adadelta = 1, adam = 0.1 [1]</dd>
<dt>max_grad_norm</dt>
<dd>If the norm of the gradient vector exceeds this renormalize it to have the norm equal to max_grad_norm [5]</dd>
<dt>dropout</dt>
<dd>Dropout probability. Dropout is applied between vertical LSTM stacks. [0.3]</dd>
<dt>lr_decay</dt>
<dd>Decay learning rate by this much if (i) perplexity does not decreaseon the validation set or (ii) epoch has gone past the start_decay_at_limit [0.5]</dd>
<dt>start_decay_at</dt>
<dd>Start decay after this epoch [9]</dd>
<dt>curriculum</dt>
<dd>For this many epochs, order the minibatches based on sourcesequence length. Sometimes setting this to 1 will increase convergence speed. [0]</dd>
<dt>pre_word_vecs_enc</dt>
<dd>If a valid path is specified, then this will loadpretrained word embeddings on the encoder side.See README for specific formatting instructions. []</dd>
<dt>pre_word_vecs_dec</dt>
<dd>If a valid path is specified, then this will loadpretrained word embeddings on the decoder side.See README for specific formatting instructions. []</dd>
<dt>fix_word_vecs_enc</dt>
<dd>Fix word embeddings on the encoder side [false]</dd>
<dt>fix_word_vecs_dec</dt>
<dd>Fix word embeddings on the decoder side [false]</dd>
</dl>
<p><strong>Other options</strong></p>
<dl>
<dt>gpuid</dt>
<dd>Which gpu to use (1-indexed). &lt; 1 = use CPU [-1]</dd>
<dt>nparallel</dt>
<dd>When using GPUs, how many batches to execute in parallel.Note: this will technically change the final batch size to max_batch_size*nparallel. [1]</dd>
<dt>disable_mem_optimization</dt>
<dd>Disable sharing internal of internal buffers between clones - which is in general safe,except if you want to look inside clones for visualization purpose for instance. [false]</dd>
<dt>save_every</dt>
<dd>Save intermediate models every this many iterations within an epoch.If = 0, will not save models within an epoch.  [0]</dd>
<dt>print_every</dt>
<dd>Print stats every this many iterations within an epoch. [50]</dd>
</dl>
<p><a name="evaluate"></a></p>
<h2 id="evaluate">Evaluate</h2>
<p><strong>evaluate.lua</strong></p>
<dl>
<dt>config</dt>
<dd>Read options from this file []</dd>
</dl>
<p><strong>Data options</strong></p>
<dl>
<dt>model</dt>
<dd>Path to model .t7 file []</dd>
<dt>src_file</dt>
<dd>Source sequence to decode (one line per sequence) []</dd>
<dt>targ_file</dt>
<dd>True target sequence (optional) []</dd>
<dt>output_file</dt>
<dd>Path to output the predictions (each line will be the decoded sequence [pred.txt]</dd>
</dl>
<p><strong>Beam Search options</strong></p>
<dl>
<dt>beam</dt>
<dd>Beam size [5]</dd>
<dt>batch</dt>
<dd>Batch size [30]</dd>
<dt>max_sent_l</dt>
<dd>Maximum sentence length. If any sequences in srcfile are longer than this then it will error out [250]</dd>
<dt>replace_unk</dt>
<dd>Replace the generated UNK tokens with the source token thathad the highest attention weight. If srctarg_dict is provided,it will lookup the identified source token and give the correspondingtarget token. If it is not provided (or the identified source tokendoes not exist in the table) then it will copy the source token [false]</dd>
<dt>srctarg_dict</dt>
<dd>Path to source-target dictionary to replace UNKtokens. See README.md for the format this file should be in []</dd>
<dt>n_best</dt>
<dd>If &gt; 1, it will also output an n_best list of decoded sentences [1]</dd>
</dl>
<p><strong>Other options</strong></p>
<dl>
<dt>gpuid</dt>
<dd>ID of the GPU to use (-1 = use CPU, 0 = let cuda choose between available GPUs) [-1]</dd>
<dt>fallback_to_cpu</dt>
<dd>If = true, fallback to CPU if no GPU available [false]</dd>
</dl>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../code/eval/" class="btn btn-neutral float-right" title="Eval">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href=".." class="btn btn-neutral" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href=".." style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../code/eval/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>
      <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
