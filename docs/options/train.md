<!--- This file was automatically generated. Do not modify it manually but use the docs/options/generate.sh script instead. -->

`train.lua` options:

* `-h`<br/>This help.
* `-md`<br/>Dump help in Markdown format.
* `-config <string>`<br/>Load options from this file.
* `-save_config <string>`<br/>Save options to this file.

## Data options

* `-data <string>`<br/>Path to the data package `*-train.t7` generated by the preprocessing step.
* `-save_model <string>`<br/>Model filename (the model will be saved as `<save_model>_epochN_PPL.t7` where `PPL` is the validation perplexity.

## Sampled dataset options

* `-sample <number>` (default: `0`)<br/>Number of instances to sample from train data in each epoch.
* `-sample_w_ppl`<br/>If set, ese perplexity as a probability distribution when sampling.
* `-sample_w_ppl_init <number>` (default: `15`)<br/>Start perplexity-based sampling when average train perplexity per batch falls below this value.
* `-sample_w_ppl_max <number>` (default: `-1.5`)<br/>When greater than 0, instances with perplexity above this value will be considered as noise and ignored; when less than 0, mode + `-sample_w_ppl_max` * stdev will be used as threshold.

## Model options

* `-model_type <string>` (accepted: `lm`, `seq2seq`, `seqtagger`; default: `seq2seq`)<br/>Type of model to train. This option impacts all options choices.
* `-param_init <number>` (default: `0.1`)<br/>Parameters are initialized over uniform distribution with support (-`param_init`, `param_init`).

## Sequence to Sequence with Attention options

* `-word_vec_size <number>` (default: `0`)<br/>Shared word embedding size. If set, this overrides `-src_word_vec_size` and `-tgt_word_vec_size`.
* `-src_word_vec_size <string>` (default: `500`)<br/>Comma-separated list of source embedding sizes: `word[,feat1[,feat2[,...] ] ]`.
* `-tgt_word_vec_size <string>` (default: `500`)<br/>Comma-separated list of target embedding sizes: `word[,feat1[,feat2[,...] ] ]`.
* `-pre_word_vecs_enc <string>`<br/>Path to pretrained word embeddings on the encoder side serialized as a Torch tensor.
* `-pre_word_vecs_dec <string>`<br/>Path to pretrained word embeddings on the decoder side serialized as a Torch tensor.
* `-fix_word_vecs_enc <number>` (accepted: `0`, `1`; default: `0`)<br/>Fix word embeddings on the encoder side.
* `-fix_word_vecs_dec <number>` (accepted: `0`, `1`; default: `0`)<br/>Fix word embeddings on the decoder side.
* `-feat_merge <string>` (accepted: `concat`, `sum`; default: `concat`)<br/>Merge action for the features embeddings.
* `-feat_vec_exponent <number>` (default: `0.7`)<br/>When features embedding sizes are not set and using `-feat_merge concat`, their dimension will be set to `N^feat_vec_exponent` where `N` is the number of values the feature takes.
* `-feat_vec_size <number>` (default: `20`)<br/>When features embedding sizes are not set and using `-feat_merge sum`, this is the common embedding size of the features
* `-input_feed <number>` (accepted: `0`, `1`; default: `1`)<br/>Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.
* `-layers <number>` (default: `2`)<br/>Number of recurrent layers of the encoder and decoder.
* `-rnn_size <number>` (default: `500`)<br/>Hidden size of the recurrent unit.
* `-rnn_type <string>` (accepted: `LSTM`, `GRU`; default: `LSTM`)<br/>Type of recurrent cell.
* `-dropout <number>` (default: `0.3`)<br/>Dropout probability applied between recurrent layers.
* `-dropout_input`<br/>Also apply dropout to the input of the recurrent module.
* `-residual`<br/>Add residual connections between recurrent layers.
* `-brnn`<br/>Use a bidirectional encoder.
* `-dbrnn`<br/>Use a deep bidirectional encoder.
* `-pdbrnn`<br/>Use a pyramidal deep bidirectional encoder.
* `-attention <string>` (accepted: `none`, `global`; default: `global`)<br/>Attention model.
* `-brnn_merge <string>` (accepted: `concat`, `sum`; default: `sum`)<br/>Merge action for the bidirectional states.
* `-pdbrnn_reduction <number>` (default: `2`)<br/>Time-reduction factor at each layer.

## Global Attention Model options

* `-global_attention <string>` (accepted: `general`, `dot`, `concat`; default: `general`)<br/>Global attention model type.

## Optimization options

* `-max_batch_size <number>` (default: `64`)<br/>Maximum batch size.
* `-uneven_batches`<br/>If set, batches are filled up to max_batch_size even if source lengths are different. Slower but needed for some tasks.
* `-optim <string>` (accepted: `sgd`, `adagrad`, `adadelta`, `adam`; default: `sgd`)<br/>Optimization method.
* `-learning_rate <number>` (default: `1`)<br/>Starting learning rate. If adagrad or adam is used, then this is the global learning rate. Recommended settings are: sgd = 1, adagrad = 0.1, adam = 0.0002.
* `-min_learning_rate <number>` (default: `0`)<br/>Do not continue the training past this learning rate value.
* `-max_grad_norm <number>` (default: `5`)<br/>Clip the gradients norm to this value.
* `-learning_rate_decay <number>` (default: `0.7`)<br/>Learning rate decay factor: `learning_rate = learning_rate * learning_rate_decay`.
* `-start_decay_at <number>` (default: `9`)<br/>In "default" decay mode, start decay after this epoch.
* `-start_decay_ppl_delta <number>` (default: `0`)<br/>Start decay when validation perplexity improvement is lower than this value.
* `-decay <string>` (accepted: `default`, `perplexity_only`; default: `default`)<br/>When to apply learning rate decay. `default`: decay after each epoch past `-start_decay_at` or as soon as the validation perplexity is not improving more than `-start_decay_ppl_delta`, `perplexity_only`: only decay when validation perplexity is not improving more than `-start_decay_ppl_delta`.

## Trainer options

* `-save_every <number>` (default: `5000`)<br/>Save intermediate models every this many iterations within an epoch. If = 0, will not save intermediate models.
* `-report_every <number>` (default: `50`)<br/>Report progress every this many iterations within an epoch.
* `-async_parallel`<br/>When training on multiple GPUs, update parameters asynchronously.
* `-async_parallel_minbatch <number>` (default: `1000`)<br/>In asynchronous training, minimal number of sequential batches before being parallel.
* `-start_iteration <number>` (default: `1`)<br/>If loading from a checkpoint, the iteration from which to start.
* `-start_epoch <number>` (default: `1`)<br/>If loading from a checkpoint, the epoch from which to start.
* `-end_epoch <number>` (default: `13`)<br/>The final epoch of the training.
* `-curriculum <number>` (default: `0`)<br/>For this many epochs, order the minibatches based on source length (from smaller to longer). Sometimes setting this to 1 will increase convergence speed.

## Checkpoint options

* `-train_from <string>`<br/>Path to a checkpoint.
* `-continue`<br/>If set, continue the training where it left off.

## Crayon options

* `-exp_host <string>` (default: `127.0.0.1`)<br/>Crayon server IP.
* `-exp_port <string>` (default: `8889`)<br/>Crayon server port.
* `-exp <string>`<br/>Crayon experiment name.

## Cuda options

* `-gpuid <string>` (default: `0`)<br/>List of comma-separated GPU identifiers (1-indexed). CPU is used when set to 0.
* `-fallback_to_cpu`<br/>If GPU can't be used, rollback on the CPU.
* `-fp16`<br/>Use half-precision float on GPU.
* `-no_nccl`<br/>Disable usage of nccl in parallel mode.

## Logger options

* `-log_file <string>`<br/>Output logs to a file under this path instead of stdout.
* `-disable_logs`<br/>If set, output nothing.
* `-log_level <string>` (accepted: `DEBUG`, `INFO`, `WARNING`, `ERROR`; default: `INFO`)<br/>Output logs at this level and above.

## Other options

* `-disable_mem_optimization`<br/>Disable sharing of internal buffers between clones for visualization or development.
* `-profiler`<br/>Generate profiling logs.
* `-seed <number>` (default: `3435`)<br/>Random seed.

