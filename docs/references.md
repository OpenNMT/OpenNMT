This is the list of papers, OpenNMT has been inspired on:

* <a name="Luong2015"></a>Luong, M. T., Pham, H., & Manning, C. D. (2015). [Effective approaches to attention-based neural machine translation](https://arxiv.org/abs/1508.04025). arXiv preprint arXiv:1508.04025.
* <a name="Senrich2016-1"></a>Sennrich, R., & Haddow, B. (2016). [Linguistic input features improve neural machine translation](https://arxiv.org/abs/1606.02892). arXiv preprint arXiv:1606.02892.
* <a name="Senrich2016-2"></a>Sennrich, R., Haddow, B., & Birch, A. (2015). [Neural machine translation of rare words with subword units](https://arxiv.org/abs/1508.07909). arXiv preprint arXiv:1508.07909.
* <a name="GNMT"></a>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... & Klingner, J. (2016). [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144). arXiv preprint arXiv:1609.08144.
* <a name="Jean2015"></a>Jean, S., Cho, K., Memisevic, R., Bengio, Y. (2015). [On Using Very Large Target Vocabulary for Neural Machine Translation](http://www.aclweb.org/anthology/P15-1001). ACL 2015
* <a name="CNNEncoder"></a>Gehring, J., Auli, M., Grangier D., Dauphin Y. N. (2017). [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344). arXiv preprint arXiv:1611.02344.
* <a name=""></a>Bengio, S., Vinyals, O., Jaitly, N., & Shazeer, N. (2015). [Scheduled sampling for sequence prediction with recurrent neural networks](http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf). In Advances in Neural Information Processing Systems (pp. 1171-1179).
* <a name="LMShallowFusion"></a>Gulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H. C., ... & Bengio, Y. (2015). [On using monolingual corpora in neural machine translation](http://arxiv.org/abs/1503.03535). arXiv preprint arXiv:1503.03535.
