{
    "docs": [
        {
            "location": "/",
            "text": "",
            "title": "Home"
        },
        {
            "location": "/nmt/LICENSE/",
            "text": "The MIT License (MIT)\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.",
            "title": "LICENSE"
        },
        {
            "location": "/nmt/README/",
            "text": "Highlight.js\n\n\nHighlight.js highlights syntax in code examples on blogs, forums and,\nin fact, on any web page. It's very easy to use because it works\nautomatically: finds blocks of code, detects a language, highlights it.\n\n\nAutodetection can be fine tuned when it fails by itself (see \"Heuristics\").\n\n\n\n\n\n\nBasic usage\n\n\nLink the library and a stylesheet from your page and hook highlighting to\nthe page load event:\n\n\nlink rel=\nstylesheet\n href=\nstyles/default.css\n\n\nscript src=\nhighlight.pack.js\n/script\n\n\nscript\nhljs.initHighlightingOnLoad();\n/script\n\n\n\n\n\nThis will highlight all code on the page marked up as \npre\ncode\n .. \n/code\n/pre\n.\nIf you use different markup or need to apply highlighting dynamically, read\n\"Custom initialization\" below.\n\n\n\n\n\n\nYou can download your own customized version of \"highlight.pack.js\" or\n  use the hosted one as described on the download page:\n  \nhttp://highlightjs.org/download/\n\n\n\n\n\n\nStyle themes are available in the download package or as hosted files.\n  To create a custom style for your site see the class reference in the file\n  \nclassref.txt\n from the downloaded package.\n\n\n\n\n\n\n\n\n\n\nnode.js\n\n\nHighlight.js can be used under node.js. The package with all supported languages is\ninstallable from NPM:\n\n\nnpm install highlight.js\n\n\n\nAlternatively, you can build it from the source with only languages you need:\n\n\npython3 tools/build.py -tnode lang1 lang2 ..\n\n\n\nUsing the library:\n\n\nvar hljs = require('highlight.js');\n\n// If you know the language\nhljs.highlight(lang, code).value;\n\n// Automatic language detection\nhljs.highlightAuto(code).value;\n\n\n\n\n\n\n\n\nAMD\n\n\nHighlight.js can be used with an AMD loader.  You will need to build it from\nsource in order to do so:\n\n\n$ python3 tools/build.py -tamd lang1 lang2 ..\n\n\n\n\nWhich will generate a \nbuild/highlight.pack.js\n which will load as an AMD\nmodule with support for the built languages and can be used like so:\n\n\nrequire([\nhighlight.js/build/highlight.pack\n], function(hljs){\n\n  // If you know the language\n  hljs.highlight(lang, code).value;\n\n  // Automatic language detection\n  hljs.highlightAuto(code).value;\n});\n\n\n\n\n\n\n\n\nTab replacement\n\n\nYou can replace TAB ('\\x09') characters used for indentation in your code\nwith some fixed number of spaces or with a \nspan\n to give them special\nstyling:\n\n\nscript type=\ntext/javascript\n\n  hljs.tabReplace = '    '; // 4 spaces\n  // ... or\n  hljs.tabReplace = '\nspan class=\nindent\n\\t\n/span\n';\n\n  hljs.initHighlightingOnLoad();\n\n/script\n\n\n\n\n\n\n\n\n\nCustom initialization\n\n\nIf you use different markup for code blocks you can initialize them manually\nwith \nhighlightBlock(code, tabReplace, useBR)\n function. It takes a DOM element\ncontaining the code to highlight and optionally a string with which to replace\nTAB characters.\n\n\nInitialization using, for example, jQuery might look like this:\n\n\n$(document).ready(function() {\n  $('pre code').each(function(i, e) {hljs.highlightBlock(e)});\n});\n\n\n\n\nYou can use \nhighlightBlock\n to highlight blocks dynamically inserted into\nthe page. Just make sure you don't do it twice for already highlighted\nblocks.\n\n\nIf your code container relies on \nbr\n tags instead of line breaks (i.e. if\nit's not \npre\n) pass \ntrue\n into the third parameter of \nhighlightBlock\n\nto make highlight.js use \nbr\n in the output:\n\n\n$('div.code').each(function(i, e) {hljs.highlightBlock(e, null, true)});\n\n\n\n\n\n\n\n\nHeuristics\n\n\nAutodetection of a code's language is done using a simple heuristic:\nthe program tries to highlight a fragment with all available languages and\ncounts all syntactic structures that it finds along the way. The language\nwith greatest count wins.\n\n\nThis means that in short fragments the probability of an error is high\n(and it really happens sometimes). In this cases you can set the fragment's\nlanguage explicitly by assigning a class to the \ncode\n element:\n\n\npre\ncode class=\nhtml\n...\n/code\n/pre\n\n\n\n\n\nYou can use class names recommended in HTML5: \"language-html\",\n\"language-php\". Classes also can be assigned to the \npre\n element.\n\n\nTo disable highlighting of a fragment altogether use \"no-highlight\" class:\n\n\npre\ncode class=\nno-highlight\n...\n/code\n/pre\n\n\n\n\n\n\n\n\n\nExport\n\n\nFile export.html contains a little program that allows you to paste in a code\nsnippet and then copy and paste the resulting HTML code generated by the\nhighlighter. This is useful in situations when you can't use the script itself\non a site.\n\n\n\n\n\n\nMeta\n\n\n\n\nVersion: 7.5\n\n\nURL:     http://highlightjs.org/\n\n\n\n\nFor the license terms see LICENSE files.\nFor authors and contributors see AUTHORS.en.txt file.",
            "title": "README"
        },
        {
            "location": "/nmt/README/#highlightjs",
            "text": "Highlight.js highlights syntax in code examples on blogs, forums and,\nin fact, on any web page. It's very easy to use because it works\nautomatically: finds blocks of code, detects a language, highlights it.  Autodetection can be fine tuned when it fails by itself (see \"Heuristics\").",
            "title": "Highlight.js"
        },
        {
            "location": "/nmt/README/#basic-usage",
            "text": "Link the library and a stylesheet from your page and hook highlighting to\nthe page load event:  link rel= stylesheet  href= styles/default.css  script src= highlight.pack.js /script  script hljs.initHighlightingOnLoad(); /script   This will highlight all code on the page marked up as  pre code  ..  /code /pre .\nIf you use different markup or need to apply highlighting dynamically, read\n\"Custom initialization\" below.    You can download your own customized version of \"highlight.pack.js\" or\n  use the hosted one as described on the download page:\n   http://highlightjs.org/download/    Style themes are available in the download package or as hosted files.\n  To create a custom style for your site see the class reference in the file\n   classref.txt  from the downloaded package.",
            "title": "Basic usage"
        },
        {
            "location": "/nmt/README/#nodejs",
            "text": "Highlight.js can be used under node.js. The package with all supported languages is\ninstallable from NPM:  npm install highlight.js  Alternatively, you can build it from the source with only languages you need:  python3 tools/build.py -tnode lang1 lang2 ..  Using the library:  var hljs = require('highlight.js');\n\n// If you know the language\nhljs.highlight(lang, code).value;\n\n// Automatic language detection\nhljs.highlightAuto(code).value;",
            "title": "node.js"
        },
        {
            "location": "/nmt/README/#amd",
            "text": "Highlight.js can be used with an AMD loader.  You will need to build it from\nsource in order to do so:  $ python3 tools/build.py -tamd lang1 lang2 ..  Which will generate a  build/highlight.pack.js  which will load as an AMD\nmodule with support for the built languages and can be used like so:  require([ highlight.js/build/highlight.pack ], function(hljs){\n\n  // If you know the language\n  hljs.highlight(lang, code).value;\n\n  // Automatic language detection\n  hljs.highlightAuto(code).value;\n});",
            "title": "AMD"
        },
        {
            "location": "/nmt/README/#tab-replacement",
            "text": "You can replace TAB ('\\x09') characters used for indentation in your code\nwith some fixed number of spaces or with a  span  to give them special\nstyling:  script type= text/javascript \n  hljs.tabReplace = '    '; // 4 spaces\n  // ... or\n  hljs.tabReplace = ' span class= indent \\t /span ';\n\n  hljs.initHighlightingOnLoad(); /script",
            "title": "Tab replacement"
        },
        {
            "location": "/nmt/README/#custom-initialization",
            "text": "If you use different markup for code blocks you can initialize them manually\nwith  highlightBlock(code, tabReplace, useBR)  function. It takes a DOM element\ncontaining the code to highlight and optionally a string with which to replace\nTAB characters.  Initialization using, for example, jQuery might look like this:  $(document).ready(function() {\n  $('pre code').each(function(i, e) {hljs.highlightBlock(e)});\n});  You can use  highlightBlock  to highlight blocks dynamically inserted into\nthe page. Just make sure you don't do it twice for already highlighted\nblocks.  If your code container relies on  br  tags instead of line breaks (i.e. if\nit's not  pre ) pass  true  into the third parameter of  highlightBlock \nto make highlight.js use  br  in the output:  $('div.code').each(function(i, e) {hljs.highlightBlock(e, null, true)});",
            "title": "Custom initialization"
        },
        {
            "location": "/nmt/README/#heuristics",
            "text": "Autodetection of a code's language is done using a simple heuristic:\nthe program tries to highlight a fragment with all available languages and\ncounts all syntactic structures that it finds along the way. The language\nwith greatest count wins.  This means that in short fragments the probability of an error is high\n(and it really happens sometimes). In this cases you can set the fragment's\nlanguage explicitly by assigning a class to the  code  element:  pre code class= html ... /code /pre   You can use class names recommended in HTML5: \"language-html\",\n\"language-php\". Classes also can be assigned to the  pre  element.  To disable highlighting of a fragment altogether use \"no-highlight\" class:  pre code class= no-highlight ... /code /pre",
            "title": "Heuristics"
        },
        {
            "location": "/nmt/README/#export",
            "text": "File export.html contains a little program that allows you to paste in a code\nsnippet and then copy and paste the resulting HTML code generated by the\nhighlighter. This is useful in situations when you can't use the script itself\non a site.",
            "title": "Export"
        },
        {
            "location": "/nmt/README/#meta",
            "text": "Version: 7.5  URL:     http://highlightjs.org/   For the license terms see LICENSE files.\nFor authors and contributors see AUTHORS.en.txt file.",
            "title": "Meta"
        },
        {
            "location": "/nmt/STYLE/",
            "text": "Style Guide\n\n\n\n\n\n\nComments\n\n\n\n\n\n\nComments should follow:\nhttps://github.com/deepmind/torch-dokx/blob/master/doc/usage.md\n\n\n\n\n\n\nAll non-private method should have dokx comments describing input/output.  \n\n\n\n\n\n\nAll classes should have a class docstring at the top of the file. \n\n\n\n\n\n\nAll comments should be on their own line, and be a complete English\nsentence with capitalization.\n\n\n\n\n\n\nUse torch-dokx and this command to build docs \n\n\n\n\ndokx-build-package-docs -o docs .\ngoogle-chrome doc/index.html\n\n\n\n\n\n\n\n\n\n\n\n\nStyle:\n\n\n\n\nPlease run and correct all warnings from luacheck before sending a pull request. \n\n\n\n\n\n\nluacheck *\n\n\n\n\n\n\nAll indentation should be 2 spaces.",
            "title": "STYLE"
        },
        {
            "location": "/nmt/STYLE/#style-guide",
            "text": "",
            "title": "Style Guide"
        },
        {
            "location": "/nmt/STYLE/#comments",
            "text": "Comments should follow:\nhttps://github.com/deepmind/torch-dokx/blob/master/doc/usage.md    All non-private method should have dokx comments describing input/output.      All classes should have a class docstring at the top of the file.     All comments should be on their own line, and be a complete English\nsentence with capitalization.    Use torch-dokx and this command to build docs    dokx-build-package-docs -o docs .\ngoogle-chrome doc/index.html",
            "title": "Comments"
        },
        {
            "location": "/nmt/STYLE/#style",
            "text": "Please run and correct all warnings from luacheck before sending a pull request.     luacheck *    All indentation should be 2 spaces.",
            "title": "Style:"
        },
        {
            "location": "/nmt/evaluate/",
            "text": "",
            "title": "Evaluate"
        },
        {
            "location": "/nmt/",
            "text": "",
            "title": "Home"
        },
        {
            "location": "/nmt/init/",
            "text": "Style Guide\n\n\n\n\n\n\nComments\n\n\n\n\n\n\nComments should follow:\nhttps://github.com/deepmind/torch-dokx/blob/master/doc/usage.md\n\n\n\n\n\n\nAll non-private method should have dokx comments describing input/output.  \n\n\n\n\n\n\nAll classes should have a class docstring at the top of the file. \n\n\n\n\n\n\nAll comments should be on their own line, and be a complete English\nsentence with capitalization.\n\n\n\n\n\n\nUse torch-dokx and this command to build docs \n\n\n\n\ndokx-build-package-docs -o docs .\ngoogle-chrome doc/index.html\n\n\n\n\n\n\n\n\n\n\n\n\nStyle:\n\n\n\n\nPlease run and correct all warnings from luacheck before sending a pull request. \n\n\n\n\n\n\nluacheck *\n\n\n\n\n\n\nAll indentation should be 2 spaces.\nThe MIT License (MIT)\n\n\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n\n\n\n\nSequence-to-Sequence Learning with Attentional Neural Networks\n\n\nTorch\n implementation of a standard sequence-to-sequence model with (optional)\nattention where the encoder-decoder are LSTMs. Encoder can be a bidirectional LSTM.\nAdditionally has the option to use characters\n(instead of input word embeddings) by running a convolutional neural network followed by a\n\nhighway network\n over character embeddings to use as inputs.\n\n\nThe attention model is from\n\nEffective Approaches to Attention-based\nNeural Machine Translation\n,\nLuong et al. EMNLP 2015. We use the \nglobal-general-attention\n model with the\n\ninput-feeding\n approach from the paper. Input-feeding is optional and can be turned off.\n\n\nThe character model is from \nCharacter-Aware Neural\nLanguage Models\n, Kim et al. AAAI 2016.\n\n\nThere are a lot of additional options on top of the baseline model, mainly thanks to the fantastic folks \nat \nSYSTRAN\n. Specifically, there are functionalities which implement:\n\n \nEffective Approaches to Attention-based Neural Machine Translation\n. Luong et al., EMNLP 2015.\n\n \nCharacter-based Neural Machine Translation\n. Costa-Jussa and Fonollosa, ACL 2016.\n\n \nCompression of Neural Machine Translation Models via Pruning\n. See et al., COLING 2016.\n\n \nSequence-Level Knowledge Distillation\n. Kim and Rush., EMNLP 2016.\n\n \nDeep Recurrent Models with Fast Forward Connections for Neural Machine Translation\n.\nZhou et al, TACL 2016.\n\n \nGuided Alignment Training for Topic-Aware Neural Machine Translation\n. Chen et al., arXiv:1607.01628.\n* \nLinguistic Input Features Improve Neural Machine Translation\n. Senrich et al., arXiv:1606.02892\n\n\nSee below for more details on how to use them.\n\n\nThis project is maintained by \nYoon Kim\n.\nFeel free to post any questions/issues on the issues page.\n\n\n\n\n\n\nDependencies\n\n\n\n\n\n\nLua\n\n\nYou will need the following packages:\n\n nn\n\n nngraph\n\n\nGPU usage will additionally require:\n\n cutorch\n\n cunn\n\n\nIf running the character model, you should also install:\n\n cudnn\n\n luautf8\n\n\n\n\n\n\nQuickstart\n\n\nWe are going to be working with some example data in \ndata/\n folder.\nFirst run the data-processing code\n\n\nth preprocess.lua -train_src_file data/src-train.txt -train_targ_file data/targ-train.txt\n    -valid_src_file data/src-val.txt -valid_targ_file data/targ-val.txt -output_file data/demo\n\n\n\n\nThis will take the source/target train/valid files (\nsrc-train.txt, targ-train.txt,\nsrc-val.txt, targ-val.txt\n) and build the following files:\n\n\n\n\ndemo.src.dict\n: Dictionary of source vocab to index mappings.\n\n\ndemo.targ.dict\n: Dictionary of target vocab to index mappings.\n\n\ndemo-train.t7\n: serialized torch file containing vocabulary, training and validation data\n\n\n\n\nThe \n*.dict\n files are needed to check vocabulary, or to preprocess data with fixed vocabularies\n\n\nNow run the model\n\n\nth train.lua -data_file data/demo-train.t7 -savefile demo-model\n\n\n\n\nThis will run the default model, which consists of a 2-layer LSTM with 500 hidden units\non both the encoder/decoder.\nYou can also add \n-gpuid 1\n to use (say) GPU 1 in the cluster.\n\n\nNow you have a model which you can use to predict on new data. To do this we are\ngoing to be running beam search\n\n\nth evaluate.lua -model demo-model_final.t7 -src_file data/src-val.txt -output_file pred.txt\n-src_dict data/demo.src.dict -targ_dict data/demo.targ.dict\n\n\n\n\nThis will output predictions into \npred.txt\n. The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for \ntranslation\n\nor \nsummarization\n.\n\n\n\n\n\n\nDetails\n\n\n\n\n\n\nPreprocessing options (\npreprocess.py\n)\n\n\n\n\nsrcvocabsize, targetvocabsize\n: Size of source/target vocabularies. This is constructed\nby taking the top X most frequent words. Rest are replaced with special UNK tokens.\n\n\nsrcfile, targetfile\n: Path to source/target training data, where each line represents a single\nsource/target sequence.\n\n\nsrcvalfile, targetvalfile\n: Path to source/target validation data.\n\n\nbatchsize\n: Size of each mini-batch.\n\n\nseqlength\n: Maximum sequence length (sequences longer than this are dropped).\n\n\noutputfile\n: Prefix of the output file names.\n\n\nmaxwordlength\n: For the character models, words are truncated (if longer than maxwordlength)\nor zero-padded (if shorter) to \nmaxwordlength\n.\n\n\nchars\n: If 1, construct the character-level dataset as well.  This might take up a lot of space\ndepending on your data size, so you may want to break up the training data into different shards.\n\n\nsrcvocabfile, targetvocabfile\n: If working with a preset vocab, then including these paths\nwill ignore the \nsrcvocabsize,targetvocabsize\n.\n\n\nunkfilter\n: Ignore sentences with too many UNK tokens. Can be an absolute count limit (if \n 1)\nor a proportional limit (0 \n unkfilter \n 1).\n\n\nshuffle\n: Shuffle sentences.\n\n\nalignfile\n, \nalignvalfile\n: If provided with filenames that contain 'Pharaoh' format alignment\non the train and validation data, source-to-target alignments are stored in the dataset.\n\n\n\n\n\n\n\n\nTraining options (\ntrain.lua\n)\n\n\nData options\n\n\n\n\ndata_file, val_data_file\n: Path to the training/validation \n*.hdf5\n files created from running\n\npreprocess.py\n.\n\n\nsavefile\n: Savefile name (model will be saved as \nsavefile_epochX_PPL.t7\n after every \nsave_every\n\nepoch where X is the X-th epoch and PPL is the validation perplexity at the epoch.\n\n\nnum_shards\n: If the training data has been broken up into different shards,\nthen this is the number of shards.\n\n\ntrain_from\n: If training from a checkpoint then this is the path to the pre-trained model.\n\n\n\n\nModel options\n\n\n\n\nnum_layers\n: Number of layers in the LSTM encoder/decoder (i.e. number of stacks).\n\n\nrnn_size\n: Size of LSTM hidden states.\n\n\nword_vec_size\n: Word embedding size.\n\n\nattn\n:  If = 1, use attention over the source sequence during decoding. If = 0, then it\nuses the last hidden state of the encoder as the context at each time step.\n\n\nbrnn\n: If = 1, use a bidirectional LSTM on the encoder side. Input embeddings (or CharCNN\nif using characters)  are shared between the forward/backward LSTM, and hidden states of the\ncorresponding forward/backward LSTMs are added to obtain the hidden representation for that\ntime step.\n\n\nuse_chars_enc\n: If = 1, use characters on the encoder side (as inputs).\n\n\nuse_chars_dec\n: If = 1, use characters on the decoder side (as inputs).\n\n\nreverse_src\n: If = 1, reverse the source sequence. The original sequence-to-sequence paper\nfound that this was crucial to achieving good performance, but with attention models this\ndoes not seem necessary. Recommend leaving it to 0.\n\n\ninit_dec\n: Initialize the hidden/cell state of the decoder at time 0 to be the last\nhidden/cell state of the encoder. If 0, the initial states of the decoder are set to zero vectors.\n\n\ninput_feed\n: If = 1, feed the context vector at each time step as additional input (via\nconcatenation with the word embeddings) to the decoder.\n\n\nmulti_attn\n: If \n 0, then use a \nmulti-attention\n on this layer of the decoder. For example, if\n\nnum_layers = 3\n and \nmulti_attn = 2\n, then the model will do an attention over the source sequence\non the second layer (and use that as input to the third layer) \nand\n the penultimate layer.\nWe've found that this did not really improve performance on translation, but may be helpful for\nother tasks where multiple attentional passes over the source sequence are required\n(e.g. for more complex reasoning tasks).\n\n\nres_net\n: Use residual connections between LSTM stacks whereby the input to the l-th LSTM\nlayer of the hidden state of the l-1-th LSTM layer summed with hidden state of the l-2th LSTM layer.\nWe didn't find this to really help in our experiments.\n\n\n\n\nBelow options only apply if using the character model.\n\n\n\n\nchar_vec_size\n: If using characters, size of the character embeddings.\n\n\nkernel_width\n: Size (i.e. width) of the convolutional filter.\n\n\nnum_kernels\n: Number of convolutional filters (feature maps). So the representation from characters will have this many dimensions.\n\n\nnum_highway_layers\n: Number of highway layers in the character composition model.\n\n\n\n\nTo build a model with guided alignment (implemented similarly to \nGuided Alignment Training for Topic-Aware Neural Machine Translation\n (Chen et al. 2016)):\n\n \nguided_alignment\n: If 1, use external alignments to guide the attention weights\n\n \nguided_alignment_weight\n: weight for guided alignment criterion\n* \nguided_alignment_decay\n: decay rate per epoch for alignment weight\n\n\nOptimization options\n\n\n\n\nepochs\n: Number of training epochs.\n\n\nstart_epoch\n: If loading from a checkpoint, the epoch from which to start.\n\n\nparam_init\n: Parameters of the model are initialized over a uniform distribution with support\n\n(-param_init, param_init)\n.\n\n\noptim\n: Optimization method, possible choices are 'sgd', 'adagrad', 'adadelta', 'adam'.\nFor seq2seq I've found vanilla SGD to work well but feel free to experiment.\n\n\nlearning_rate\n: Starting learning rate. For 'adagrad', 'adadelta', and 'adam', this is the global\nlearning rate. Recommended settings vary based on \noptim\n: sgd (\nlearning_rate = 1\n), adagrad\n(\nlearning_rate = 0.1\n), adadelta (\nlearning_rate = 1\n), adam (\nlearning_rate = 0.1\n).\n\n\nmax_grad_norm\n: If the norm of the gradient vector exceeds this, renormalize to have its norm equal to \nmax_grad_norm\n.\n\n\ndropout\n: Dropout probability. Dropout is applied between vertical LSTM stacks.\n\n\nlr_decay\n: Decay learning rate by this much if (i) perplexity does not decrease on the validation\nset or (ii) epoch has gone past the \nstart_decay_at\n epoch limit.\n\n\nstart_decay_at\n: Start decay after this epoch.\n\n\ncurriculum\n: For this many epochs, order the minibatches based on source sequence length. (Sometimes setting this to 1 will increase convergence speed).\n\n\nfeature_embeddings_dim_exponent\n: If the additional feature takes \nN\n values, then the embbeding dimension will be set to \nN^exponent\n.\n\n\npre_word_vecs_enc\n: If using pretrained word embeddings (on the encoder side), this is the\npath to the file with the embeddings. The file should be a serialized Torch tensor with dimensions\nvocab size by embedding size. Each row should be a word embedding and follow the same indexing\nscheme as the *.dict files from running \npreprocess.lua\n. In order to be consistent with \nbeam.lua\n,\nthe first 4 indices should always be \nblank\n, \nunk\n, \ns\n, \n/s\n tokens.\n\n\npre_word_vecs_dec\n: Path to the file for pretrained word embeddings on the decoder side. See above.\n\n\nfix_word_vecs_enc\n: If = 1, fix word embeddings on the encoder side.\n\n\nfix_word_vecs_dec\n: If = 1, fix word embeddings on the decoder side.\n\n\nmax_batch_l\n: Batch size used to create the data in \npreprocess.py\n. If this is left blank\n(recommended), then the batch size will be inferred from the validation set.\n\n\n\n\nOther options\n\n\n\n\nstart_symbol\n: Use special start-of-sentence and end-of-sentence tokens on the source side.\nWe've found this to make minimal difference.\n\n\ngpuid\n: Which GPU to use (-1 = use cpu).\n\n\ngpuid2\n: If this is \n=0, then the model will use two GPUs whereby the encoder is on the first\nGPU and the decoder is on the second GPU. This will allow you to train bigger models.\n\n\ncudnn\n: Whether to use cudnn or not for convolutions (for the character model). \ncudnn\n\nhas much faster convolutions so this is highly recommended if using the character model.\n\n\nsave_every\n: Save every this many epochs.\n\n\nprint_every\n: Print various stats after this many batches.\n\n\nseed\n: Change the random seed for random numbers in torch - use that option to train alternate models for ensemble\n\n\nprealloc\n: when set to 1 (default), enable memory preallocation and sharing between clones - this reduces by a lot the used memory - there should not be\nany situation where you don't need it. Also - since memory is preallocated, there is not (major)\nmemory increase during the training. When set to 0, it rolls back to original memory optimization.\n\n\n\n\n\n\n\n\nDecoding options (\nbeam.lua\n)\n\n\n\n\nmodel\n: Path to model .t7 file.\n\n\nsrc_file\n: Source sequence to decode (one line per sequence).\n\n\ntarg_file\n: True target sequence (optional).\n\n\noutput_file\n: Path to output the predictions (each line will be the decoded sequence).\n\n\nsrc_dict\n: Path to source vocabulary (\n*.src.dict\n file from \npreprocess.py\n).\n\n\ntarg_dict\n: Path to target vocabulary (\n*.targ.dict\n file from \npreprocess.py\n).\n\n\nfeature_dict_prefix\n: Prefix of the path to the features vocabularies (\n*.feature_N.dict\n files from \npreprocess.py\n).\n\n\nchar_dict\n: Path to character vocabulary (\n*.char.dict\n file from \npreprocess.py\n).\n\n\nbeam\n: Beam size (recommend keeping this at 5).\n\n\nmax_sent_l\n: Maximum sentence length. If any of the sequences in \nsrcfile\n are longer than this\nit will error out.\n\n\nsimple\n: If = 1, output prediction is simply the first time the top of the beam\nends with an end-of-sentence token. If = 0, the model considers all hypotheses that have\nbeen generated so far that ends with end-of-sentence token and takes the highest scoring\nof all of them.\n\n\nreplace_unk\n: Replace the generated UNK tokens with the source token that had the highest\nattention weight. If \nsrctarg_dict\n is provided, it will lookup the identified source token\nand give the corresponding target token. If it is not provided (or the identified source token\ndoes not exist in the table) then it will copy the source token.\n\n\nsrctarg_dict\n: Path to source-target dictionary to replace UNK tokens. Each line should be a\nsource token and its corresponding target token, separated by \n|||\n. For example\n\n\n\n\nhello|||hallo\nukraine|||ukrainische\n\n\n\n\nThis dictionary can be obtained by, for example, running an alignment model as a preprocessing step.\nWe recommend \nfast_align\n.\n\n \nscore_gold\n: If = 1, score the true target output as well.\n\n \nn_best\n: If \n 1, then it will also output an n_best list of decoded sentences in the following\nformat.\n\n\n1 ||| sentence_1 ||| sentence_1_score\n2 ||| sentence_2 ||| sentence_2_score\n\n\n\n\n\n\ngpuid\n: ID of the GPU to use (-1 = use CPU).\n\n\ngpuid2\n: ID if the second GPU (if specified).\n\n\ncudnn\n: If the model was trained with \ncudnn\n, then this should be set to 1 (otherwise the model\nwill fail to load).\n\n\nrescore\n: when set to scorer name, use scorer to find hypothesis with highest score - available 'bleu', 'gleu'\n\n\nrescore_param\n: parameter to rescorer - for bleu/gleu ngram length\n\n\n\n\n\n\n\n\nUsing additional input features\n\n\nLinguistic Input Features Improve Neural Machine Translation\n (Senrich et al. 2016) shows that translation performance can be increased by using additional input features.\n\n\nSimilarly to this work, you can annotate each word in the \nsource\n text by using the \n-|-\n separator:\n\n\nword1-|-feat1-|-feat2 word2-|-feat1-|-feat2\n\n\n\n\nIt supports an arbitrary number of features with arbitrary labels. However, all input words must have the \nsame\n number of annotations. See for example \ndata/src-train-case.txt\n which annotates each word with the case information.\n\n\nTo evaluate the model, the option \n-feature_dict_prefix\n is required on \nevaluate.lua\n which points to the prefix of the features dictionnaries generated during the preprocessing.\n\n\n\n\n\n\nPruning a model\n\n\nCompression of Neural Machine Translation Models via Pruning\n (See et al. 2016) shows that a model can be aggressively pruned while keeping the same performace.\n\n\nTo prune a model - you can use \nprune.lua\n which implement class-bind, and class-uniform pruning technique from the paper.\n\n\n\n\nmodel\n: the model to prune\n\n\nsavefile\n: name of the pruned model\n\n\ngpuid\n: Which gpu to use. -1 = use CPU. Depends if the model is serialized for GPU or CPU\n\n\nratio\n: pruning rate\n\n\nprune\n: pruning technique \nblind\n or \nuniform\n, by default \nblind\n\n\n\n\nnote that the pruning cut connection with lowest weight in the linear models by using a boolean mask. The size of the file is a little larger since it stores the actual full matrix and the binary mask.\n\n\nModels can be retrained - typically you can recover full capacity of a model pruned at 60% or even 80% by few epochs of additional trainings.\n\n\n\n\n\n\nSwitching between GPU/CPU models\n\n\nBy default, the model will always save the final model as a CPU model, but it will save the\nintermediate models as a CPU/GPU model depending on how you specified \n-gpuid\n.\nIf you want to run beam search on the CPU with an intermediate model trained on the GPU,\nyou can use \nconvert_to_cpu.lua\n to convert the model to CPU and run beam search.\n\n\n\n\n\n\nGPU memory requirements/Training speed\n\n\nTraining large sequence-to-sequence models can be memory-intensive. Memory requirements will\ndependent on batch size, maximum sequence length, vocabulary size, and (obviously) model size.\nHere are some benchmark numbers on a GeForce GTX Titan X.\n(assuming batch size of 64, maximum sequence length of 50 on both the source/target sequence,\nvocabulary size of 50000, and word embedding size equal to rnn size):\n\n\n(\nprealloc = 0\n)\n\n 1-layer, 100 hidden units: 0.7G, 21.5K tokens/sec\n\n 1-layer, 250 hidden units: 1.4G, 14.1K tokens/sec\n\n 1-layer, 500 hidden units: 2.6G, 9.4K tokens/sec\n\n 2-layers, 500 hidden units: 3.2G, 7.4K tokens/sec\n* 4-layers, 1000 hidden units: 9.4G, 2.5K tokens/sec\n\n\nThanks to some fantastic work from folks at \nSYSTRAN\n, turning \nprealloc\n on\nwill lead to much more memory efficient training\n\n\n(\nprealloc = 1\n)\n\n 1-layer, 100 hidden units: 0.5G, 22.4K tokens/sec\n\n 1-layer, 250 hidden units: 1.1G, 14.5K tokens/sec\n\n 1-layer, 500 hidden units: 2.1G, 10.0K tokens/sec\n\n 2-layers, 500 hidden units: 2.3G, 8.2K tokens/sec\n* 4-layers, 1000 hidden units: 6.4G, 3.3K tokens/sec\n\n\nTokens/sec refers to total (i.e. source + target) tokens processed per second.\nIf using different batch sizes/sequence length, you should (linearly) scale\nthe above numbers accordingly. You can make use of memory on multiple GPUs by using\n\n-gpuid2\n option in \ntrain.lua\n. This will put the encoder on the GPU specified by\n\n-gpuid\n, and the decoder on the GPU specified by \n-gpuid2\n.\n\n\n\n\n\n\nEvaluation\n\n\nFor translation, evaluation via BLEU can be done by taking the output from \nbeam.lua\n and using the\n\nmulti-bleu.perl\n script from \nMoses\n. For example\n\n\nperl multi-bleu.perl gold.txt \n pred.txt\n\n\n\n\n\n\n\n\nEvaluation of States and Attention\n\n\nattention_extraction.lua can be used to extract the attention and the LSTM states. It uses the following (required) options:\n\n\n\n\nmodel\n: Path to model .t7 file.\n\n\nsrc_file\n: Source sequence to decode (one line per sequence).\n\n\ntarg_file\n: True target sequence.\n\n\nsrc_dict\n: Path to source vocabulary (\n*.src.dict\n file from \npreprocess.py\n).\n\n\ntarg_dict\n: Path to target vocabulary (\n*.targ.dict\n file from \npreprocess.py\n).\n\n\n\n\nOutput of the script are two files, \nencoder.hdf5\n and \ndecoder.hdf5\n. The encoder contains the states for every layer of the encoder LSTM and the offsets for the start of each source sentence. The decoder contains the states for the decoder LSTM layers and the offsets for the start of gold sentence. It additionally contains the attention for each time step (if the model uses attention).\n\n\n\n\n\n\nPre-trained models\n\n\nWe've uploaded English \n-\n German models trained on 4 million sentences from\n\nWorkshop on Machine Translation 2015\n.\nDownload link is below:\n\n\nhttps://drive.google.com/open?id=0BzhmYioWLRn_aEVnd0ZNcWd0Y2c\n\n\nThese models are 4-layer LSTMs with 1000 hidden units and essentially replicates the results from\n\nEffective Approaches to Attention-based\nNeural Machine Translation\n,\nLuong et al. EMNLP 2015.\n\n\n\n\n\n\nAcknowledgments\n\n\nOur implementation utilizes code from the following:\n\n \nAndrej Karpathy's char-rnn repo\n\n\n \nWojciech Zaremba's lstm repo\n\n* \nElement rnn library\n\n\n\n\n\n\nLicence\n\n\nMIT\n\n\n\n\n\nHighlight.js\n\n\nHighlight.js highlights syntax in code examples on blogs, forums and,\nin fact, on any web page. It's very easy to use because it works\nautomatically: finds blocks of code, detects a language, highlights it.\n\n\nAutodetection can be fine tuned when it fails by itself (see \"Heuristics\").\n\n\n\n\n\n\nBasic usage\n\n\nLink the library and a stylesheet from your page and hook highlighting to\nthe page load event:\n\n\nlink rel=\nstylesheet\n href=\nstyles/default.css\n\n\nscript src=\nhighlight.pack.js\n/script\n\n\nscript\nhljs.initHighlightingOnLoad();\n/script\n\n\n\n\n\nThis will highlight all code on the page marked up as \npre\ncode\n .. \n/code\n/pre\n.\nIf you use different markup or need to apply highlighting dynamically, read\n\"Custom initialization\" below.\n\n\n\n\n\n\nYou can download your own customized version of \"highlight.pack.js\" or\n  use the hosted one as described on the download page:\n  \nhttp://highlightjs.org/download/\n\n\n\n\n\n\nStyle themes are available in the download package or as hosted files.\n  To create a custom style for your site see the class reference in the file\n  \nclassref.txt\n from the downloaded package.\n\n\n\n\n\n\n\n\n\n\nnode.js\n\n\nHighlight.js can be used under node.js. The package with all supported languages is\ninstallable from NPM:\n\n\nnpm install highlight.js\n\n\n\nAlternatively, you can build it from the source with only languages you need:\n\n\npython3 tools/build.py -tnode lang1 lang2 ..\n\n\n\nUsing the library:\n\n\nvar hljs = require('highlight.js');\n\n// If you know the language\nhljs.highlight(lang, code).value;\n\n// Automatic language detection\nhljs.highlightAuto(code).value;\n\n\n\n\n\n\n\n\nAMD\n\n\nHighlight.js can be used with an AMD loader.  You will need to build it from\nsource in order to do so:\n\n\n$ python3 tools/build.py -tamd lang1 lang2 ..\n\n\n\n\nWhich will generate a \nbuild/highlight.pack.js\n which will load as an AMD\nmodule with support for the built languages and can be used like so:\n\n\nrequire([\nhighlight.js/build/highlight.pack\n], function(hljs){\n\n  // If you know the language\n  hljs.highlight(lang, code).value;\n\n  // Automatic language detection\n  hljs.highlightAuto(code).value;\n});\n\n\n\n\n\n\n\n\nTab replacement\n\n\nYou can replace TAB ('\\x09') characters used for indentation in your code\nwith some fixed number of spaces or with a \nspan\n to give them special\nstyling:\n\n\nscript type=\ntext/javascript\n\n  hljs.tabReplace = '    '; // 4 spaces\n  // ... or\n  hljs.tabReplace = '\nspan class=\nindent\n\\t\n/span\n';\n\n  hljs.initHighlightingOnLoad();\n\n/script\n\n\n\n\n\n\n\n\n\nCustom initialization\n\n\nIf you use different markup for code blocks you can initialize them manually\nwith \nhighlightBlock(code, tabReplace, useBR)\n function. It takes a DOM element\ncontaining the code to highlight and optionally a string with which to replace\nTAB characters.\n\n\nInitialization using, for example, jQuery might look like this:\n\n\n$(document).ready(function() {\n  $('pre code').each(function(i, e) {hljs.highlightBlock(e)});\n});\n\n\n\n\nYou can use \nhighlightBlock\n to highlight blocks dynamically inserted into\nthe page. Just make sure you don't do it twice for already highlighted\nblocks.\n\n\nIf your code container relies on \nbr\n tags instead of line breaks (i.e. if\nit's not \npre\n) pass \ntrue\n into the third parameter of \nhighlightBlock\n\nto make highlight.js use \nbr\n in the output:\n\n\n$('div.code').each(function(i, e) {hljs.highlightBlock(e, null, true)});\n\n\n\n\n\n\n\n\nHeuristics\n\n\nAutodetection of a code's language is done using a simple heuristic:\nthe program tries to highlight a fragment with all available languages and\ncounts all syntactic structures that it finds along the way. The language\nwith greatest count wins.\n\n\nThis means that in short fragments the probability of an error is high\n(and it really happens sometimes). In this cases you can set the fragment's\nlanguage explicitly by assigning a class to the \ncode\n element:\n\n\npre\ncode class=\nhtml\n...\n/code\n/pre\n\n\n\n\n\nYou can use class names recommended in HTML5: \"language-html\",\n\"language-php\". Classes also can be assigned to the \npre\n element.\n\n\nTo disable highlighting of a fragment altogether use \"no-highlight\" class:\n\n\npre\ncode class=\nno-highlight\n...\n/code\n/pre\n\n\n\n\n\n\n\n\n\nExport\n\n\nFile export.html contains a little program that allows you to paste in a code\nsnippet and then copy and paste the resulting HTML code generated by the\nhighlighter. This is useful in situations when you can't use the script itself\non a site.\n\n\n\n\n\n\nMeta\n\n\n\n\nVersion: 7.5\n\n\nURL:     http://highlightjs.org/\n\n\n\n\nFor the license terms see LICENSE files.\nFor authors and contributors see AUTHORS.en.txt file.\n\n\n\n\n\n\n\nOpenNMT.dict\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.dict(data)\n\n\n\n * \nOpenNMT.dict:load_file(filename)\n\n\n\n * \nOpenNMT.dict:write_file(filename)\n\n\n\n * \nOpenNMT.dict:lookup(key)\n\n\n\n * \nOpenNMT.dict:set_special(special)\n\n\n\n * \nOpenNMT.dict:add_special(label, idx)\n\n\n\n * \nOpenNMT.dict:add_specials(labels)\n\n\n\n * \nOpenNMT.dict:add(label, idx)\n\n\n\n * \nOpenNMT.dict:prune(size)\n\n\n\n * \nOpenNMT.dict:convert_to_idx(labels, start_symbols)\n\n\n\n * \nOpenNMT.dict:convert_to_labels(idx, stop)\n\n\n\n\n\n\n\n\nOpenNMT.Encoder\n\n\nEncoder is a unidirectional Sequencer used for the source language. \n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Encoder(args, network)\n\n\nConstructor takes global \nargs\n and optional \nnetwork\n. \n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Encoder:resize_proto(batch_size)\n\n\nCall to change the \nbatch_size\n. \n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Encoder:forward(batch)\n\n\nCompute the context representation of an input.\n\n\nParameters:\n\n\n\n\nbatch\n - a \nbatch struct\n as defined data.lua.\n\n\n\n\nReturns:\n\n\n\n\n\n\n\n\nlast hidden states\n\n\n\n\n\n\n\n\n\n\ncontext matrix H\n\n\n\n\n\n\n\n\nTODO:\n\n\n\n\nChange \nbatch\n to \ninput\n.\n\n\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Encoder:backward(batch, grad_states_output, grad_context_output)\n\n\nBackward pass (only called during training)\n\n\nParameters:\n\n\n\n\nbatch\n - must be same as for forward\n\n\ngrad_states_output\n\n\ngrad_context_output\n - gradient of loss\n      wrt last states and context.\n\n\n\n\nTODO: change this to (input, gradOutput) as in nngraph.\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Encoder:convert(f)\n\n\n\n\n\n\n\n\nOpenNMT.BiEncoder\n\n\nBiEncoder is a bidirectional Sequencer used for the source language. \n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.BiEncoder(args, merge, net_fwd, net_bwd)\n\n\nCreates two Encoder's (encoder.lua) \nnet_fwd\n and \nnet_bwd\n.\n  The two are combined use \nmerge\n operation (concat/sum).\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.BiEncoder:resize_proto(batch_size)\n\n\n\n * \nOpenNMT.BiEncoder:forward(batch)\n\n\n\n * \nOpenNMT.BiEncoder:backward(batch, grad_states_output, grad_context_output)\n\n\n\n * \nOpenNMT.BiEncoder:training()\n\n\n\n * \nOpenNMT.BiEncoder:evaluate()\n\n\n\n * \nOpenNMT.BiEncoder:convert(f)\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Cuda.init(opt)\n\n\n\n * \nOpenNMT.Cuda.convert(obj)\n\n\n\n\n\n\n\n\nOpenNMT.EpochState\n\n\nClass for managing the training process by logging and storing\n  the state of the current epoch.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.EpochState(epoch, status)\n\n\nInitialize for epoch \nepoch\n and training \nstatus\n (current loss)\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.EpochState:update(batch, loss)\n\n\nUpdate training status. Takes \nbatch\n (described in data.lua) and last loss.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.EpochState:log(batch_index, data_size, learning_rate)\n\n\nLog to status stdout.\n  TODO: these args shouldn't need to be passed in each time. \n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.EpochState:get_train_ppl()\n\n\n\n * \nOpenNMT.EpochState:get_time()\n\n\n\n * \nOpenNMT.EpochState:get_status()\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Memory.optimize(model, batch)\n\n\n\n\n\n\n\n\nStyle Guide\n\n\n\n\n\n\n\n\nComments\n\n\n\n\n\n\nComments should follow:\nhttps://github.com/deepmind/torch-dokx/blob/master/doc/usage.md\n\n\n\n\n\n\nAll non-private method should have dokx comments describing input/output.  \n\n\n\n\n\n\nAll classes should have a class docstring at the top of the file. \n\n\n\n\n\n\nAll comments should be on their own line, and be a complete English\nsentence with capitalization.\n\n\n\n\n\n\nUse torch-dokx and this command to build docs \n\n\n\n\ndokx-build-package-docs -o docs .\ngoogle-chrome doc/index.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStyle:\n\n\n\n\nPlease run and correct all warnings from luacheck before sending a pull request. \n\n\n\n\n\n\nluacheck *\n\n\n\n\n\n\nAll indentation should be 2 spaces.\n\n\n\n\n\n\n\n\n\nOpenNMT.Decoder\n\n\nDecoder is the sequencer for the target words.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Decoder:resize_proto(batch_size)\n\n\nCall to change the \nbatch_size\n.\n\n\nTODO: rename.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Decoder:reset(source_sizes, source_length, beam_size)\n\n\nUpdate internals to prepare for new batch.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Decoder:forward_one(input, prev_states, context, prev_out, t)\n\n\nRun one step of the decoder.\n\n\nParameters:\n * \ninput\n - sparse input (1)\n * \nprev_states\n - stack of hidden states (batch x layers*model x rnn_size)\n * \ncontext\n - encoder output (batch x n x rnn_size)\n * \nprev_out\n - previous distribution (batch x #words)\n * \nt\n - current timestep\n\n\nReturns:\n 1. \nout\n - Top-layer Hidden state\n 2. \nstates\n - All states\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Decoder:forward_and_apply(batch, encoder_states, context, func)\n\n\nCompute all forward steps.\n\n\nParameters:\n  * \nbatch\n - based on data.lua\n  * \nencoder_states\n\n  * \ncontext\n\n  * \nfunc\n - Calls \nfunc(out, t)\n each timestep.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Decoder:forward(batch, encoder_states, context)\n\n\nCompute all forward steps.\n\n\nParameters:\n  * \nbatch\n - based on data.lua\n  * \nencoder_states\n\n  * \ncontext\n\n\nReturns:\n  1. \noutputs\n - Top Hidden layer at each time-step.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Decoder:compute_loss(batch, encoder_states, context, generator)\n\n\nCompute the loss on a batch based on final layer \ngenerator\n.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Decoder:backward(batch, outputs, generator)\n\n\nCompute the standard backward update.\n  With input \nbatch\n, target \noutputs\n, and \ngenerator\n\n  Note: This code is both the standard backward and criterion forward/backward.\n  It returns both the gradInputs (ret 1 and 2) and the loss.\n\n\nTODO: This object should own \ngenerator\n and or, generator should\n  control its own backward pass.\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Decoder(args, network)\n\n\n\n * \nOpenNMT.Decoder:compute_score(batch, encoder_states, context, generator)\n\n\n\n * \nOpenNMT.Decoder:convert(f)\n\nThe MIT License (MIT)\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n\n\n\n\n\n\nOpenNMT.Sequencer\n\n\nSequencer is the base class for our time series LSTM models.\n  Acts similarly to an \nnn.Module\n.\n   Main task is to manage \nself.network_clones\n, the unrolled LSTM\n  used during training.\n  Classes encoder/decoder/biencoder generalize these definitions.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Sequencer(model, args, network)\n\n\nConstructor\n\n\nParameters:\n  * \nmodel\n - type of model (enc,dec)\n  * \nargs\n - global arguments\n  * \nnetwork\n - optional preconstructed network.\n\n\nTODO: Should initialize all the members in this method.\n   i.e. word_vecs, fix_word_vecs, network_clones, eval_mode, etc.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Sequencer:training()\n\n\nTell the network to prepare for training mode. \n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Sequencer:evaluate()\n\n\nTell the network to prepare for evaluation mode. \n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Sequencer:resize_proto(batch_size)\n\n\n\n * \nOpenNMT.Sequencer:backward_word_vecs()\n\n\n\n * \nOpenNMT.Sequencer:net(t)\n\n\n\n * \nOpenNMT.Sequencer:convert(f)\n\n\n\n\n\n\n\n\nOpenNMT.Optim\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Optim:update_learning_rate(score, epoch)\n\n\ndecay learning rate if val perf does not improve or we hit the start_decay_at limit\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Optim(args)\n\n\n\n * \nOpenNMT.Optim:update_params(params, grad_params, max_grad_norm)\n\n\n\n * \nOpenNMT.Optim:get_learning_rate()\n\n\n\n * \nOpenNMT.Optim:get_states()\n\n\n\n\n\n\n\n\nOpenNMT.Data\n\n\nData management and batch creation.\n\n\nBatch interface [size]: \n\n\n\n\nsize: number of sentences in the batch [1]\n\n\nsource_length: max length in source batch [1]\n\n\nsource_size:  lengths of each source [batch x 1]\n\n\nsource_input:  left-padded idx's of source (PPPPPPABCDE) [batch x max]\n\n\nsource_input_rev: right-padded  idx's of source rev (EDCBAPPPPPP) [batch x max]\n\n\ntarget_length: max length in source batch [1]\n\n\ntarget_size: lengths of each source [batch x 1]\n\n\ntarget_non_zeros: number of non-ignored words in batch [1]\n\n\ntarget_input: input idx's of target (SABCDEPPPPPP) [batch x max]\n\n\ntarget_output: expected output idx's of target (ABCDESPPPPPP) [batch x max]\n\n\n\n\nTODO: change name of size =\n maxlen\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Data(src, targ)\n\n\nInitialize a data object given aligned tables of IntTensors \nsrc\n\n  and \ntarg\n.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Data:set_batch_size(max_batch_size)\n\n\nSetup up the training data to respect \nmax_batch_size\n. \n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Data:get_data(src, targ)\n\n\nCreate a batch object given aligned sent tables \nsrc\n and \ntarg\n\n  (optional). Data format is shown at the top of the file.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Data:get_batch(idx)\n\n\nGet batch \nidx\n. If nil make a batch of all the data. \n\n\n\n\n\n\n\nSequence-to-Sequence Learning with Attentional Neural Networks\n\n\nTorch\n implementation of a standard sequence-to-sequence model with (optional)\nattention where the encoder-decoder are LSTMs. Encoder can be a bidirectional LSTM.\nAdditionally has the option to use characters\n(instead of input word embeddings) by running a convolutional neural network followed by a\n\nhighway network\n over character embeddings to use as inputs.\n\n\nThe attention model is from\n\nEffective Approaches to Attention-based\nNeural Machine Translation\n,\nLuong et al. EMNLP 2015. We use the \nglobal-general-attention\n model with the\n\ninput-feeding\n approach from the paper. Input-feeding is optional and can be turned off.\n\n\nThe character model is from \nCharacter-Aware Neural\nLanguage Models\n, Kim et al. AAAI 2016.\n\n\nThere are a lot of additional options on top of the baseline model, mainly thanks to the fantastic folks \nat \nSYSTRAN\n. Specifically, there are functionalities which implement:\n\n \nEffective Approaches to Attention-based Neural Machine Translation\n. Luong et al., EMNLP 2015.\n\n \nCharacter-based Neural Machine Translation\n. Costa-Jussa and Fonollosa, ACL 2016.\n\n \nCompression of Neural Machine Translation Models via Pruning\n. See et al., COLING 2016.\n\n \nSequence-Level Knowledge Distillation\n. Kim and Rush., EMNLP 2016.\n\n \nDeep Recurrent Models with Fast Forward Connections for Neural Machine Translation\n.\nZhou et al, TACL 2016.\n\n \nGuided Alignment Training for Topic-Aware Neural Machine Translation\n. Chen et al., arXiv:1607.01628.\n* \nLinguistic Input Features Improve Neural Machine Translation\n. Senrich et al., arXiv:1606.02892\n\n\nSee below for more details on how to use them.\n\n\nThis project is maintained by \nYoon Kim\n.\nFeel free to post any questions/issues on the issues page.\n\n\n\n\n\n\n\n\nDependencies\n\n\n\n\n\n\n\n\nLua\n\n\nYou will need the following packages:\n\n nn\n\n nngraph\n\n\nGPU usage will additionally require:\n\n cutorch\n\n cunn\n\n\nIf running the character model, you should also install:\n\n cudnn\n\n luautf8\n\n\n\n\n\n\n\n\nQuickstart\n\n\nWe are going to be working with some example data in \ndata/\n folder.\nFirst run the data-processing code\n\n\nth preprocess.lua -train_src_file data/src-train.txt -train_targ_file data/targ-train.txt\n    -valid_src_file data/src-val.txt -valid_targ_file data/targ-val.txt -output_file data/demo\n\n\n\n\nThis will take the source/target train/valid files (\nsrc-train.txt, targ-train.txt,\nsrc-val.txt, targ-val.txt\n) and build the following files:\n\n\n\n\ndemo.src.dict\n: Dictionary of source vocab to index mappings.\n\n\ndemo.targ.dict\n: Dictionary of target vocab to index mappings.\n\n\ndemo-train.t7\n: serialized torch file containing vocabulary, training and validation data\n\n\n\n\nThe \n*.dict\n files are needed to check vocabulary, or to preprocess data with fixed vocabularies\n\n\nNow run the model\n\n\nth train.lua -data_file data/demo-train.t7 -savefile demo-model\n\n\n\n\nThis will run the default model, which consists of a 2-layer LSTM with 500 hidden units\non both the encoder/decoder.\nYou can also add \n-gpuid 1\n to use (say) GPU 1 in the cluster.\n\n\nNow you have a model which you can use to predict on new data. To do this we are\ngoing to be running beam search\n\n\nth evaluate.lua -model demo-model_final.t7 -src_file data/src-val.txt -output_file pred.txt\n-src_dict data/demo.src.dict -targ_dict data/demo.targ.dict\n\n\n\n\nThis will output predictions into \npred.txt\n. The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for \ntranslation\n\nor \nsummarization\n.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\n\n\n\n\nPreprocessing options (\npreprocess.py\n)\n\n\n\n\nsrcvocabsize, targetvocabsize\n: Size of source/target vocabularies. This is constructed\nby taking the top X most frequent words. Rest are replaced with special UNK tokens.\n\n\nsrcfile, targetfile\n: Path to source/target training data, where each line represents a single\nsource/target sequence.\n\n\nsrcvalfile, targetvalfile\n: Path to source/target validation data.\n\n\nbatchsize\n: Size of each mini-batch.\n\n\nseqlength\n: Maximum sequence length (sequences longer than this are dropped).\n\n\noutputfile\n: Prefix of the output file names.\n\n\nmaxwordlength\n: For the character models, words are truncated (if longer than maxwordlength)\nor zero-padded (if shorter) to \nmaxwordlength\n.\n\n\nchars\n: If 1, construct the character-level dataset as well.  This might take up a lot of space\ndepending on your data size, so you may want to break up the training data into different shards.\n\n\nsrcvocabfile, targetvocabfile\n: If working with a preset vocab, then including these paths\nwill ignore the \nsrcvocabsize,targetvocabsize\n.\n\n\nunkfilter\n: Ignore sentences with too many UNK tokens. Can be an absolute count limit (if \n 1)\nor a proportional limit (0 \n unkfilter \n 1).\n\n\nshuffle\n: Shuffle sentences.\n\n\nalignfile\n, \nalignvalfile\n: If provided with filenames that contain 'Pharaoh' format alignment\non the train and validation data, source-to-target alignments are stored in the dataset.\n\n\n\n\n\n\n\n\n\n\nTraining options (\ntrain.lua\n)\n\n\nData options\n\n\n\n\ndata_file, val_data_file\n: Path to the training/validation \n*.hdf5\n files created from running\n\npreprocess.py\n.\n\n\nsavefile\n: Savefile name (model will be saved as \nsavefile_epochX_PPL.t7\n after every \nsave_every\n\nepoch where X is the X-th epoch and PPL is the validation perplexity at the epoch.\n\n\nnum_shards\n: If the training data has been broken up into different shards,\nthen this is the number of shards.\n\n\ntrain_from\n: If training from a checkpoint then this is the path to the pre-trained model.\n\n\n\n\nModel options\n\n\n\n\nnum_layers\n: Number of layers in the LSTM encoder/decoder (i.e. number of stacks).\n\n\nrnn_size\n: Size of LSTM hidden states.\n\n\nword_vec_size\n: Word embedding size.\n\n\nattn\n:  If = 1, use attention over the source sequence during decoding. If = 0, then it\nuses the last hidden state of the encoder as the context at each time step.\n\n\nbrnn\n: If = 1, use a bidirectional LSTM on the encoder side. Input embeddings (or CharCNN\nif using characters)  are shared between the forward/backward LSTM, and hidden states of the\ncorresponding forward/backward LSTMs are added to obtain the hidden representation for that\ntime step.\n\n\nuse_chars_enc\n: If = 1, use characters on the encoder side (as inputs).\n\n\nuse_chars_dec\n: If = 1, use characters on the decoder side (as inputs).\n\n\nreverse_src\n: If = 1, reverse the source sequence. The original sequence-to-sequence paper\nfound that this was crucial to achieving good performance, but with attention models this\ndoes not seem necessary. Recommend leaving it to 0.\n\n\ninit_dec\n: Initialize the hidden/cell state of the decoder at time 0 to be the last\nhidden/cell state of the encoder. If 0, the initial states of the decoder are set to zero vectors.\n\n\ninput_feed\n: If = 1, feed the context vector at each time step as additional input (via\nconcatenation with the word embeddings) to the decoder.\n\n\nmulti_attn\n: If \n 0, then use a \nmulti-attention\n on this layer of the decoder. For example, if\n\nnum_layers = 3\n and \nmulti_attn = 2\n, then the model will do an attention over the source sequence\non the second layer (and use that as input to the third layer) \nand\n the penultimate layer.\nWe've found that this did not really improve performance on translation, but may be helpful for\nother tasks where multiple attentional passes over the source sequence are required\n(e.g. for more complex reasoning tasks).\n\n\nres_net\n: Use residual connections between LSTM stacks whereby the input to the l-th LSTM\nlayer of the hidden state of the l-1-th LSTM layer summed with hidden state of the l-2th LSTM layer.\nWe didn't find this to really help in our experiments.\n\n\n\n\nBelow options only apply if using the character model.\n\n\n\n\nchar_vec_size\n: If using characters, size of the character embeddings.\n\n\nkernel_width\n: Size (i.e. width) of the convolutional filter.\n\n\nnum_kernels\n: Number of convolutional filters (feature maps). So the representation from characters will have this many dimensions.\n\n\nnum_highway_layers\n: Number of highway layers in the character composition model.\n\n\n\n\nTo build a model with guided alignment (implemented similarly to \nGuided Alignment Training for Topic-Aware Neural Machine Translation\n (Chen et al. 2016)):\n\n \nguided_alignment\n: If 1, use external alignments to guide the attention weights\n\n \nguided_alignment_weight\n: weight for guided alignment criterion\n* \nguided_alignment_decay\n: decay rate per epoch for alignment weight\n\n\nOptimization options\n\n\n\n\nepochs\n: Number of training epochs.\n\n\nstart_epoch\n: If loading from a checkpoint, the epoch from which to start.\n\n\nparam_init\n: Parameters of the model are initialized over a uniform distribution with support\n\n(-param_init, param_init)\n.\n\n\noptim\n: Optimization method, possible choices are 'sgd', 'adagrad', 'adadelta', 'adam'.\nFor seq2seq I've found vanilla SGD to work well but feel free to experiment.\n\n\nlearning_rate\n: Starting learning rate. For 'adagrad', 'adadelta', and 'adam', this is the global\nlearning rate. Recommended settings vary based on \noptim\n: sgd (\nlearning_rate = 1\n), adagrad\n(\nlearning_rate = 0.1\n), adadelta (\nlearning_rate = 1\n), adam (\nlearning_rate = 0.1\n).\n\n\nmax_grad_norm\n: If the norm of the gradient vector exceeds this, renormalize to have its norm equal to \nmax_grad_norm\n.\n\n\ndropout\n: Dropout probability. Dropout is applied between vertical LSTM stacks.\n\n\nlr_decay\n: Decay learning rate by this much if (i) perplexity does not decrease on the validation\nset or (ii) epoch has gone past the \nstart_decay_at\n epoch limit.\n\n\nstart_decay_at\n: Start decay after this epoch.\n\n\ncurriculum\n: For this many epochs, order the minibatches based on source sequence length. (Sometimes setting this to 1 will increase convergence speed).\n\n\nfeature_embeddings_dim_exponent\n: If the additional feature takes \nN\n values, then the embbeding dimension will be set to \nN^exponent\n.\n\n\npre_word_vecs_enc\n: If using pretrained word embeddings (on the encoder side), this is the\npath to the file with the embeddings. The file should be a serialized Torch tensor with dimensions\nvocab size by embedding size. Each row should be a word embedding and follow the same indexing\nscheme as the *.dict files from running \npreprocess.lua\n. In order to be consistent with \nbeam.lua\n,\nthe first 4 indices should always be \nblank\n, \nunk\n, \ns\n, \n/s\n tokens.\n\n\npre_word_vecs_dec\n: Path to the file for pretrained word embeddings on the decoder side. See above.\n\n\nfix_word_vecs_enc\n: If = 1, fix word embeddings on the encoder side.\n\n\nfix_word_vecs_dec\n: If = 1, fix word embeddings on the decoder side.\n\n\nmax_batch_l\n: Batch size used to create the data in \npreprocess.py\n. If this is left blank\n(recommended), then the batch size will be inferred from the validation set.\n\n\n\n\nOther options\n\n\n\n\nstart_symbol\n: Use special start-of-sentence and end-of-sentence tokens on the source side.\nWe've found this to make minimal difference.\n\n\ngpuid\n: Which GPU to use (-1 = use cpu).\n\n\ngpuid2\n: If this is \n=0, then the model will use two GPUs whereby the encoder is on the first\nGPU and the decoder is on the second GPU. This will allow you to train bigger models.\n\n\ncudnn\n: Whether to use cudnn or not for convolutions (for the character model). \ncudnn\n\nhas much faster convolutions so this is highly recommended if using the character model.\n\n\nsave_every\n: Save every this many epochs.\n\n\nprint_every\n: Print various stats after this many batches.\n\n\nseed\n: Change the random seed for random numbers in torch - use that option to train alternate models for ensemble\n\n\nprealloc\n: when set to 1 (default), enable memory preallocation and sharing between clones - this reduces by a lot the used memory - there should not be\nany situation where you don't need it. Also - since memory is preallocated, there is not (major)\nmemory increase during the training. When set to 0, it rolls back to original memory optimization.\n\n\n\n\n\n\n\n\n\n\nDecoding options (\nbeam.lua\n)\n\n\n\n\nmodel\n: Path to model .t7 file.\n\n\nsrc_file\n: Source sequence to decode (one line per sequence).\n\n\ntarg_file\n: True target sequence (optional).\n\n\noutput_file\n: Path to output the predictions (each line will be the decoded sequence).\n\n\nsrc_dict\n: Path to source vocabulary (\n*.src.dict\n file from \npreprocess.py\n).\n\n\ntarg_dict\n: Path to target vocabulary (\n*.targ.dict\n file from \npreprocess.py\n).\n\n\nfeature_dict_prefix\n: Prefix of the path to the features vocabularies (\n*.feature_N.dict\n files from \npreprocess.py\n).\n\n\nchar_dict\n: Path to character vocabulary (\n*.char.dict\n file from \npreprocess.py\n).\n\n\nbeam\n: Beam size (recommend keeping this at 5).\n\n\nmax_sent_l\n: Maximum sentence length. If any of the sequences in \nsrcfile\n are longer than this\nit will error out.\n\n\nsimple\n: If = 1, output prediction is simply the first time the top of the beam\nends with an end-of-sentence token. If = 0, the model considers all hypotheses that have\nbeen generated so far that ends with end-of-sentence token and takes the highest scoring\nof all of them.\n\n\nreplace_unk\n: Replace the generated UNK tokens with the source token that had the highest\nattention weight. If \nsrctarg_dict\n is provided, it will lookup the identified source token\nand give the corresponding target token. If it is not provided (or the identified source token\ndoes not exist in the table) then it will copy the source token.\n\n\nsrctarg_dict\n: Path to source-target dictionary to replace UNK tokens. Each line should be a\nsource token and its corresponding target token, separated by \n|||\n. For example\n\n\n\n\nhello|||hallo\nukraine|||ukrainische\n\n\n\n\nThis dictionary can be obtained by, for example, running an alignment model as a preprocessing step.\nWe recommend \nfast_align\n.\n\n \nscore_gold\n: If = 1, score the true target output as well.\n\n \nn_best\n: If \n 1, then it will also output an n_best list of decoded sentences in the following\nformat.\n\n\n1 ||| sentence_1 ||| sentence_1_score\n2 ||| sentence_2 ||| sentence_2_score\n\n\n\n\n\n\ngpuid\n: ID of the GPU to use (-1 = use CPU).\n\n\ngpuid2\n: ID if the second GPU (if specified).\n\n\ncudnn\n: If the model was trained with \ncudnn\n, then this should be set to 1 (otherwise the model\nwill fail to load).\n\n\nrescore\n: when set to scorer name, use scorer to find hypothesis with highest score - available 'bleu', 'gleu'\n\n\nrescore_param\n: parameter to rescorer - for bleu/gleu ngram length\n\n\n\n\n\n\n\n\n\n\nUsing additional input features\n\n\nLinguistic Input Features Improve Neural Machine Translation\n (Senrich et al. 2016) shows that translation performance can be increased by using additional input features.\n\n\nSimilarly to this work, you can annotate each word in the \nsource\n text by using the \n-|-\n separator:\n\n\nword1-|-feat1-|-feat2 word2-|-feat1-|-feat2\n\n\n\n\nIt supports an arbitrary number of features with arbitrary labels. However, all input words must have the \nsame\n number of annotations. See for example \ndata/src-train-case.txt\n which annotates each word with the case information.\n\n\nTo evaluate the model, the option \n-feature_dict_prefix\n is required on \nevaluate.lua\n which points to the prefix of the features dictionnaries generated during the preprocessing.\n\n\n\n\n\n\n\n\nPruning a model\n\n\nCompression of Neural Machine Translation Models via Pruning\n (See et al. 2016) shows that a model can be aggressively pruned while keeping the same performace.\n\n\nTo prune a model - you can use \nprune.lua\n which implement class-bind, and class-uniform pruning technique from the paper.\n\n\n\n\nmodel\n: the model to prune\n\n\nsavefile\n: name of the pruned model\n\n\ngpuid\n: Which gpu to use. -1 = use CPU. Depends if the model is serialized for GPU or CPU\n\n\nratio\n: pruning rate\n\n\nprune\n: pruning technique \nblind\n or \nuniform\n, by default \nblind\n\n\n\n\nnote that the pruning cut connection with lowest weight in the linear models by using a boolean mask. The size of the file is a little larger since it stores the actual full matrix and the binary mask.\n\n\nModels can be retrained - typically you can recover full capacity of a model pruned at 60% or even 80% by few epochs of additional trainings.\n\n\n\n\n\n\n\n\nSwitching between GPU/CPU models\n\n\nBy default, the model will always save the final model as a CPU model, but it will save the\nintermediate models as a CPU/GPU model depending on how you specified \n-gpuid\n.\nIf you want to run beam search on the CPU with an intermediate model trained on the GPU,\nyou can use \nconvert_to_cpu.lua\n to convert the model to CPU and run beam search.\n\n\n\n\n\n\n\n\nGPU memory requirements/Training speed\n\n\nTraining large sequence-to-sequence models can be memory-intensive. Memory requirements will\ndependent on batch size, maximum sequence length, vocabulary size, and (obviously) model size.\nHere are some benchmark numbers on a GeForce GTX Titan X.\n(assuming batch size of 64, maximum sequence length of 50 on both the source/target sequence,\nvocabulary size of 50000, and word embedding size equal to rnn size):\n\n\n(\nprealloc = 0\n)\n\n 1-layer, 100 hidden units: 0.7G, 21.5K tokens/sec\n\n 1-layer, 250 hidden units: 1.4G, 14.1K tokens/sec\n\n 1-layer, 500 hidden units: 2.6G, 9.4K tokens/sec\n\n 2-layers, 500 hidden units: 3.2G, 7.4K tokens/sec\n* 4-layers, 1000 hidden units: 9.4G, 2.5K tokens/sec\n\n\nThanks to some fantastic work from folks at \nSYSTRAN\n, turning \nprealloc\n on\nwill lead to much more memory efficient training\n\n\n(\nprealloc = 1\n)\n\n 1-layer, 100 hidden units: 0.5G, 22.4K tokens/sec\n\n 1-layer, 250 hidden units: 1.1G, 14.5K tokens/sec\n\n 1-layer, 500 hidden units: 2.1G, 10.0K tokens/sec\n\n 2-layers, 500 hidden units: 2.3G, 8.2K tokens/sec\n* 4-layers, 1000 hidden units: 6.4G, 3.3K tokens/sec\n\n\nTokens/sec refers to total (i.e. source + target) tokens processed per second.\nIf using different batch sizes/sequence length, you should (linearly) scale\nthe above numbers accordingly. You can make use of memory on multiple GPUs by using\n\n-gpuid2\n option in \ntrain.lua\n. This will put the encoder on the GPU specified by\n\n-gpuid\n, and the decoder on the GPU specified by \n-gpuid2\n.\n\n\n\n\n\n\n\n\nEvaluation\n\n\nFor translation, evaluation via BLEU can be done by taking the output from \nbeam.lua\n and using the\n\nmulti-bleu.perl\n script from \nMoses\n. For example\n\n\nperl multi-bleu.perl gold.txt \n pred.txt\n\n\n\n\n\n\n\n\n\n\nEvaluation of States and Attention\n\n\nattention_extraction.lua can be used to extract the attention and the LSTM states. It uses the following (required) options:\n\n\n\n\nmodel\n: Path to model .t7 file.\n\n\nsrc_file\n: Source sequence to decode (one line per sequence).\n\n\ntarg_file\n: True target sequence.\n\n\nsrc_dict\n: Path to source vocabulary (\n*.src.dict\n file from \npreprocess.py\n).\n\n\ntarg_dict\n: Path to target vocabulary (\n*.targ.dict\n file from \npreprocess.py\n).\n\n\n\n\nOutput of the script are two files, \nencoder.hdf5\n and \ndecoder.hdf5\n. The encoder contains the states for every layer of the encoder LSTM and the offsets for the start of each source sentence. The decoder contains the states for the decoder LSTM layers and the offsets for the start of gold sentence. It additionally contains the attention for each time step (if the model uses attention).\n\n\n\n\n\n\n\n\nPre-trained models\n\n\nWe've uploaded English \n-\n German models trained on 4 million sentences from\n\nWorkshop on Machine Translation 2015\n.\nDownload link is below:\n\n\nhttps://drive.google.com/open?id=0BzhmYioWLRn_aEVnd0ZNcWd0Y2c\n\n\nThese models are 4-layer LSTMs with 1000 hidden units and essentially replicates the results from\n\nEffective Approaches to Attention-based\nNeural Machine Translation\n,\nLuong et al. EMNLP 2015.\n\n\n\n\n\n\n\n\nAcknowledgments\n\n\nOur implementation utilizes code from the following:\n\n \nAndrej Karpathy's char-rnn repo\n\n\n \nWojciech Zaremba's lstm repo\n\n* \nElement rnn library\n\n\n\n\n\n\n\n\nLicence\n\n\nMIT\n\n\n\n\n\n\n\nOpenNMT.Beam\n\n\nClass for managing the beam search process. \n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Beam(size)\n\n\nConstructor\n\n\nParameters:\n  * \nsize\n : The beam \nK\n.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Beam:get_current_state()\n\n\nGet the outputs for the current timestep.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Beam:get_current_origin()\n\n\nGet the backpointers for the current timestep.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Beam:advance(out, attn_out)\n\n\nGiven prob over words for every last beam \nout\n and attention\n \nattn_out\n. Compute and update the beam search.\n\n\nParameters:\n  * \nout\n- probs at the last step\n  * \nattn_out\n- attention at the last step\n\n\nReturns: true if beam search is complete.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Beam:get_hyp(k)\n\n\nWalk back to construct the full hypothesis \nk\n.\n\n\nParameters:\n  * \nk\n - the position in the beam to construct.\n\n\nReturns:\n  1. The hypothesis\n  2. The attention at each time step.\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Beam:sort_best()\n\n\n\n * \nOpenNMT.Beam:get_best()\n\n\n\n\n\n\n\n\nOpenNMT.file_reader\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.file_reader(filename)\n\n\n\n * \nOpenNMT.file_reader:next()\n\n\n\n * \nOpenNMT.file_reader:close()\n\n\n\n\n\n\n\n\nOpenNMT.Model\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Model()\n\n\n\n * \nOpenNMT.Model:double()\n\n\n\n * \nOpenNMT.Model:float()\n\n\n\n * \nOpenNMT.Model:cuda()\n\n\n\n\n\n\n\n\nOpenNMT.Generator\n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Generator(args, network)\n\n\n\n * \nOpenNMT.Generator:forward_one(input)\n\n\n\n * \nOpenNMT.Generator:training()\n\n\n\n * \nOpenNMT.Generator:evaluate()\n\n\n\n * \nOpenNMT.Generator:convert(f)\n\n\n\n\n\n\n\n\nOpenNMT.Checkpoint\n\n\nClass for saving and loading models during training.\n\n\n[src]\n\n\n\n\n\n\n\n\nOpenNMT.Checkpoint:save_iteration(iteration, epoch_state, batch_order)\n\n\nSave the model and data in the middle of an epoch sorting the iteration. \n\n\n\n\n\n\nUndocumented methods\n\n\n\n * \nOpenNMT.Checkpoint(args)\n\n\n\n * \nOpenNMT.Checkpoint:save(file_path, info)\n\n\n\n * \nOpenNMT.Checkpoint:save_epoch(valid_ppl, epoch_state)",
            "title": "Init"
        },
        {
            "location": "/nmt/init/#style-guide",
            "text": "",
            "title": "Style Guide"
        },
        {
            "location": "/nmt/init/#comments",
            "text": "Comments should follow:\nhttps://github.com/deepmind/torch-dokx/blob/master/doc/usage.md    All non-private method should have dokx comments describing input/output.      All classes should have a class docstring at the top of the file.     All comments should be on their own line, and be a complete English\nsentence with capitalization.    Use torch-dokx and this command to build docs    dokx-build-package-docs -o docs .\ngoogle-chrome doc/index.html",
            "title": "Comments"
        },
        {
            "location": "/nmt/init/#style",
            "text": "Please run and correct all warnings from luacheck before sending a pull request.     luacheck *    All indentation should be 2 spaces.\nThe MIT License (MIT)   Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.",
            "title": "Style:"
        },
        {
            "location": "/nmt/init/#sequence-to-sequence-learning-with-attentional-neural-networks",
            "text": "Torch  implementation of a standard sequence-to-sequence model with (optional)\nattention where the encoder-decoder are LSTMs. Encoder can be a bidirectional LSTM.\nAdditionally has the option to use characters\n(instead of input word embeddings) by running a convolutional neural network followed by a highway network  over character embeddings to use as inputs.  The attention model is from Effective Approaches to Attention-based\nNeural Machine Translation ,\nLuong et al. EMNLP 2015. We use the  global-general-attention  model with the input-feeding  approach from the paper. Input-feeding is optional and can be turned off.  The character model is from  Character-Aware Neural\nLanguage Models , Kim et al. AAAI 2016.  There are a lot of additional options on top of the baseline model, mainly thanks to the fantastic folks \nat  SYSTRAN . Specifically, there are functionalities which implement:   Effective Approaches to Attention-based Neural Machine Translation . Luong et al., EMNLP 2015.   Character-based Neural Machine Translation . Costa-Jussa and Fonollosa, ACL 2016.   Compression of Neural Machine Translation Models via Pruning . See et al., COLING 2016.   Sequence-Level Knowledge Distillation . Kim and Rush., EMNLP 2016.   Deep Recurrent Models with Fast Forward Connections for Neural Machine Translation .\nZhou et al, TACL 2016.   Guided Alignment Training for Topic-Aware Neural Machine Translation . Chen et al., arXiv:1607.01628.\n*  Linguistic Input Features Improve Neural Machine Translation . Senrich et al., arXiv:1606.02892  See below for more details on how to use them.  This project is maintained by  Yoon Kim .\nFeel free to post any questions/issues on the issues page.",
            "title": "Sequence-to-Sequence Learning with Attentional Neural Networks"
        },
        {
            "location": "/nmt/init/#dependencies",
            "text": "",
            "title": "Dependencies"
        },
        {
            "location": "/nmt/init/#lua",
            "text": "You will need the following packages:  nn  nngraph  GPU usage will additionally require:  cutorch  cunn  If running the character model, you should also install:  cudnn  luautf8",
            "title": "Lua"
        },
        {
            "location": "/nmt/init/#quickstart",
            "text": "We are going to be working with some example data in  data/  folder.\nFirst run the data-processing code  th preprocess.lua -train_src_file data/src-train.txt -train_targ_file data/targ-train.txt\n    -valid_src_file data/src-val.txt -valid_targ_file data/targ-val.txt -output_file data/demo  This will take the source/target train/valid files ( src-train.txt, targ-train.txt,\nsrc-val.txt, targ-val.txt ) and build the following files:   demo.src.dict : Dictionary of source vocab to index mappings.  demo.targ.dict : Dictionary of target vocab to index mappings.  demo-train.t7 : serialized torch file containing vocabulary, training and validation data   The  *.dict  files are needed to check vocabulary, or to preprocess data with fixed vocabularies  Now run the model  th train.lua -data_file data/demo-train.t7 -savefile demo-model  This will run the default model, which consists of a 2-layer LSTM with 500 hidden units\non both the encoder/decoder.\nYou can also add  -gpuid 1  to use (say) GPU 1 in the cluster.  Now you have a model which you can use to predict on new data. To do this we are\ngoing to be running beam search  th evaluate.lua -model demo-model_final.t7 -src_file data/src-val.txt -output_file pred.txt\n-src_dict data/demo.src.dict -targ_dict data/demo.targ.dict  This will output predictions into  pred.txt . The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for  translation \nor  summarization .",
            "title": "Quickstart"
        },
        {
            "location": "/nmt/init/#details",
            "text": "",
            "title": "Details"
        },
        {
            "location": "/nmt/init/#preprocessing-options-preprocesspy",
            "text": "srcvocabsize, targetvocabsize : Size of source/target vocabularies. This is constructed\nby taking the top X most frequent words. Rest are replaced with special UNK tokens.  srcfile, targetfile : Path to source/target training data, where each line represents a single\nsource/target sequence.  srcvalfile, targetvalfile : Path to source/target validation data.  batchsize : Size of each mini-batch.  seqlength : Maximum sequence length (sequences longer than this are dropped).  outputfile : Prefix of the output file names.  maxwordlength : For the character models, words are truncated (if longer than maxwordlength)\nor zero-padded (if shorter) to  maxwordlength .  chars : If 1, construct the character-level dataset as well.  This might take up a lot of space\ndepending on your data size, so you may want to break up the training data into different shards.  srcvocabfile, targetvocabfile : If working with a preset vocab, then including these paths\nwill ignore the  srcvocabsize,targetvocabsize .  unkfilter : Ignore sentences with too many UNK tokens. Can be an absolute count limit (if   1)\nor a proportional limit (0   unkfilter   1).  shuffle : Shuffle sentences.  alignfile ,  alignvalfile : If provided with filenames that contain 'Pharaoh' format alignment\non the train and validation data, source-to-target alignments are stored in the dataset.",
            "title": "Preprocessing options (preprocess.py)"
        },
        {
            "location": "/nmt/init/#training-options-trainlua",
            "text": "Data options   data_file, val_data_file : Path to the training/validation  *.hdf5  files created from running preprocess.py .  savefile : Savefile name (model will be saved as  savefile_epochX_PPL.t7  after every  save_every \nepoch where X is the X-th epoch and PPL is the validation perplexity at the epoch.  num_shards : If the training data has been broken up into different shards,\nthen this is the number of shards.  train_from : If training from a checkpoint then this is the path to the pre-trained model.   Model options   num_layers : Number of layers in the LSTM encoder/decoder (i.e. number of stacks).  rnn_size : Size of LSTM hidden states.  word_vec_size : Word embedding size.  attn :  If = 1, use attention over the source sequence during decoding. If = 0, then it\nuses the last hidden state of the encoder as the context at each time step.  brnn : If = 1, use a bidirectional LSTM on the encoder side. Input embeddings (or CharCNN\nif using characters)  are shared between the forward/backward LSTM, and hidden states of the\ncorresponding forward/backward LSTMs are added to obtain the hidden representation for that\ntime step.  use_chars_enc : If = 1, use characters on the encoder side (as inputs).  use_chars_dec : If = 1, use characters on the decoder side (as inputs).  reverse_src : If = 1, reverse the source sequence. The original sequence-to-sequence paper\nfound that this was crucial to achieving good performance, but with attention models this\ndoes not seem necessary. Recommend leaving it to 0.  init_dec : Initialize the hidden/cell state of the decoder at time 0 to be the last\nhidden/cell state of the encoder. If 0, the initial states of the decoder are set to zero vectors.  input_feed : If = 1, feed the context vector at each time step as additional input (via\nconcatenation with the word embeddings) to the decoder.  multi_attn : If   0, then use a  multi-attention  on this layer of the decoder. For example, if num_layers = 3  and  multi_attn = 2 , then the model will do an attention over the source sequence\non the second layer (and use that as input to the third layer)  and  the penultimate layer.\nWe've found that this did not really improve performance on translation, but may be helpful for\nother tasks where multiple attentional passes over the source sequence are required\n(e.g. for more complex reasoning tasks).  res_net : Use residual connections between LSTM stacks whereby the input to the l-th LSTM\nlayer of the hidden state of the l-1-th LSTM layer summed with hidden state of the l-2th LSTM layer.\nWe didn't find this to really help in our experiments.   Below options only apply if using the character model.   char_vec_size : If using characters, size of the character embeddings.  kernel_width : Size (i.e. width) of the convolutional filter.  num_kernels : Number of convolutional filters (feature maps). So the representation from characters will have this many dimensions.  num_highway_layers : Number of highway layers in the character composition model.   To build a model with guided alignment (implemented similarly to  Guided Alignment Training for Topic-Aware Neural Machine Translation  (Chen et al. 2016)):   guided_alignment : If 1, use external alignments to guide the attention weights   guided_alignment_weight : weight for guided alignment criterion\n*  guided_alignment_decay : decay rate per epoch for alignment weight  Optimization options   epochs : Number of training epochs.  start_epoch : If loading from a checkpoint, the epoch from which to start.  param_init : Parameters of the model are initialized over a uniform distribution with support (-param_init, param_init) .  optim : Optimization method, possible choices are 'sgd', 'adagrad', 'adadelta', 'adam'.\nFor seq2seq I've found vanilla SGD to work well but feel free to experiment.  learning_rate : Starting learning rate. For 'adagrad', 'adadelta', and 'adam', this is the global\nlearning rate. Recommended settings vary based on  optim : sgd ( learning_rate = 1 ), adagrad\n( learning_rate = 0.1 ), adadelta ( learning_rate = 1 ), adam ( learning_rate = 0.1 ).  max_grad_norm : If the norm of the gradient vector exceeds this, renormalize to have its norm equal to  max_grad_norm .  dropout : Dropout probability. Dropout is applied between vertical LSTM stacks.  lr_decay : Decay learning rate by this much if (i) perplexity does not decrease on the validation\nset or (ii) epoch has gone past the  start_decay_at  epoch limit.  start_decay_at : Start decay after this epoch.  curriculum : For this many epochs, order the minibatches based on source sequence length. (Sometimes setting this to 1 will increase convergence speed).  feature_embeddings_dim_exponent : If the additional feature takes  N  values, then the embbeding dimension will be set to  N^exponent .  pre_word_vecs_enc : If using pretrained word embeddings (on the encoder side), this is the\npath to the file with the embeddings. The file should be a serialized Torch tensor with dimensions\nvocab size by embedding size. Each row should be a word embedding and follow the same indexing\nscheme as the *.dict files from running  preprocess.lua . In order to be consistent with  beam.lua ,\nthe first 4 indices should always be  blank ,  unk ,  s ,  /s  tokens.  pre_word_vecs_dec : Path to the file for pretrained word embeddings on the decoder side. See above.  fix_word_vecs_enc : If = 1, fix word embeddings on the encoder side.  fix_word_vecs_dec : If = 1, fix word embeddings on the decoder side.  max_batch_l : Batch size used to create the data in  preprocess.py . If this is left blank\n(recommended), then the batch size will be inferred from the validation set.   Other options   start_symbol : Use special start-of-sentence and end-of-sentence tokens on the source side.\nWe've found this to make minimal difference.  gpuid : Which GPU to use (-1 = use cpu).  gpuid2 : If this is  =0, then the model will use two GPUs whereby the encoder is on the first\nGPU and the decoder is on the second GPU. This will allow you to train bigger models.  cudnn : Whether to use cudnn or not for convolutions (for the character model).  cudnn \nhas much faster convolutions so this is highly recommended if using the character model.  save_every : Save every this many epochs.  print_every : Print various stats after this many batches.  seed : Change the random seed for random numbers in torch - use that option to train alternate models for ensemble  prealloc : when set to 1 (default), enable memory preallocation and sharing between clones - this reduces by a lot the used memory - there should not be\nany situation where you don't need it. Also - since memory is preallocated, there is not (major)\nmemory increase during the training. When set to 0, it rolls back to original memory optimization.",
            "title": "Training options (train.lua)"
        },
        {
            "location": "/nmt/init/#decoding-options-beamlua",
            "text": "model : Path to model .t7 file.  src_file : Source sequence to decode (one line per sequence).  targ_file : True target sequence (optional).  output_file : Path to output the predictions (each line will be the decoded sequence).  src_dict : Path to source vocabulary ( *.src.dict  file from  preprocess.py ).  targ_dict : Path to target vocabulary ( *.targ.dict  file from  preprocess.py ).  feature_dict_prefix : Prefix of the path to the features vocabularies ( *.feature_N.dict  files from  preprocess.py ).  char_dict : Path to character vocabulary ( *.char.dict  file from  preprocess.py ).  beam : Beam size (recommend keeping this at 5).  max_sent_l : Maximum sentence length. If any of the sequences in  srcfile  are longer than this\nit will error out.  simple : If = 1, output prediction is simply the first time the top of the beam\nends with an end-of-sentence token. If = 0, the model considers all hypotheses that have\nbeen generated so far that ends with end-of-sentence token and takes the highest scoring\nof all of them.  replace_unk : Replace the generated UNK tokens with the source token that had the highest\nattention weight. If  srctarg_dict  is provided, it will lookup the identified source token\nand give the corresponding target token. If it is not provided (or the identified source token\ndoes not exist in the table) then it will copy the source token.  srctarg_dict : Path to source-target dictionary to replace UNK tokens. Each line should be a\nsource token and its corresponding target token, separated by  ||| . For example   hello|||hallo\nukraine|||ukrainische  This dictionary can be obtained by, for example, running an alignment model as a preprocessing step.\nWe recommend  fast_align .   score_gold : If = 1, score the true target output as well.   n_best : If   1, then it will also output an n_best list of decoded sentences in the following\nformat.  1 ||| sentence_1 ||| sentence_1_score\n2 ||| sentence_2 ||| sentence_2_score   gpuid : ID of the GPU to use (-1 = use CPU).  gpuid2 : ID if the second GPU (if specified).  cudnn : If the model was trained with  cudnn , then this should be set to 1 (otherwise the model\nwill fail to load).  rescore : when set to scorer name, use scorer to find hypothesis with highest score - available 'bleu', 'gleu'  rescore_param : parameter to rescorer - for bleu/gleu ngram length",
            "title": "Decoding options (beam.lua)"
        },
        {
            "location": "/nmt/init/#using-additional-input-features",
            "text": "Linguistic Input Features Improve Neural Machine Translation  (Senrich et al. 2016) shows that translation performance can be increased by using additional input features.  Similarly to this work, you can annotate each word in the  source  text by using the  -|-  separator:  word1-|-feat1-|-feat2 word2-|-feat1-|-feat2  It supports an arbitrary number of features with arbitrary labels. However, all input words must have the  same  number of annotations. See for example  data/src-train-case.txt  which annotates each word with the case information.  To evaluate the model, the option  -feature_dict_prefix  is required on  evaluate.lua  which points to the prefix of the features dictionnaries generated during the preprocessing.",
            "title": "Using additional input features"
        },
        {
            "location": "/nmt/init/#pruning-a-model",
            "text": "Compression of Neural Machine Translation Models via Pruning  (See et al. 2016) shows that a model can be aggressively pruned while keeping the same performace.  To prune a model - you can use  prune.lua  which implement class-bind, and class-uniform pruning technique from the paper.   model : the model to prune  savefile : name of the pruned model  gpuid : Which gpu to use. -1 = use CPU. Depends if the model is serialized for GPU or CPU  ratio : pruning rate  prune : pruning technique  blind  or  uniform , by default  blind   note that the pruning cut connection with lowest weight in the linear models by using a boolean mask. The size of the file is a little larger since it stores the actual full matrix and the binary mask.  Models can be retrained - typically you can recover full capacity of a model pruned at 60% or even 80% by few epochs of additional trainings.",
            "title": "Pruning a model"
        },
        {
            "location": "/nmt/init/#switching-between-gpucpu-models",
            "text": "By default, the model will always save the final model as a CPU model, but it will save the\nintermediate models as a CPU/GPU model depending on how you specified  -gpuid .\nIf you want to run beam search on the CPU with an intermediate model trained on the GPU,\nyou can use  convert_to_cpu.lua  to convert the model to CPU and run beam search.",
            "title": "Switching between GPU/CPU models"
        },
        {
            "location": "/nmt/init/#gpu-memory-requirementstraining-speed",
            "text": "Training large sequence-to-sequence models can be memory-intensive. Memory requirements will\ndependent on batch size, maximum sequence length, vocabulary size, and (obviously) model size.\nHere are some benchmark numbers on a GeForce GTX Titan X.\n(assuming batch size of 64, maximum sequence length of 50 on both the source/target sequence,\nvocabulary size of 50000, and word embedding size equal to rnn size):  ( prealloc = 0 )  1-layer, 100 hidden units: 0.7G, 21.5K tokens/sec  1-layer, 250 hidden units: 1.4G, 14.1K tokens/sec  1-layer, 500 hidden units: 2.6G, 9.4K tokens/sec  2-layers, 500 hidden units: 3.2G, 7.4K tokens/sec\n* 4-layers, 1000 hidden units: 9.4G, 2.5K tokens/sec  Thanks to some fantastic work from folks at  SYSTRAN , turning  prealloc  on\nwill lead to much more memory efficient training  ( prealloc = 1 )  1-layer, 100 hidden units: 0.5G, 22.4K tokens/sec  1-layer, 250 hidden units: 1.1G, 14.5K tokens/sec  1-layer, 500 hidden units: 2.1G, 10.0K tokens/sec  2-layers, 500 hidden units: 2.3G, 8.2K tokens/sec\n* 4-layers, 1000 hidden units: 6.4G, 3.3K tokens/sec  Tokens/sec refers to total (i.e. source + target) tokens processed per second.\nIf using different batch sizes/sequence length, you should (linearly) scale\nthe above numbers accordingly. You can make use of memory on multiple GPUs by using -gpuid2  option in  train.lua . This will put the encoder on the GPU specified by -gpuid , and the decoder on the GPU specified by  -gpuid2 .",
            "title": "GPU memory requirements/Training speed"
        },
        {
            "location": "/nmt/init/#evaluation",
            "text": "For translation, evaluation via BLEU can be done by taking the output from  beam.lua  and using the multi-bleu.perl  script from  Moses . For example  perl multi-bleu.perl gold.txt   pred.txt",
            "title": "Evaluation"
        },
        {
            "location": "/nmt/init/#evaluation-of-states-and-attention",
            "text": "attention_extraction.lua can be used to extract the attention and the LSTM states. It uses the following (required) options:   model : Path to model .t7 file.  src_file : Source sequence to decode (one line per sequence).  targ_file : True target sequence.  src_dict : Path to source vocabulary ( *.src.dict  file from  preprocess.py ).  targ_dict : Path to target vocabulary ( *.targ.dict  file from  preprocess.py ).   Output of the script are two files,  encoder.hdf5  and  decoder.hdf5 . The encoder contains the states for every layer of the encoder LSTM and the offsets for the start of each source sentence. The decoder contains the states for the decoder LSTM layers and the offsets for the start of gold sentence. It additionally contains the attention for each time step (if the model uses attention).",
            "title": "Evaluation of States and Attention"
        },
        {
            "location": "/nmt/init/#pre-trained-models",
            "text": "We've uploaded English  -  German models trained on 4 million sentences from Workshop on Machine Translation 2015 .\nDownload link is below:  https://drive.google.com/open?id=0BzhmYioWLRn_aEVnd0ZNcWd0Y2c  These models are 4-layer LSTMs with 1000 hidden units and essentially replicates the results from Effective Approaches to Attention-based\nNeural Machine Translation ,\nLuong et al. EMNLP 2015.",
            "title": "Pre-trained models"
        },
        {
            "location": "/nmt/init/#acknowledgments",
            "text": "Our implementation utilizes code from the following:   Andrej Karpathy's char-rnn repo    Wojciech Zaremba's lstm repo \n*  Element rnn library",
            "title": "Acknowledgments"
        },
        {
            "location": "/nmt/init/#licence",
            "text": "MIT",
            "title": "Licence"
        },
        {
            "location": "/nmt/init/#highlightjs",
            "text": "Highlight.js highlights syntax in code examples on blogs, forums and,\nin fact, on any web page. It's very easy to use because it works\nautomatically: finds blocks of code, detects a language, highlights it.  Autodetection can be fine tuned when it fails by itself (see \"Heuristics\").",
            "title": "Highlight.js"
        },
        {
            "location": "/nmt/init/#basic-usage",
            "text": "Link the library and a stylesheet from your page and hook highlighting to\nthe page load event:  link rel= stylesheet  href= styles/default.css  script src= highlight.pack.js /script  script hljs.initHighlightingOnLoad(); /script   This will highlight all code on the page marked up as  pre code  ..  /code /pre .\nIf you use different markup or need to apply highlighting dynamically, read\n\"Custom initialization\" below.    You can download your own customized version of \"highlight.pack.js\" or\n  use the hosted one as described on the download page:\n   http://highlightjs.org/download/    Style themes are available in the download package or as hosted files.\n  To create a custom style for your site see the class reference in the file\n   classref.txt  from the downloaded package.",
            "title": "Basic usage"
        },
        {
            "location": "/nmt/init/#nodejs",
            "text": "Highlight.js can be used under node.js. The package with all supported languages is\ninstallable from NPM:  npm install highlight.js  Alternatively, you can build it from the source with only languages you need:  python3 tools/build.py -tnode lang1 lang2 ..  Using the library:  var hljs = require('highlight.js');\n\n// If you know the language\nhljs.highlight(lang, code).value;\n\n// Automatic language detection\nhljs.highlightAuto(code).value;",
            "title": "node.js"
        },
        {
            "location": "/nmt/init/#amd",
            "text": "Highlight.js can be used with an AMD loader.  You will need to build it from\nsource in order to do so:  $ python3 tools/build.py -tamd lang1 lang2 ..  Which will generate a  build/highlight.pack.js  which will load as an AMD\nmodule with support for the built languages and can be used like so:  require([ highlight.js/build/highlight.pack ], function(hljs){\n\n  // If you know the language\n  hljs.highlight(lang, code).value;\n\n  // Automatic language detection\n  hljs.highlightAuto(code).value;\n});",
            "title": "AMD"
        },
        {
            "location": "/nmt/init/#tab-replacement",
            "text": "You can replace TAB ('\\x09') characters used for indentation in your code\nwith some fixed number of spaces or with a  span  to give them special\nstyling:  script type= text/javascript \n  hljs.tabReplace = '    '; // 4 spaces\n  // ... or\n  hljs.tabReplace = ' span class= indent \\t /span ';\n\n  hljs.initHighlightingOnLoad(); /script",
            "title": "Tab replacement"
        },
        {
            "location": "/nmt/init/#custom-initialization",
            "text": "If you use different markup for code blocks you can initialize them manually\nwith  highlightBlock(code, tabReplace, useBR)  function. It takes a DOM element\ncontaining the code to highlight and optionally a string with which to replace\nTAB characters.  Initialization using, for example, jQuery might look like this:  $(document).ready(function() {\n  $('pre code').each(function(i, e) {hljs.highlightBlock(e)});\n});  You can use  highlightBlock  to highlight blocks dynamically inserted into\nthe page. Just make sure you don't do it twice for already highlighted\nblocks.  If your code container relies on  br  tags instead of line breaks (i.e. if\nit's not  pre ) pass  true  into the third parameter of  highlightBlock \nto make highlight.js use  br  in the output:  $('div.code').each(function(i, e) {hljs.highlightBlock(e, null, true)});",
            "title": "Custom initialization"
        },
        {
            "location": "/nmt/init/#heuristics",
            "text": "Autodetection of a code's language is done using a simple heuristic:\nthe program tries to highlight a fragment with all available languages and\ncounts all syntactic structures that it finds along the way. The language\nwith greatest count wins.  This means that in short fragments the probability of an error is high\n(and it really happens sometimes). In this cases you can set the fragment's\nlanguage explicitly by assigning a class to the  code  element:  pre code class= html ... /code /pre   You can use class names recommended in HTML5: \"language-html\",\n\"language-php\". Classes also can be assigned to the  pre  element.  To disable highlighting of a fragment altogether use \"no-highlight\" class:  pre code class= no-highlight ... /code /pre",
            "title": "Heuristics"
        },
        {
            "location": "/nmt/init/#export",
            "text": "File export.html contains a little program that allows you to paste in a code\nsnippet and then copy and paste the resulting HTML code generated by the\nhighlighter. This is useful in situations when you can't use the script itself\non a site.",
            "title": "Export"
        },
        {
            "location": "/nmt/init/#meta",
            "text": "Version: 7.5  URL:     http://highlightjs.org/   For the license terms see LICENSE files.\nFor authors and contributors see AUTHORS.en.txt file.",
            "title": "Meta"
        },
        {
            "location": "/nmt/init/#opennmtdict",
            "text": "",
            "title": "OpenNMT.dict"
        },
        {
            "location": "/nmt/init/#undocumented-methods",
            "text": "*  OpenNMT.dict(data)  \n *  OpenNMT.dict:load_file(filename)  \n *  OpenNMT.dict:write_file(filename)  \n *  OpenNMT.dict:lookup(key)  \n *  OpenNMT.dict:set_special(special)  \n *  OpenNMT.dict:add_special(label, idx)  \n *  OpenNMT.dict:add_specials(labels)  \n *  OpenNMT.dict:add(label, idx)  \n *  OpenNMT.dict:prune(size)  \n *  OpenNMT.dict:convert_to_idx(labels, start_symbols)  \n *  OpenNMT.dict:convert_to_labels(idx, stop)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtencoder",
            "text": "Encoder is a unidirectional Sequencer used for the source language.   [src]",
            "title": "OpenNMT.Encoder"
        },
        {
            "location": "/nmt/init/#opennmtencoderargs-network",
            "text": "Constructor takes global  args  and optional  network .   [src]",
            "title": "OpenNMT.Encoder(args, network)"
        },
        {
            "location": "/nmt/init/#opennmtencoderresize_protobatch_size",
            "text": "Call to change the  batch_size .   [src]",
            "title": "OpenNMT.Encoder:resize_proto(batch_size)"
        },
        {
            "location": "/nmt/init/#opennmtencoderforwardbatch",
            "text": "Compute the context representation of an input.  Parameters:   batch  - a  batch struct  as defined data.lua.   Returns:     last hidden states      context matrix H     TODO:   Change  batch  to  input .   [src]",
            "title": "OpenNMT.Encoder:forward(batch)"
        },
        {
            "location": "/nmt/init/#opennmtencoderbackwardbatch-grad_states_output-grad_context_output",
            "text": "Backward pass (only called during training)  Parameters:   batch  - must be same as for forward  grad_states_output  grad_context_output  - gradient of loss\n      wrt last states and context.   TODO: change this to (input, gradOutput) as in nngraph.",
            "title": "OpenNMT.Encoder:backward(batch, grad_states_output, grad_context_output)"
        },
        {
            "location": "/nmt/init/#undocumented-methods_1",
            "text": "*  OpenNMT.Encoder:convert(f)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtbiencoder",
            "text": "BiEncoder is a bidirectional Sequencer used for the source language.   [src]",
            "title": "OpenNMT.BiEncoder"
        },
        {
            "location": "/nmt/init/#opennmtbiencoderargs-merge-net_fwd-net_bwd",
            "text": "Creates two Encoder's (encoder.lua)  net_fwd  and  net_bwd .\n  The two are combined use  merge  operation (concat/sum).",
            "title": "OpenNMT.BiEncoder(args, merge, net_fwd, net_bwd)"
        },
        {
            "location": "/nmt/init/#undocumented-methods_2",
            "text": "*  OpenNMT.BiEncoder:resize_proto(batch_size)  \n *  OpenNMT.BiEncoder:forward(batch)  \n *  OpenNMT.BiEncoder:backward(batch, grad_states_output, grad_context_output)  \n *  OpenNMT.BiEncoder:training()  \n *  OpenNMT.BiEncoder:evaluate()  \n *  OpenNMT.BiEncoder:convert(f)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#undocumented-methods_3",
            "text": "*  OpenNMT.Cuda.init(opt)  \n *  OpenNMT.Cuda.convert(obj)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtepochstate",
            "text": "Class for managing the training process by logging and storing\n  the state of the current epoch.  [src]",
            "title": "OpenNMT.EpochState"
        },
        {
            "location": "/nmt/init/#opennmtepochstateepoch-status",
            "text": "Initialize for epoch  epoch  and training  status  (current loss)  [src]",
            "title": "OpenNMT.EpochState(epoch, status)"
        },
        {
            "location": "/nmt/init/#opennmtepochstateupdatebatch-loss",
            "text": "Update training status. Takes  batch  (described in data.lua) and last loss.  [src]",
            "title": "OpenNMT.EpochState:update(batch, loss)"
        },
        {
            "location": "/nmt/init/#opennmtepochstatelogbatch_index-data_size-learning_rate",
            "text": "Log to status stdout.\n  TODO: these args shouldn't need to be passed in each time.",
            "title": "OpenNMT.EpochState:log(batch_index, data_size, learning_rate)"
        },
        {
            "location": "/nmt/init/#undocumented-methods_4",
            "text": "*  OpenNMT.EpochState:get_train_ppl()  \n *  OpenNMT.EpochState:get_time()  \n *  OpenNMT.EpochState:get_status()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#undocumented-methods_5",
            "text": "*  OpenNMT.Memory.optimize(model, batch)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#style-guide_1",
            "text": "",
            "title": "Style Guide"
        },
        {
            "location": "/nmt/init/#comments_1",
            "text": "Comments should follow:\nhttps://github.com/deepmind/torch-dokx/blob/master/doc/usage.md    All non-private method should have dokx comments describing input/output.      All classes should have a class docstring at the top of the file.     All comments should be on their own line, and be a complete English\nsentence with capitalization.    Use torch-dokx and this command to build docs    dokx-build-package-docs -o docs .\ngoogle-chrome doc/index.html",
            "title": "Comments"
        },
        {
            "location": "/nmt/init/#style_1",
            "text": "Please run and correct all warnings from luacheck before sending a pull request.     luacheck *    All indentation should be 2 spaces.",
            "title": "Style:"
        },
        {
            "location": "/nmt/init/#opennmtdecoder",
            "text": "Decoder is the sequencer for the target words.  [src]",
            "title": "OpenNMT.Decoder"
        },
        {
            "location": "/nmt/init/#opennmtdecoderresize_protobatch_size",
            "text": "Call to change the  batch_size .  TODO: rename.  [src]",
            "title": "OpenNMT.Decoder:resize_proto(batch_size)"
        },
        {
            "location": "/nmt/init/#opennmtdecoderresetsource_sizes-source_length-beam_size",
            "text": "Update internals to prepare for new batch.  [src]",
            "title": "OpenNMT.Decoder:reset(source_sizes, source_length, beam_size)"
        },
        {
            "location": "/nmt/init/#opennmtdecoderforward_oneinput-prev_states-context-prev_out-t",
            "text": "Run one step of the decoder.  Parameters:\n *  input  - sparse input (1)\n *  prev_states  - stack of hidden states (batch x layers*model x rnn_size)\n *  context  - encoder output (batch x n x rnn_size)\n *  prev_out  - previous distribution (batch x #words)\n *  t  - current timestep  Returns:\n 1.  out  - Top-layer Hidden state\n 2.  states  - All states  [src]",
            "title": "OpenNMT.Decoder:forward_one(input, prev_states, context, prev_out, t)"
        },
        {
            "location": "/nmt/init/#opennmtdecoderforward_and_applybatch-encoder_states-context-func",
            "text": "Compute all forward steps.  Parameters:\n  *  batch  - based on data.lua\n  *  encoder_states \n  *  context \n  *  func  - Calls  func(out, t)  each timestep.  [src]",
            "title": "OpenNMT.Decoder:forward_and_apply(batch, encoder_states, context, func)"
        },
        {
            "location": "/nmt/init/#opennmtdecoderforwardbatch-encoder_states-context",
            "text": "Compute all forward steps.  Parameters:\n  *  batch  - based on data.lua\n  *  encoder_states \n  *  context  Returns:\n  1.  outputs  - Top Hidden layer at each time-step.  [src]",
            "title": "OpenNMT.Decoder:forward(batch, encoder_states, context)"
        },
        {
            "location": "/nmt/init/#opennmtdecodercompute_lossbatch-encoder_states-context-generator",
            "text": "Compute the loss on a batch based on final layer  generator .  [src]",
            "title": "OpenNMT.Decoder:compute_loss(batch, encoder_states, context, generator)"
        },
        {
            "location": "/nmt/init/#opennmtdecoderbackwardbatch-outputs-generator",
            "text": "Compute the standard backward update.\n  With input  batch , target  outputs , and  generator \n  Note: This code is both the standard backward and criterion forward/backward.\n  It returns both the gradInputs (ret 1 and 2) and the loss.  TODO: This object should own  generator  and or, generator should\n  control its own backward pass.",
            "title": "OpenNMT.Decoder:backward(batch, outputs, generator)"
        },
        {
            "location": "/nmt/init/#undocumented-methods_6",
            "text": "*  OpenNMT.Decoder(args, network)  \n *  OpenNMT.Decoder:compute_score(batch, encoder_states, context, generator)  \n *  OpenNMT.Decoder:convert(f) \nThe MIT License (MIT)  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtsequencer",
            "text": "Sequencer is the base class for our time series LSTM models.\n  Acts similarly to an  nn.Module .\n   Main task is to manage  self.network_clones , the unrolled LSTM\n  used during training.\n  Classes encoder/decoder/biencoder generalize these definitions.  [src]",
            "title": "OpenNMT.Sequencer"
        },
        {
            "location": "/nmt/init/#opennmtsequencermodel-args-network",
            "text": "Constructor  Parameters:\n  *  model  - type of model (enc,dec)\n  *  args  - global arguments\n  *  network  - optional preconstructed network.  TODO: Should initialize all the members in this method.\n   i.e. word_vecs, fix_word_vecs, network_clones, eval_mode, etc.  [src]",
            "title": "OpenNMT.Sequencer(model, args, network)"
        },
        {
            "location": "/nmt/init/#opennmtsequencertraining",
            "text": "Tell the network to prepare for training mode.   [src]",
            "title": "OpenNMT.Sequencer:training()"
        },
        {
            "location": "/nmt/init/#opennmtsequencerevaluate",
            "text": "Tell the network to prepare for evaluation mode.",
            "title": "OpenNMT.Sequencer:evaluate()"
        },
        {
            "location": "/nmt/init/#undocumented-methods_7",
            "text": "*  OpenNMT.Sequencer:resize_proto(batch_size)  \n *  OpenNMT.Sequencer:backward_word_vecs()  \n *  OpenNMT.Sequencer:net(t)  \n *  OpenNMT.Sequencer:convert(f)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtoptim",
            "text": "[src]",
            "title": "OpenNMT.Optim"
        },
        {
            "location": "/nmt/init/#opennmtoptimupdate_learning_ratescore-epoch",
            "text": "decay learning rate if val perf does not improve or we hit the start_decay_at limit",
            "title": "OpenNMT.Optim:update_learning_rate(score, epoch)"
        },
        {
            "location": "/nmt/init/#undocumented-methods_8",
            "text": "*  OpenNMT.Optim(args)  \n *  OpenNMT.Optim:update_params(params, grad_params, max_grad_norm)  \n *  OpenNMT.Optim:get_learning_rate()  \n *  OpenNMT.Optim:get_states()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtdata",
            "text": "Data management and batch creation.  Batch interface [size]:    size: number of sentences in the batch [1]  source_length: max length in source batch [1]  source_size:  lengths of each source [batch x 1]  source_input:  left-padded idx's of source (PPPPPPABCDE) [batch x max]  source_input_rev: right-padded  idx's of source rev (EDCBAPPPPPP) [batch x max]  target_length: max length in source batch [1]  target_size: lengths of each source [batch x 1]  target_non_zeros: number of non-ignored words in batch [1]  target_input: input idx's of target (SABCDEPPPPPP) [batch x max]  target_output: expected output idx's of target (ABCDESPPPPPP) [batch x max]   TODO: change name of size =  maxlen  [src]",
            "title": "OpenNMT.Data"
        },
        {
            "location": "/nmt/init/#opennmtdatasrc-targ",
            "text": "Initialize a data object given aligned tables of IntTensors  src \n  and  targ .  [src]",
            "title": "OpenNMT.Data(src, targ)"
        },
        {
            "location": "/nmt/init/#opennmtdataset_batch_sizemax_batch_size",
            "text": "Setup up the training data to respect  max_batch_size .   [src]",
            "title": "OpenNMT.Data:set_batch_size(max_batch_size)"
        },
        {
            "location": "/nmt/init/#opennmtdataget_datasrc-targ",
            "text": "Create a batch object given aligned sent tables  src  and  targ \n  (optional). Data format is shown at the top of the file.  [src]",
            "title": "OpenNMT.Data:get_data(src, targ)"
        },
        {
            "location": "/nmt/init/#opennmtdataget_batchidx",
            "text": "Get batch  idx . If nil make a batch of all the data.",
            "title": "OpenNMT.Data:get_batch(idx)"
        },
        {
            "location": "/nmt/init/#sequence-to-sequence-learning-with-attentional-neural-networks_1",
            "text": "Torch  implementation of a standard sequence-to-sequence model with (optional)\nattention where the encoder-decoder are LSTMs. Encoder can be a bidirectional LSTM.\nAdditionally has the option to use characters\n(instead of input word embeddings) by running a convolutional neural network followed by a highway network  over character embeddings to use as inputs.  The attention model is from Effective Approaches to Attention-based\nNeural Machine Translation ,\nLuong et al. EMNLP 2015. We use the  global-general-attention  model with the input-feeding  approach from the paper. Input-feeding is optional and can be turned off.  The character model is from  Character-Aware Neural\nLanguage Models , Kim et al. AAAI 2016.  There are a lot of additional options on top of the baseline model, mainly thanks to the fantastic folks \nat  SYSTRAN . Specifically, there are functionalities which implement:   Effective Approaches to Attention-based Neural Machine Translation . Luong et al., EMNLP 2015.   Character-based Neural Machine Translation . Costa-Jussa and Fonollosa, ACL 2016.   Compression of Neural Machine Translation Models via Pruning . See et al., COLING 2016.   Sequence-Level Knowledge Distillation . Kim and Rush., EMNLP 2016.   Deep Recurrent Models with Fast Forward Connections for Neural Machine Translation .\nZhou et al, TACL 2016.   Guided Alignment Training for Topic-Aware Neural Machine Translation . Chen et al., arXiv:1607.01628.\n*  Linguistic Input Features Improve Neural Machine Translation . Senrich et al., arXiv:1606.02892  See below for more details on how to use them.  This project is maintained by  Yoon Kim .\nFeel free to post any questions/issues on the issues page.",
            "title": "Sequence-to-Sequence Learning with Attentional Neural Networks"
        },
        {
            "location": "/nmt/init/#dependencies_1",
            "text": "",
            "title": "Dependencies"
        },
        {
            "location": "/nmt/init/#lua_1",
            "text": "You will need the following packages:  nn  nngraph  GPU usage will additionally require:  cutorch  cunn  If running the character model, you should also install:  cudnn  luautf8",
            "title": "Lua"
        },
        {
            "location": "/nmt/init/#quickstart_1",
            "text": "We are going to be working with some example data in  data/  folder.\nFirst run the data-processing code  th preprocess.lua -train_src_file data/src-train.txt -train_targ_file data/targ-train.txt\n    -valid_src_file data/src-val.txt -valid_targ_file data/targ-val.txt -output_file data/demo  This will take the source/target train/valid files ( src-train.txt, targ-train.txt,\nsrc-val.txt, targ-val.txt ) and build the following files:   demo.src.dict : Dictionary of source vocab to index mappings.  demo.targ.dict : Dictionary of target vocab to index mappings.  demo-train.t7 : serialized torch file containing vocabulary, training and validation data   The  *.dict  files are needed to check vocabulary, or to preprocess data with fixed vocabularies  Now run the model  th train.lua -data_file data/demo-train.t7 -savefile demo-model  This will run the default model, which consists of a 2-layer LSTM with 500 hidden units\non both the encoder/decoder.\nYou can also add  -gpuid 1  to use (say) GPU 1 in the cluster.  Now you have a model which you can use to predict on new data. To do this we are\ngoing to be running beam search  th evaluate.lua -model demo-model_final.t7 -src_file data/src-val.txt -output_file pred.txt\n-src_dict data/demo.src.dict -targ_dict data/demo.targ.dict  This will output predictions into  pred.txt . The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for  translation \nor  summarization .",
            "title": "Quickstart"
        },
        {
            "location": "/nmt/init/#details_1",
            "text": "",
            "title": "Details"
        },
        {
            "location": "/nmt/init/#preprocessing-options-preprocesspy_1",
            "text": "srcvocabsize, targetvocabsize : Size of source/target vocabularies. This is constructed\nby taking the top X most frequent words. Rest are replaced with special UNK tokens.  srcfile, targetfile : Path to source/target training data, where each line represents a single\nsource/target sequence.  srcvalfile, targetvalfile : Path to source/target validation data.  batchsize : Size of each mini-batch.  seqlength : Maximum sequence length (sequences longer than this are dropped).  outputfile : Prefix of the output file names.  maxwordlength : For the character models, words are truncated (if longer than maxwordlength)\nor zero-padded (if shorter) to  maxwordlength .  chars : If 1, construct the character-level dataset as well.  This might take up a lot of space\ndepending on your data size, so you may want to break up the training data into different shards.  srcvocabfile, targetvocabfile : If working with a preset vocab, then including these paths\nwill ignore the  srcvocabsize,targetvocabsize .  unkfilter : Ignore sentences with too many UNK tokens. Can be an absolute count limit (if   1)\nor a proportional limit (0   unkfilter   1).  shuffle : Shuffle sentences.  alignfile ,  alignvalfile : If provided with filenames that contain 'Pharaoh' format alignment\non the train and validation data, source-to-target alignments are stored in the dataset.",
            "title": "Preprocessing options (preprocess.py)"
        },
        {
            "location": "/nmt/init/#training-options-trainlua_1",
            "text": "Data options   data_file, val_data_file : Path to the training/validation  *.hdf5  files created from running preprocess.py .  savefile : Savefile name (model will be saved as  savefile_epochX_PPL.t7  after every  save_every \nepoch where X is the X-th epoch and PPL is the validation perplexity at the epoch.  num_shards : If the training data has been broken up into different shards,\nthen this is the number of shards.  train_from : If training from a checkpoint then this is the path to the pre-trained model.   Model options   num_layers : Number of layers in the LSTM encoder/decoder (i.e. number of stacks).  rnn_size : Size of LSTM hidden states.  word_vec_size : Word embedding size.  attn :  If = 1, use attention over the source sequence during decoding. If = 0, then it\nuses the last hidden state of the encoder as the context at each time step.  brnn : If = 1, use a bidirectional LSTM on the encoder side. Input embeddings (or CharCNN\nif using characters)  are shared between the forward/backward LSTM, and hidden states of the\ncorresponding forward/backward LSTMs are added to obtain the hidden representation for that\ntime step.  use_chars_enc : If = 1, use characters on the encoder side (as inputs).  use_chars_dec : If = 1, use characters on the decoder side (as inputs).  reverse_src : If = 1, reverse the source sequence. The original sequence-to-sequence paper\nfound that this was crucial to achieving good performance, but with attention models this\ndoes not seem necessary. Recommend leaving it to 0.  init_dec : Initialize the hidden/cell state of the decoder at time 0 to be the last\nhidden/cell state of the encoder. If 0, the initial states of the decoder are set to zero vectors.  input_feed : If = 1, feed the context vector at each time step as additional input (via\nconcatenation with the word embeddings) to the decoder.  multi_attn : If   0, then use a  multi-attention  on this layer of the decoder. For example, if num_layers = 3  and  multi_attn = 2 , then the model will do an attention over the source sequence\non the second layer (and use that as input to the third layer)  and  the penultimate layer.\nWe've found that this did not really improve performance on translation, but may be helpful for\nother tasks where multiple attentional passes over the source sequence are required\n(e.g. for more complex reasoning tasks).  res_net : Use residual connections between LSTM stacks whereby the input to the l-th LSTM\nlayer of the hidden state of the l-1-th LSTM layer summed with hidden state of the l-2th LSTM layer.\nWe didn't find this to really help in our experiments.   Below options only apply if using the character model.   char_vec_size : If using characters, size of the character embeddings.  kernel_width : Size (i.e. width) of the convolutional filter.  num_kernels : Number of convolutional filters (feature maps). So the representation from characters will have this many dimensions.  num_highway_layers : Number of highway layers in the character composition model.   To build a model with guided alignment (implemented similarly to  Guided Alignment Training for Topic-Aware Neural Machine Translation  (Chen et al. 2016)):   guided_alignment : If 1, use external alignments to guide the attention weights   guided_alignment_weight : weight for guided alignment criterion\n*  guided_alignment_decay : decay rate per epoch for alignment weight  Optimization options   epochs : Number of training epochs.  start_epoch : If loading from a checkpoint, the epoch from which to start.  param_init : Parameters of the model are initialized over a uniform distribution with support (-param_init, param_init) .  optim : Optimization method, possible choices are 'sgd', 'adagrad', 'adadelta', 'adam'.\nFor seq2seq I've found vanilla SGD to work well but feel free to experiment.  learning_rate : Starting learning rate. For 'adagrad', 'adadelta', and 'adam', this is the global\nlearning rate. Recommended settings vary based on  optim : sgd ( learning_rate = 1 ), adagrad\n( learning_rate = 0.1 ), adadelta ( learning_rate = 1 ), adam ( learning_rate = 0.1 ).  max_grad_norm : If the norm of the gradient vector exceeds this, renormalize to have its norm equal to  max_grad_norm .  dropout : Dropout probability. Dropout is applied between vertical LSTM stacks.  lr_decay : Decay learning rate by this much if (i) perplexity does not decrease on the validation\nset or (ii) epoch has gone past the  start_decay_at  epoch limit.  start_decay_at : Start decay after this epoch.  curriculum : For this many epochs, order the minibatches based on source sequence length. (Sometimes setting this to 1 will increase convergence speed).  feature_embeddings_dim_exponent : If the additional feature takes  N  values, then the embbeding dimension will be set to  N^exponent .  pre_word_vecs_enc : If using pretrained word embeddings (on the encoder side), this is the\npath to the file with the embeddings. The file should be a serialized Torch tensor with dimensions\nvocab size by embedding size. Each row should be a word embedding and follow the same indexing\nscheme as the *.dict files from running  preprocess.lua . In order to be consistent with  beam.lua ,\nthe first 4 indices should always be  blank ,  unk ,  s ,  /s  tokens.  pre_word_vecs_dec : Path to the file for pretrained word embeddings on the decoder side. See above.  fix_word_vecs_enc : If = 1, fix word embeddings on the encoder side.  fix_word_vecs_dec : If = 1, fix word embeddings on the decoder side.  max_batch_l : Batch size used to create the data in  preprocess.py . If this is left blank\n(recommended), then the batch size will be inferred from the validation set.   Other options   start_symbol : Use special start-of-sentence and end-of-sentence tokens on the source side.\nWe've found this to make minimal difference.  gpuid : Which GPU to use (-1 = use cpu).  gpuid2 : If this is  =0, then the model will use two GPUs whereby the encoder is on the first\nGPU and the decoder is on the second GPU. This will allow you to train bigger models.  cudnn : Whether to use cudnn or not for convolutions (for the character model).  cudnn \nhas much faster convolutions so this is highly recommended if using the character model.  save_every : Save every this many epochs.  print_every : Print various stats after this many batches.  seed : Change the random seed for random numbers in torch - use that option to train alternate models for ensemble  prealloc : when set to 1 (default), enable memory preallocation and sharing between clones - this reduces by a lot the used memory - there should not be\nany situation where you don't need it. Also - since memory is preallocated, there is not (major)\nmemory increase during the training. When set to 0, it rolls back to original memory optimization.",
            "title": "Training options (train.lua)"
        },
        {
            "location": "/nmt/init/#decoding-options-beamlua_1",
            "text": "model : Path to model .t7 file.  src_file : Source sequence to decode (one line per sequence).  targ_file : True target sequence (optional).  output_file : Path to output the predictions (each line will be the decoded sequence).  src_dict : Path to source vocabulary ( *.src.dict  file from  preprocess.py ).  targ_dict : Path to target vocabulary ( *.targ.dict  file from  preprocess.py ).  feature_dict_prefix : Prefix of the path to the features vocabularies ( *.feature_N.dict  files from  preprocess.py ).  char_dict : Path to character vocabulary ( *.char.dict  file from  preprocess.py ).  beam : Beam size (recommend keeping this at 5).  max_sent_l : Maximum sentence length. If any of the sequences in  srcfile  are longer than this\nit will error out.  simple : If = 1, output prediction is simply the first time the top of the beam\nends with an end-of-sentence token. If = 0, the model considers all hypotheses that have\nbeen generated so far that ends with end-of-sentence token and takes the highest scoring\nof all of them.  replace_unk : Replace the generated UNK tokens with the source token that had the highest\nattention weight. If  srctarg_dict  is provided, it will lookup the identified source token\nand give the corresponding target token. If it is not provided (or the identified source token\ndoes not exist in the table) then it will copy the source token.  srctarg_dict : Path to source-target dictionary to replace UNK tokens. Each line should be a\nsource token and its corresponding target token, separated by  ||| . For example   hello|||hallo\nukraine|||ukrainische  This dictionary can be obtained by, for example, running an alignment model as a preprocessing step.\nWe recommend  fast_align .   score_gold : If = 1, score the true target output as well.   n_best : If   1, then it will also output an n_best list of decoded sentences in the following\nformat.  1 ||| sentence_1 ||| sentence_1_score\n2 ||| sentence_2 ||| sentence_2_score   gpuid : ID of the GPU to use (-1 = use CPU).  gpuid2 : ID if the second GPU (if specified).  cudnn : If the model was trained with  cudnn , then this should be set to 1 (otherwise the model\nwill fail to load).  rescore : when set to scorer name, use scorer to find hypothesis with highest score - available 'bleu', 'gleu'  rescore_param : parameter to rescorer - for bleu/gleu ngram length",
            "title": "Decoding options (beam.lua)"
        },
        {
            "location": "/nmt/init/#using-additional-input-features_1",
            "text": "Linguistic Input Features Improve Neural Machine Translation  (Senrich et al. 2016) shows that translation performance can be increased by using additional input features.  Similarly to this work, you can annotate each word in the  source  text by using the  -|-  separator:  word1-|-feat1-|-feat2 word2-|-feat1-|-feat2  It supports an arbitrary number of features with arbitrary labels. However, all input words must have the  same  number of annotations. See for example  data/src-train-case.txt  which annotates each word with the case information.  To evaluate the model, the option  -feature_dict_prefix  is required on  evaluate.lua  which points to the prefix of the features dictionnaries generated during the preprocessing.",
            "title": "Using additional input features"
        },
        {
            "location": "/nmt/init/#pruning-a-model_1",
            "text": "Compression of Neural Machine Translation Models via Pruning  (See et al. 2016) shows that a model can be aggressively pruned while keeping the same performace.  To prune a model - you can use  prune.lua  which implement class-bind, and class-uniform pruning technique from the paper.   model : the model to prune  savefile : name of the pruned model  gpuid : Which gpu to use. -1 = use CPU. Depends if the model is serialized for GPU or CPU  ratio : pruning rate  prune : pruning technique  blind  or  uniform , by default  blind   note that the pruning cut connection with lowest weight in the linear models by using a boolean mask. The size of the file is a little larger since it stores the actual full matrix and the binary mask.  Models can be retrained - typically you can recover full capacity of a model pruned at 60% or even 80% by few epochs of additional trainings.",
            "title": "Pruning a model"
        },
        {
            "location": "/nmt/init/#switching-between-gpucpu-models_1",
            "text": "By default, the model will always save the final model as a CPU model, but it will save the\nintermediate models as a CPU/GPU model depending on how you specified  -gpuid .\nIf you want to run beam search on the CPU with an intermediate model trained on the GPU,\nyou can use  convert_to_cpu.lua  to convert the model to CPU and run beam search.",
            "title": "Switching between GPU/CPU models"
        },
        {
            "location": "/nmt/init/#gpu-memory-requirementstraining-speed_1",
            "text": "Training large sequence-to-sequence models can be memory-intensive. Memory requirements will\ndependent on batch size, maximum sequence length, vocabulary size, and (obviously) model size.\nHere are some benchmark numbers on a GeForce GTX Titan X.\n(assuming batch size of 64, maximum sequence length of 50 on both the source/target sequence,\nvocabulary size of 50000, and word embedding size equal to rnn size):  ( prealloc = 0 )  1-layer, 100 hidden units: 0.7G, 21.5K tokens/sec  1-layer, 250 hidden units: 1.4G, 14.1K tokens/sec  1-layer, 500 hidden units: 2.6G, 9.4K tokens/sec  2-layers, 500 hidden units: 3.2G, 7.4K tokens/sec\n* 4-layers, 1000 hidden units: 9.4G, 2.5K tokens/sec  Thanks to some fantastic work from folks at  SYSTRAN , turning  prealloc  on\nwill lead to much more memory efficient training  ( prealloc = 1 )  1-layer, 100 hidden units: 0.5G, 22.4K tokens/sec  1-layer, 250 hidden units: 1.1G, 14.5K tokens/sec  1-layer, 500 hidden units: 2.1G, 10.0K tokens/sec  2-layers, 500 hidden units: 2.3G, 8.2K tokens/sec\n* 4-layers, 1000 hidden units: 6.4G, 3.3K tokens/sec  Tokens/sec refers to total (i.e. source + target) tokens processed per second.\nIf using different batch sizes/sequence length, you should (linearly) scale\nthe above numbers accordingly. You can make use of memory on multiple GPUs by using -gpuid2  option in  train.lua . This will put the encoder on the GPU specified by -gpuid , and the decoder on the GPU specified by  -gpuid2 .",
            "title": "GPU memory requirements/Training speed"
        },
        {
            "location": "/nmt/init/#evaluation_1",
            "text": "For translation, evaluation via BLEU can be done by taking the output from  beam.lua  and using the multi-bleu.perl  script from  Moses . For example  perl multi-bleu.perl gold.txt   pred.txt",
            "title": "Evaluation"
        },
        {
            "location": "/nmt/init/#evaluation-of-states-and-attention_1",
            "text": "attention_extraction.lua can be used to extract the attention and the LSTM states. It uses the following (required) options:   model : Path to model .t7 file.  src_file : Source sequence to decode (one line per sequence).  targ_file : True target sequence.  src_dict : Path to source vocabulary ( *.src.dict  file from  preprocess.py ).  targ_dict : Path to target vocabulary ( *.targ.dict  file from  preprocess.py ).   Output of the script are two files,  encoder.hdf5  and  decoder.hdf5 . The encoder contains the states for every layer of the encoder LSTM and the offsets for the start of each source sentence. The decoder contains the states for the decoder LSTM layers and the offsets for the start of gold sentence. It additionally contains the attention for each time step (if the model uses attention).",
            "title": "Evaluation of States and Attention"
        },
        {
            "location": "/nmt/init/#pre-trained-models_1",
            "text": "We've uploaded English  -  German models trained on 4 million sentences from Workshop on Machine Translation 2015 .\nDownload link is below:  https://drive.google.com/open?id=0BzhmYioWLRn_aEVnd0ZNcWd0Y2c  These models are 4-layer LSTMs with 1000 hidden units and essentially replicates the results from Effective Approaches to Attention-based\nNeural Machine Translation ,\nLuong et al. EMNLP 2015.",
            "title": "Pre-trained models"
        },
        {
            "location": "/nmt/init/#acknowledgments_1",
            "text": "Our implementation utilizes code from the following:   Andrej Karpathy's char-rnn repo    Wojciech Zaremba's lstm repo \n*  Element rnn library",
            "title": "Acknowledgments"
        },
        {
            "location": "/nmt/init/#licence_1",
            "text": "MIT",
            "title": "Licence"
        },
        {
            "location": "/nmt/init/#opennmtbeam",
            "text": "Class for managing the beam search process.   [src]",
            "title": "OpenNMT.Beam"
        },
        {
            "location": "/nmt/init/#opennmtbeamsize",
            "text": "Constructor  Parameters:\n  *  size  : The beam  K .  [src]",
            "title": "OpenNMT.Beam(size)"
        },
        {
            "location": "/nmt/init/#opennmtbeamget_current_state",
            "text": "Get the outputs for the current timestep.  [src]",
            "title": "OpenNMT.Beam:get_current_state()"
        },
        {
            "location": "/nmt/init/#opennmtbeamget_current_origin",
            "text": "Get the backpointers for the current timestep.  [src]",
            "title": "OpenNMT.Beam:get_current_origin()"
        },
        {
            "location": "/nmt/init/#opennmtbeamadvanceout-attn_out",
            "text": "Given prob over words for every last beam  out  and attention\n  attn_out . Compute and update the beam search.  Parameters:\n  *  out - probs at the last step\n  *  attn_out - attention at the last step  Returns: true if beam search is complete.  [src]",
            "title": "OpenNMT.Beam:advance(out, attn_out)"
        },
        {
            "location": "/nmt/init/#opennmtbeamget_hypk",
            "text": "Walk back to construct the full hypothesis  k .  Parameters:\n  *  k  - the position in the beam to construct.  Returns:\n  1. The hypothesis\n  2. The attention at each time step.",
            "title": "OpenNMT.Beam:get_hyp(k)"
        },
        {
            "location": "/nmt/init/#undocumented-methods_9",
            "text": "*  OpenNMT.Beam:sort_best()  \n *  OpenNMT.Beam:get_best()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtfile_reader",
            "text": "",
            "title": "OpenNMT.file_reader"
        },
        {
            "location": "/nmt/init/#undocumented-methods_10",
            "text": "*  OpenNMT.file_reader(filename)  \n *  OpenNMT.file_reader:next()  \n *  OpenNMT.file_reader:close()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtmodel",
            "text": "",
            "title": "OpenNMT.Model"
        },
        {
            "location": "/nmt/init/#undocumented-methods_11",
            "text": "*  OpenNMT.Model()  \n *  OpenNMT.Model:double()  \n *  OpenNMT.Model:float()  \n *  OpenNMT.Model:cuda()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtgenerator",
            "text": "",
            "title": "OpenNMT.Generator"
        },
        {
            "location": "/nmt/init/#undocumented-methods_12",
            "text": "*  OpenNMT.Generator(args, network)  \n *  OpenNMT.Generator:forward_one(input)  \n *  OpenNMT.Generator:training()  \n *  OpenNMT.Generator:evaluate()  \n *  OpenNMT.Generator:convert(f)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/init/#opennmtcheckpoint",
            "text": "Class for saving and loading models during training.  [src]",
            "title": "OpenNMT.Checkpoint"
        },
        {
            "location": "/nmt/init/#opennmtcheckpointsave_iterationiteration-epoch_state-batch_order",
            "text": "Save the model and data in the middle of an epoch sorting the iteration.",
            "title": "OpenNMT.Checkpoint:save_iteration(iteration, epoch_state, batch_order)"
        },
        {
            "location": "/nmt/init/#undocumented-methods_13",
            "text": "*  OpenNMT.Checkpoint(args)  \n *  OpenNMT.Checkpoint:save(file_path, info)  \n *  OpenNMT.Checkpoint:save_epoch(valid_ppl, epoch_state)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+biencoder/",
            "text": "nmt.BiEncoder\n\n\n[src]\n\n\n\n\nnmt.BiEncoder(args, merge, net_fwd, net_bwd)\n\n\nCreates two Encoder's (encoder.lua) \nnet_fwd\n and \nnet_bwd\n.\n  The two are combined use \nmerge\n operation (concat/sum).\n\n\nUndocumented methods\n\n\n\n * \nnmt.BiEncoder:resize_proto(batch_size)\n\n\n\n * \nnmt.BiEncoder:forward(batch)\n\n\n\n * \nnmt.BiEncoder:backward(batch, grad_states_output, grad_context_output)\n\n\n\n * \nnmt.BiEncoder:training()\n\n\n\n * \nnmt.BiEncoder:evaluate()\n\n\n\n * \nnmt.BiEncoder:convert(f)",
            "title": "Lib+biencoder"
        },
        {
            "location": "/nmt/lib+biencoder/#nmtbiencoder",
            "text": "[src]",
            "title": "nmt.BiEncoder"
        },
        {
            "location": "/nmt/lib+biencoder/#nmtbiencoderargs-merge-net_fwd-net_bwd",
            "text": "Creates two Encoder's (encoder.lua)  net_fwd  and  net_bwd .\n  The two are combined use  merge  operation (concat/sum).",
            "title": "nmt.BiEncoder(args, merge, net_fwd, net_bwd)"
        },
        {
            "location": "/nmt/lib+biencoder/#undocumented-methods",
            "text": "*  nmt.BiEncoder:resize_proto(batch_size)  \n *  nmt.BiEncoder:forward(batch)  \n *  nmt.BiEncoder:backward(batch, grad_states_output, grad_context_output)  \n *  nmt.BiEncoder:training()  \n *  nmt.BiEncoder:evaluate()  \n *  nmt.BiEncoder:convert(f)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+data/",
            "text": "nmt.Data\n\n\nData management and batch creation.\n\n\nBatch interface [size]: \n\n\n\n\nsize: number of sentences in the batch [1]\n\n\nsource_length: max length in source batch [1]\n\n\nsource_size:  lengths of each source [batch x 1]\n\n\nsource_input:  left-padded idx's of source (PPPPPPABCDE) [batch x max]\n\n\nsource_input_rev: right-padded  idx's of source rev (EDCBAPPPPPP) [batch x max]\n\n\ntarget_length: max length in source batch [1]\n\n\ntarget_size: lengths of each source [batch x 1]\n\n\ntarget_non_zeros: number of non-ignored words in batch [1]\n\n\ntarget_input: input idx's of target (SABCDEPPPPPP) [batch x max]\n\n\ntarget_output: expected output idx's of target (ABCDESPPPPPP) [batch x max]\n\n\n\n\nTODO: change name of size =\n maxlen\n\n\n[src]\n\n\n\n\nnmt.Data(src, targ)\n\n\nInitialize a data object given aligned tables of IntTensors \nsrc\n\n  and \ntarg\n.\n\n\n[src]\n\n\n\n\nnmt.Data:set_batch_size(max_batch_size)\n\n\nSetup up the training data to respect \nmax_batch_size\n. \n\n\n[src]\n\n\n\n\nnmt.Data:get_data(src, targ)\n\n\nCreate a batch object given aligned sent tables \nsrc\n and \ntarg\n\n  (optional). Data format is shown at the top of the file.\n\n\n[src]\n\n\n\n\nnmt.Data:get_batch(idx)\n\n\nGet batch \nidx\n. If nil make a batch of all the data.",
            "title": "Lib+data"
        },
        {
            "location": "/nmt/lib+data/#nmtdata",
            "text": "Data management and batch creation.  Batch interface [size]:    size: number of sentences in the batch [1]  source_length: max length in source batch [1]  source_size:  lengths of each source [batch x 1]  source_input:  left-padded idx's of source (PPPPPPABCDE) [batch x max]  source_input_rev: right-padded  idx's of source rev (EDCBAPPPPPP) [batch x max]  target_length: max length in source batch [1]  target_size: lengths of each source [batch x 1]  target_non_zeros: number of non-ignored words in batch [1]  target_input: input idx's of target (SABCDEPPPPPP) [batch x max]  target_output: expected output idx's of target (ABCDESPPPPPP) [batch x max]   TODO: change name of size =  maxlen  [src]",
            "title": "nmt.Data"
        },
        {
            "location": "/nmt/lib+data/#nmtdatasrc-targ",
            "text": "Initialize a data object given aligned tables of IntTensors  src \n  and  targ .  [src]",
            "title": "nmt.Data(src, targ)"
        },
        {
            "location": "/nmt/lib+data/#nmtdataset_batch_sizemax_batch_size",
            "text": "Setup up the training data to respect  max_batch_size .   [src]",
            "title": "nmt.Data:set_batch_size(max_batch_size)"
        },
        {
            "location": "/nmt/lib+data/#nmtdataget_datasrc-targ",
            "text": "Create a batch object given aligned sent tables  src  and  targ \n  (optional). Data format is shown at the top of the file.  [src]",
            "title": "nmt.Data:get_data(src, targ)"
        },
        {
            "location": "/nmt/lib+data/#nmtdataget_batchidx",
            "text": "Get batch  idx . If nil make a batch of all the data.",
            "title": "nmt.Data:get_batch(idx)"
        },
        {
            "location": "/nmt/lib+decoder/",
            "text": "nmt.Decoder\n\n\nDecoder is the sequencer for the target words.\n\n\n .      .      .             .\n |      |      |             |     \nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n\n\n\n\nInherits from \nSequencer\n.\n\n\n[src]\n\n\n\n\nnmt.Decoder:resize_proto(batch_size)\n\n\nCall to change the \nbatch_size\n. \n\n\n[src]\n\n\n\n\nnmt.Decoder:reset(source_sizes, source_length, beam_size)\n\n\nUpdate internals to prepare for new batch.\n\n\n[src]\n\n\n\n\nnmt.Decoder:forward_one(input, prev_states, context, prev_out, t)\n\n\nRun one step of the decoder.\n\n\nParameters:\n\n\n\n\ninput\n - sparse input (1)\n\n\nprev_states\n - stack of hidden states (batch x layers*model x rnn_size)\n\n\ncontext\n - encoder output (batch x n x rnn_size)\n\n\nprev_out\n - previous distribution (batch x #words)\n\n\nt\n - current timestep\n\n\n\n\nReturns:\n\n\n\n\nout\n - Top-layer Hidden state\n\n\nstates\n - All states\n\n\n\n\n[src]\n\n\n\n\nnmt.Decoder:forward_and_apply(batch, encoder_states, context, func)\n\n\nCompute all forward steps.\n\n\nParameters:\n\n\n\n\nbatch\n - based on data.lua\n\n\nencoder_states\n\n\ncontext\n\n\nfunc\n - Calls \nfunc(out, t)\n each timestep.\n\n\n\n\n[src]\n\n\n\n\nnmt.Decoder:forward(batch, encoder_states, context)\n\n\nCompute all forward steps.\n\n\nParameters:\n\n\n\n\nbatch\n - based on data.lua\n\n\nencoder_states\n\n\ncontext\n\n\n\n\nReturns:\n\n\n\n\noutputs\n - Top Hidden layer at each time-step.\n\n\n\n\n[src]\n\n\n\n\nnmt.Decoder:compute_loss(batch, encoder_states, context, generator)\n\n\nCompute the loss on a batch based on final layer \ngenerator\n.\n\n\n[src]\n\n\n\n\nnmt.Decoder:backward(batch, outputs, generator)\n\n\nCompute the standard backward update.\n  With input \nbatch\n, target \noutputs\n, and \ngenerator\n\n  Note: This code is both the standard backward and criterion forward/backward.\n  It returns both the gradInputs (ret 1 and 2) and the loss.\n\n\nUndocumented methods\n\n\n\n * \nnmt.Decoder(args, network)\n\n\n\n * \nnmt.Decoder:compute_score(batch, encoder_states, context, generator)\n\n\n\n * \nnmt.Decoder:convert(f)",
            "title": "Lib+decoder"
        },
        {
            "location": "/nmt/lib+decoder/#nmtdecoder",
            "text": "Decoder is the sequencer for the target words.   .      .      .             .\n |      |      |             |     \nh_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n  Inherits from  Sequencer .  [src]",
            "title": "nmt.Decoder"
        },
        {
            "location": "/nmt/lib+decoder/#nmtdecoderresize_protobatch_size",
            "text": "Call to change the  batch_size .   [src]",
            "title": "nmt.Decoder:resize_proto(batch_size)"
        },
        {
            "location": "/nmt/lib+decoder/#nmtdecoderresetsource_sizes-source_length-beam_size",
            "text": "Update internals to prepare for new batch.  [src]",
            "title": "nmt.Decoder:reset(source_sizes, source_length, beam_size)"
        },
        {
            "location": "/nmt/lib+decoder/#nmtdecoderforward_oneinput-prev_states-context-prev_out-t",
            "text": "Run one step of the decoder.  Parameters:   input  - sparse input (1)  prev_states  - stack of hidden states (batch x layers*model x rnn_size)  context  - encoder output (batch x n x rnn_size)  prev_out  - previous distribution (batch x #words)  t  - current timestep   Returns:   out  - Top-layer Hidden state  states  - All states   [src]",
            "title": "nmt.Decoder:forward_one(input, prev_states, context, prev_out, t)"
        },
        {
            "location": "/nmt/lib+decoder/#nmtdecoderforward_and_applybatch-encoder_states-context-func",
            "text": "Compute all forward steps.  Parameters:   batch  - based on data.lua  encoder_states  context  func  - Calls  func(out, t)  each timestep.   [src]",
            "title": "nmt.Decoder:forward_and_apply(batch, encoder_states, context, func)"
        },
        {
            "location": "/nmt/lib+decoder/#nmtdecoderforwardbatch-encoder_states-context",
            "text": "Compute all forward steps.  Parameters:   batch  - based on data.lua  encoder_states  context   Returns:   outputs  - Top Hidden layer at each time-step.   [src]",
            "title": "nmt.Decoder:forward(batch, encoder_states, context)"
        },
        {
            "location": "/nmt/lib+decoder/#nmtdecodercompute_lossbatch-encoder_states-context-generator",
            "text": "Compute the loss on a batch based on final layer  generator .  [src]",
            "title": "nmt.Decoder:compute_loss(batch, encoder_states, context, generator)"
        },
        {
            "location": "/nmt/lib+decoder/#nmtdecoderbackwardbatch-outputs-generator",
            "text": "Compute the standard backward update.\n  With input  batch , target  outputs , and  generator \n  Note: This code is both the standard backward and criterion forward/backward.\n  It returns both the gradInputs (ret 1 and 2) and the loss.",
            "title": "nmt.Decoder:backward(batch, outputs, generator)"
        },
        {
            "location": "/nmt/lib+decoder/#undocumented-methods",
            "text": "*  nmt.Decoder(args, network)  \n *  nmt.Decoder:compute_score(batch, encoder_states, context, generator)  \n *  nmt.Decoder:convert(f)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+encoder/",
            "text": "nmt.Encoder\n\n\nEncoder is a unidirectional Sequencer used for the source language. \n\n\nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n\n\n\n\nInherits from \nSequencer\n.\n\n\n[src]\n\n\n\n\nnmt.Encoder(args, network)\n\n\nConstructor takes global \nargs\n and optional \nnetwork\n. \n\n\n[src]\n\n\n\n\nnmt.Encoder:resize_proto(batch_size)\n\n\nCall to change the \nbatch_size\n. \n\n\n[src]\n\n\n\n\nnmt.Encoder:forward(batch)\n\n\nCompute the context representation of an input.\n\n\nParameters:\n\n\n\n\nbatch\n - a \nbatch struct\n as defined data.lua.\n\n\n\n\nReturns:\n\n\n\n\n\n\n\n\nlast hidden states\n\n\n\n\n\n\n\n\n\n\ncontext matrix H\n\n\n\n\n\n\n\n\nTODO:\n\n\n\n\nChange \nbatch\n to \ninput\n.\n\n\n\n\n[src]\n\n\n\n\nnmt.Encoder:backward(batch, grad_states_output, grad_context_output)\n\n\nBackward pass (only called during training)\n\n\nParameters:\n\n\n\n\nbatch\n - must be same as for forward\n\n\ngrad_states_output\n\n\ngrad_context_output\n - gradient of loss\n      wrt last states and context.\n\n\n\n\nTODO: change this to (input, gradOutput) as in nngraph.\n\n\nUndocumented methods\n\n\n\n * \nnmt.Encoder:convert(f)",
            "title": "Lib+encoder"
        },
        {
            "location": "/nmt/lib+encoder/#nmtencoder",
            "text": "Encoder is a unidirectional Sequencer used for the source language.   h_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n  Inherits from  Sequencer .  [src]",
            "title": "nmt.Encoder"
        },
        {
            "location": "/nmt/lib+encoder/#nmtencoderargs-network",
            "text": "Constructor takes global  args  and optional  network .   [src]",
            "title": "nmt.Encoder(args, network)"
        },
        {
            "location": "/nmt/lib+encoder/#nmtencoderresize_protobatch_size",
            "text": "Call to change the  batch_size .   [src]",
            "title": "nmt.Encoder:resize_proto(batch_size)"
        },
        {
            "location": "/nmt/lib+encoder/#nmtencoderforwardbatch",
            "text": "Compute the context representation of an input.  Parameters:   batch  - a  batch struct  as defined data.lua.   Returns:     last hidden states      context matrix H     TODO:   Change  batch  to  input .   [src]",
            "title": "nmt.Encoder:forward(batch)"
        },
        {
            "location": "/nmt/lib+encoder/#nmtencoderbackwardbatch-grad_states_output-grad_context_output",
            "text": "Backward pass (only called during training)  Parameters:   batch  - must be same as for forward  grad_states_output  grad_context_output  - gradient of loss\n      wrt last states and context.   TODO: change this to (input, gradOutput) as in nngraph.",
            "title": "nmt.Encoder:backward(batch, grad_states_output, grad_context_output)"
        },
        {
            "location": "/nmt/lib+encoder/#undocumented-methods",
            "text": "*  nmt.Encoder:convert(f)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+eval+beam/",
            "text": "nmt.Beam\n\n\nClass for managing the beam search process. \n\n\n[src]\n\n\n\n\nnmt.Beam(size)\n\n\nConstructor\n\n\nParameters:\n\n\n\n\nsize\n : The beam \nK\n.\n\n\n\n\n[src]\n\n\n\n\nnmt.Beam:get_current_state()\n\n\nGet the outputs for the current timestep.\n\n\n[src]\n\n\n\n\nnmt.Beam:get_current_origin()\n\n\nGet the backpointers for the current timestep.\n\n\n[src]\n\n\n\n\nnmt.Beam:advance(out, attn_out)\n\n\nGiven prob over words for every last beam \nout\n and attention\n \nattn_out\n. Compute and update the beam search.\n\n\nParameters:\n\n\n\n\nout\n- probs at the last step\n\n\nattn_out\n- attention at the last step\n\n\n\n\nReturns: true if beam search is complete.\n\n\n[src]\n\n\n\n\nnmt.Beam:get_hyp(k)\n\n\nWalk back to construct the full hypothesis \nk\n.\n\n\nParameters:\n\n\n\n\nk\n - the position in the beam to construct.\n\n\n\n\nReturns:\n\n\n\n\nThe hypothesis\n\n\nThe attention at each time step.\n\n\n\n\nUndocumented methods\n\n\n\n * \nnmt.Beam:sort_best()\n\n\n\n * \nnmt.Beam:get_best()",
            "title": "Lib+eval+beam"
        },
        {
            "location": "/nmt/lib+eval+beam/#nmtbeam",
            "text": "Class for managing the beam search process.   [src]",
            "title": "nmt.Beam"
        },
        {
            "location": "/nmt/lib+eval+beam/#nmtbeamsize",
            "text": "Constructor  Parameters:   size  : The beam  K .   [src]",
            "title": "nmt.Beam(size)"
        },
        {
            "location": "/nmt/lib+eval+beam/#nmtbeamget_current_state",
            "text": "Get the outputs for the current timestep.  [src]",
            "title": "nmt.Beam:get_current_state()"
        },
        {
            "location": "/nmt/lib+eval+beam/#nmtbeamget_current_origin",
            "text": "Get the backpointers for the current timestep.  [src]",
            "title": "nmt.Beam:get_current_origin()"
        },
        {
            "location": "/nmt/lib+eval+beam/#nmtbeamadvanceout-attn_out",
            "text": "Given prob over words for every last beam  out  and attention\n  attn_out . Compute and update the beam search.  Parameters:   out - probs at the last step  attn_out - attention at the last step   Returns: true if beam search is complete.  [src]",
            "title": "nmt.Beam:advance(out, attn_out)"
        },
        {
            "location": "/nmt/lib+eval+beam/#nmtbeamget_hypk",
            "text": "Walk back to construct the full hypothesis  k .  Parameters:   k  - the position in the beam to construct.   Returns:   The hypothesis  The attention at each time step.",
            "title": "nmt.Beam:get_hyp(k)"
        },
        {
            "location": "/nmt/lib+eval+beam/#undocumented-methods",
            "text": "*  nmt.Beam:sort_best()  \n *  nmt.Beam:get_best()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+eval+translate/",
            "text": "",
            "title": "Lib+eval+translate"
        },
        {
            "location": "/nmt/lib+generator/",
            "text": "",
            "title": "Lib+generator"
        },
        {
            "location": "/nmt/lib+memory/",
            "text": "Undocumented methods\n\n\n\n * \nnmt.Memory.optimize(model, batch)",
            "title": "Lib+memory"
        },
        {
            "location": "/nmt/lib+memory/#undocumented-methods",
            "text": "*  nmt.Memory.optimize(model, batch)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+model/",
            "text": "nmt.Model\n\n\nUndocumented methods\n\n\n\n * \nnmt.Model()\n\n\n\n * \nnmt.Model:double()\n\n\n\n * \nnmt.Model:float()\n\n\n\n * \nnmt.Model:cuda()",
            "title": "Lib+model"
        },
        {
            "location": "/nmt/lib+model/#nmtmodel",
            "text": "",
            "title": "nmt.Model"
        },
        {
            "location": "/nmt/lib+model/#undocumented-methods",
            "text": "*  nmt.Model()  \n *  nmt.Model:double()  \n *  nmt.Model:float()  \n *  nmt.Model:cuda()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+sequencer/",
            "text": "nmt.Sequencer\n\n\nSequencer is the base class for our time series LSTM models.\n  Acts similarly to an \nnn.Module\n.\n   Main task is to manage \nself.network_clones\n, the unrolled LSTM\n  used during training.\n  Classes encoder/decoder/biencoder generalize these definitions.\n\n\nInherits from \nModel\n.\n\n\n[src]\n\n\n\n\nnmt.Sequencer(model, args, network)\n\n\nConstructor\n\n\nParameters:\n\n\n\n\nmodel\n - type of model (enc,dec)\n\n\nargs\n - global arguments\n\n\nnetwork\n - optional preconstructed network.\n\n\n\n\nTODO: Should initialize all the members in this method.\n   i.e. word_vecs, fix_word_vecs, network_clones, eval_mode, etc.\n\n\n[src]\n\n\n\n\nnmt.Sequencer:training()\n\n\nTell the network to prepare for training mode. \n\n\n[src]\n\n\n\n\nnmt.Sequencer:evaluate()\n\n\nTell the network to prepare for evaluation mode. \n\n\nUndocumented methods\n\n\n\n * \nnmt.Sequencer:resize_proto(batch_size)\n\n\n\n * \nnmt.Sequencer:backward_word_vecs()\n\n\n\n * \nnmt.Sequencer:net(t)\n\n\n\n * \nnmt.Sequencer:convert(f)",
            "title": "Lib+sequencer"
        },
        {
            "location": "/nmt/lib+sequencer/#nmtsequencer",
            "text": "Sequencer is the base class for our time series LSTM models.\n  Acts similarly to an  nn.Module .\n   Main task is to manage  self.network_clones , the unrolled LSTM\n  used during training.\n  Classes encoder/decoder/biencoder generalize these definitions.  Inherits from  Model .  [src]",
            "title": "nmt.Sequencer"
        },
        {
            "location": "/nmt/lib+sequencer/#nmtsequencermodel-args-network",
            "text": "Constructor  Parameters:   model  - type of model (enc,dec)  args  - global arguments  network  - optional preconstructed network.   TODO: Should initialize all the members in this method.\n   i.e. word_vecs, fix_word_vecs, network_clones, eval_mode, etc.  [src]",
            "title": "nmt.Sequencer(model, args, network)"
        },
        {
            "location": "/nmt/lib+sequencer/#nmtsequencertraining",
            "text": "Tell the network to prepare for training mode.   [src]",
            "title": "nmt.Sequencer:training()"
        },
        {
            "location": "/nmt/lib+sequencer/#nmtsequencerevaluate",
            "text": "Tell the network to prepare for evaluation mode.",
            "title": "nmt.Sequencer:evaluate()"
        },
        {
            "location": "/nmt/lib+sequencer/#undocumented-methods",
            "text": "*  nmt.Sequencer:resize_proto(batch_size)  \n *  nmt.Sequencer:backward_word_vecs()  \n *  nmt.Sequencer:net(t)  \n *  nmt.Sequencer:convert(f)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+train+checkpoint/",
            "text": "nmt.Checkpoint\n\n\nClass for saving and loading models during training.\n\n\n[src]\n\n\n\n\nnmt.Checkpoint:save_iteration(iteration, epoch_state, batch_order)\n\n\nSave the model and data in the middle of an epoch sorting the iteration. \n\n\nUndocumented methods\n\n\n\n * \nnmt.Checkpoint(args)\n\n\n\n * \nnmt.Checkpoint:save(file_path, info)\n\n\n\n * \nnmt.Checkpoint:save_epoch(valid_ppl, epoch_state)",
            "title": "Lib+train+checkpoint"
        },
        {
            "location": "/nmt/lib+train+checkpoint/#nmtcheckpoint",
            "text": "Class for saving and loading models during training.  [src]",
            "title": "nmt.Checkpoint"
        },
        {
            "location": "/nmt/lib+train+checkpoint/#nmtcheckpointsave_iterationiteration-epoch_state-batch_order",
            "text": "Save the model and data in the middle of an epoch sorting the iteration.",
            "title": "nmt.Checkpoint:save_iteration(iteration, epoch_state, batch_order)"
        },
        {
            "location": "/nmt/lib+train+checkpoint/#undocumented-methods",
            "text": "*  nmt.Checkpoint(args)  \n *  nmt.Checkpoint:save(file_path, info)  \n *  nmt.Checkpoint:save_epoch(valid_ppl, epoch_state)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+train+epoch_state/",
            "text": "nmt.EpochState\n\n\nClass for managing the training process by logging and storing\n  the state of the current epoch.\n\n\n[src]\n\n\n\n\nnmt.EpochState(epoch, status)\n\n\nInitialize for epoch \nepoch\n and training \nstatus\n (current loss)\n\n\n[src]\n\n\n\n\nnmt.EpochState:update(batch, loss)\n\n\nUpdate training status. Takes \nbatch\n (described in data.lua) and last loss.\n\n\n[src]\n\n\n\n\nnmt.EpochState:log(batch_index, data_size, learning_rate)\n\n\nLog to status stdout.\n  TODO: these args shouldn't need to be passed in each time. \n\n\nUndocumented methods\n\n\n\n * \nnmt.EpochState:get_train_ppl()\n\n\n\n * \nnmt.EpochState:get_time()\n\n\n\n * \nnmt.EpochState:get_status()",
            "title": "Lib+train+epoch state"
        },
        {
            "location": "/nmt/lib+train+epoch_state/#nmtepochstate",
            "text": "Class for managing the training process by logging and storing\n  the state of the current epoch.  [src]",
            "title": "nmt.EpochState"
        },
        {
            "location": "/nmt/lib+train+epoch_state/#nmtepochstateepoch-status",
            "text": "Initialize for epoch  epoch  and training  status  (current loss)  [src]",
            "title": "nmt.EpochState(epoch, status)"
        },
        {
            "location": "/nmt/lib+train+epoch_state/#nmtepochstateupdatebatch-loss",
            "text": "Update training status. Takes  batch  (described in data.lua) and last loss.  [src]",
            "title": "nmt.EpochState:update(batch, loss)"
        },
        {
            "location": "/nmt/lib+train+epoch_state/#nmtepochstatelogbatch_index-data_size-learning_rate",
            "text": "Log to status stdout.\n  TODO: these args shouldn't need to be passed in each time.",
            "title": "nmt.EpochState:log(batch_index, data_size, learning_rate)"
        },
        {
            "location": "/nmt/lib+train+epoch_state/#undocumented-methods",
            "text": "*  nmt.EpochState:get_train_ppl()  \n *  nmt.EpochState:get_time()  \n *  nmt.EpochState:get_status()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+train+optim/",
            "text": "nmt.Optim\n\n\n[src]\n\n\n\n\nnmt.Optim:update_learning_rate(score, epoch)\n\n\ndecay learning rate if val perf does not improve or we hit the start_decay_at limit\n\n\nUndocumented methods\n\n\n\n * \nnmt.Optim(args)\n\n\n\n * \nnmt.Optim:update_params(params, grad_params, max_grad_norm)\n\n\n\n * \nnmt.Optim:get_learning_rate()\n\n\n\n * \nnmt.Optim:get_states()",
            "title": "Lib+train+optim"
        },
        {
            "location": "/nmt/lib+train+optim/#nmtoptim",
            "text": "[src]",
            "title": "nmt.Optim"
        },
        {
            "location": "/nmt/lib+train+optim/#nmtoptimupdate_learning_ratescore-epoch",
            "text": "decay learning rate if val perf does not improve or we hit the start_decay_at limit",
            "title": "nmt.Optim:update_learning_rate(score, epoch)"
        },
        {
            "location": "/nmt/lib+train+optim/#undocumented-methods",
            "text": "*  nmt.Optim(args)  \n *  nmt.Optim:update_params(params, grad_params, max_grad_norm)  \n *  nmt.Optim:get_learning_rate()  \n *  nmt.Optim:get_states()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+utils+constants/",
            "text": "",
            "title": "Lib+utils+constants"
        },
        {
            "location": "/nmt/lib+utils+cuda/",
            "text": "Undocumented methods\n\n\n\n * \nnmt.Cuda.init(opt)\n\n\n\n * \nnmt.Cuda.convert(obj)",
            "title": "Lib+utils+cuda"
        },
        {
            "location": "/nmt/lib+utils+cuda/#undocumented-methods",
            "text": "*  nmt.Cuda.init(opt)  \n *  nmt.Cuda.convert(obj)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+utils+dict/",
            "text": "nmt.dict\n\n\nUndocumented methods\n\n\n\n * \nnmt.dict(data)\n\n\n\n * \nnmt.dict:load_file(filename)\n\n\n\n * \nnmt.dict:write_file(filename)\n\n\n\n * \nnmt.dict:lookup(key)\n\n\n\n * \nnmt.dict:set_special(special)\n\n\n\n * \nnmt.dict:add_special(label, idx)\n\n\n\n * \nnmt.dict:add_specials(labels)\n\n\n\n * \nnmt.dict:add(label, idx)\n\n\n\n * \nnmt.dict:prune(size)\n\n\n\n * \nnmt.dict:convert_to_idx(labels, start_symbols)\n\n\n\n * \nnmt.dict:convert_to_labels(idx, stop)",
            "title": "Lib+utils+dict"
        },
        {
            "location": "/nmt/lib+utils+dict/#nmtdict",
            "text": "",
            "title": "nmt.dict"
        },
        {
            "location": "/nmt/lib+utils+dict/#undocumented-methods",
            "text": "*  nmt.dict(data)  \n *  nmt.dict:load_file(filename)  \n *  nmt.dict:write_file(filename)  \n *  nmt.dict:lookup(key)  \n *  nmt.dict:set_special(special)  \n *  nmt.dict:add_special(label, idx)  \n *  nmt.dict:add_specials(labels)  \n *  nmt.dict:add(label, idx)  \n *  nmt.dict:prune(size)  \n *  nmt.dict:convert_to_idx(labels, start_symbols)  \n *  nmt.dict:convert_to_labels(idx, stop)",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+utils+file_reader/",
            "text": "nmt.file_reader\n\n\nUndocumented methods\n\n\n\n * \nnmt.file_reader(filename)\n\n\n\n * \nnmt.file_reader:next()\n\n\n\n * \nnmt.file_reader:close()",
            "title": "Lib+utils+file reader"
        },
        {
            "location": "/nmt/lib+utils+file_reader/#nmtfile_reader",
            "text": "",
            "title": "nmt.file_reader"
        },
        {
            "location": "/nmt/lib+utils+file_reader/#undocumented-methods",
            "text": "*  nmt.file_reader(filename)  \n *  nmt.file_reader:next()  \n *  nmt.file_reader:close()",
            "title": "Undocumented methods"
        },
        {
            "location": "/nmt/lib+utils+model_utils/",
            "text": "",
            "title": "Lib+utils+model utils"
        },
        {
            "location": "/nmt/lib+utils+opt_utils/",
            "text": "",
            "title": "Lib+utils+opt utils"
        },
        {
            "location": "/nmt/lib+utils+table_utils/",
            "text": "",
            "title": "Lib+utils+table utils"
        },
        {
            "location": "/nmt/preprocess/",
            "text": "",
            "title": "Preprocess"
        },
        {
            "location": "/nmt/release_model/",
            "text": "",
            "title": "Release model"
        },
        {
            "location": "/nmt/train/",
            "text": "",
            "title": "Train"
        }
    ]
}