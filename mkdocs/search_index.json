{
    "docs": [
        {
            "location": "/", 
            "text": "This portal provides a detailled documentation of the OpenNMT toolkit. It describes how to use the Torch project and how it works.\n\n\nFor the PyTorch version, visit the project on \nGitHub\n.\n\n\nAdditional resources\n\n\nYou can find additional help or tutorials in the following resources:\n\n\n\n\nForum\n\n\nGitter channel\n\n\n\n\n\n\nNote\n\n\nIf you find an error in this documentation, please consider \nopening an issue\n or directly submitting a modification by clicking on the edit button at the top of a page.", 
            "title": "Overview"
        }, 
        {
            "location": "/#additional-resources", 
            "text": "You can find additional help or tutorials in the following resources:   Forum  Gitter channel    Note  If you find an error in this documentation, please consider  opening an issue  or directly submitting a modification by clicking on the edit button at the top of a page.", 
            "title": "Additional resources"
        }, 
        {
            "location": "/installation/", 
            "text": "Standard\n\n\n1. \nInstall Torch\n\n\n2. Install additional dependencies:\n\n\nluarocks install tds\n\n\n\n\n\n3. Clone the OpenNMT repository:\n\n\ngit clone https://github.com/OpenNMT/OpenNMT\n\ncd\n OpenNMT\n\n\n\n\n\nAnd you are ready to go! Take a look at the \nquickstart\n to familiarize yourself with the main training workflow.\n\n\nDocker (Ubuntu)\n\n\nFirst you need to install \nnvidia-docker\n:\n\n\nwget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.0-rc.3/nvidia-docker_1.0.0.rc.3-1_amd64.deb\nsudo dpkg -i /tmp/nvidia-docker*.deb\n\n\n\n\n\nIf this command does not work, you may need to run the following updates:\n\n\nsudo apt-add-repository \ndeb https://apt.dockerproject.org/repo ubuntu-xenial main\n\nsudo apt-get update\nsudo apt-get install docker-engine nvidia-modprobe\n\n\n\n\n\nThen simply run our Docker container:\n\n\nsudo nvidia-docker run -it harvardnlp/opennmt:8.0\n\n\n\n\n\nOnce in the instance, check out the latest code:\n\n\ngit clone https://github.com/OpenNMT/OpenNMT\n\n\n\n\n\nAmazon EC2\n\n\nThe best way to do this is through Docker. We have a public AMI with the preliminary CUDA drivers installed: \nami-c12f86a1\n. Start a P2/G2 GPU instance with this AMI and run the \nnvidia-docker\n command above to get started.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#standard", 
            "text": "1.  Install Torch  2. Install additional dependencies:  luarocks install tds  3. Clone the OpenNMT repository:  git clone https://github.com/OpenNMT/OpenNMT cd  OpenNMT  And you are ready to go! Take a look at the  quickstart  to familiarize yourself with the main training workflow.", 
            "title": "Standard"
        }, 
        {
            "location": "/installation/#docker-ubuntu", 
            "text": "First you need to install  nvidia-docker :  wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.0-rc.3/nvidia-docker_1.0.0.rc.3-1_amd64.deb\nsudo dpkg -i /tmp/nvidia-docker*.deb  If this command does not work, you may need to run the following updates:  sudo apt-add-repository  deb https://apt.dockerproject.org/repo ubuntu-xenial main \nsudo apt-get update\nsudo apt-get install docker-engine nvidia-modprobe  Then simply run our Docker container:  sudo nvidia-docker run -it harvardnlp/opennmt:8.0  Once in the instance, check out the latest code:  git clone https://github.com/OpenNMT/OpenNMT", 
            "title": "Docker (Ubuntu)"
        }, 
        {
            "location": "/installation/#amazon-ec2", 
            "text": "The best way to do this is through Docker. We have a public AMI with the preliminary CUDA drivers installed:  ami-c12f86a1 . Start a P2/G2 GPU instance with this AMI and run the  nvidia-docker  command above to get started.", 
            "title": "Amazon EC2"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Step 1: Preprocess the data\n\n\nth preprocess.lua -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\n\n\n\n\n\nWe will be working with some example data in \ndata/\n folder.\n\n\nThe data consists of parallel source (\nsrc\n) and target (\ntgt\n) data containing one sentence per line with tokens separated by a space:\n\n\n\n\nsrc-train.txt\n\n\ntgt-train.txt\n\n\nsrc-val.txt\n\n\ntgt-val.txt\n\n\n\n\nValidation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.\n\n\n$ head -n 3 data/src-train.txt\nIt is not acceptable that , with the help of the national bureaucracies , Parliament \napos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\nFederal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .\n\nquot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .\n\n\n\n\n\nAfter running the preprocessing, the following files are generated:\n\n\n\n\ndemo.src.dict\n: Dictionary of source vocab to index mappings.\n\n\ndemo.tgt.dict\n: Dictionary of target vocab to index mappings.\n\n\ndemo-train.t7\n: serialized Torch file containing vocabulary, training and validation data\n\n\n\n\nThe \n*.dict\n files are needed to check or reuse the vocabularies. These files are simple human-readable dictionaries.\n\n\n$ head -n 10 data/demo.src.dict\n\nblank\n 1\n\nunk\n 2\n\ns\n 3\n\n/s\n 4\nIt 5\nis 6\nnot 7\nacceptable 8\nthat 9\n, 10\nwith 11\n\n\n\n\n\nInternally the system never touches the words themselves, but uses these indices.\n\n\n\n\nNote\n\n\nIf the corpus is not tokenized, you can use \nOpenNMT's tokenizer\n.\n\n\n\n\nStep 2: Train the model\n\n\nth train.lua -data data/demo-train.t7 -save_model demo-model\n\n\n\n\n\nThe main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder. You\ncan also add \n-gpuid 1\n to use (say) GPU 1.\n\n\nStep 3: Translate\n\n\nth translate.lua -model demo-model_epochX_PPL.t7 -src data/src-test.txt -output pred.txt\n\n\n\n\n\nNow you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into \npred.txt\n.\n\n\n\n\nNote\n\n\nThe predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for \ntranslation\n or \nsummarization\n.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/quickstart/#step-1-preprocess-the-data", 
            "text": "th preprocess.lua -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo  We will be working with some example data in  data/  folder.  The data consists of parallel source ( src ) and target ( tgt ) data containing one sentence per line with tokens separated by a space:   src-train.txt  tgt-train.txt  src-val.txt  tgt-val.txt   Validation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.  $ head -n 3 data/src-train.txt\nIt is not acceptable that , with the help of the national bureaucracies , Parliament  apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\nFederal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym . quot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .  After running the preprocessing, the following files are generated:   demo.src.dict : Dictionary of source vocab to index mappings.  demo.tgt.dict : Dictionary of target vocab to index mappings.  demo-train.t7 : serialized Torch file containing vocabulary, training and validation data   The  *.dict  files are needed to check or reuse the vocabularies. These files are simple human-readable dictionaries.  $ head -n 10 data/demo.src.dict blank  1 unk  2 s  3 /s  4\nIt 5\nis 6\nnot 7\nacceptable 8\nthat 9\n, 10\nwith 11  Internally the system never touches the words themselves, but uses these indices.   Note  If the corpus is not tokenized, you can use  OpenNMT's tokenizer .", 
            "title": "Step 1: Preprocess the data"
        }, 
        {
            "location": "/quickstart/#step-2-train-the-model", 
            "text": "th train.lua -data data/demo-train.t7 -save_model demo-model  The main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder. You\ncan also add  -gpuid 1  to use (say) GPU 1.", 
            "title": "Step 2: Train the model"
        }, 
        {
            "location": "/quickstart/#step-3-translate", 
            "text": "th translate.lua -model demo-model_epochX_PPL.t7 -src data/src-test.txt -output pred.txt  Now you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into  pred.txt .   Note  The predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for  translation  or  summarization .", 
            "title": "Step 3: Translate"
        }, 
        {
            "location": "/data/preparation/", 
            "text": "The data preparation (or preprocessing) passes over the data to generate word vocabularies and sequences of indices used by the training.\n\n\nData type\n\n\nBy default, the data type is \nbitext\n which are aligned source and target files. Alignment is by default done at the line level, but can also be done through aligned index (see \nIndex files\n).\n\n\nFor training language models, data type is \nmonotext\n which is only one language file.\n\n\nFinally, you can also manipulate the \nfeattext\n data type (see \nInput vectors\n) which allows to code sequences of vectors (e.g. sequence of features generated by a device).\n\n\n\n\nNote\n\n\nInput vectors can only be used for the source.\n\n\n\n\nDelimiters\n\n\nTraining data (for \nbitext\n and \nmonotext\n data types) are expected to follow the following format:\n\n\n\n\nsentences are newline-separated\n\n\ntokens are space-separated\n\n\n\n\nIndex files\n\n\nIndex files are aligning different files by index and not by line. For instance the following files are aligned by index:\n\n\nline1 First line\nline2 Second line\n\n\n\n\n\nline2 Deuxi\u00e8me ligne\nline1 Premi\u00e8re ligne\n\n\n\n\n\nwhere the first token of each line is an index which must have an equivalent (at any position) in aligned files.\n\n\nThe option \n-idx_files\n is used (in \npreprocess.lua\n or \ntranslate.lua\n) to enable this feature.\n\n\nInput vectors\n\n\nOpenNMT supports the use of vector sequence instead of word sequence on the source side.\n\n\nThe data type is \nfeattext\n and is using the \nKaldi\n text format (\n.ark\n files). For instance the following entry, indexed by \nKEY\n is representing a sequence\nof \nm\n vectors of \nn\n values:\n\n\nKEY [\nFEAT1.1 FEAT1.2 FEAT1.3 ... FEAT1.n\n...\nFEATm.1 FEATm.2 FEATm.3 ... FEATm.n ]\n\n\n\n\n\n\n\nWarning\n\n\nNote that you need to use index files for representing input vectors.\n\n\n\n\nVocabularies\n\n\nThe main goal of the preprocessing is to build the word and features vocabularies and assign each word to an index within these dictionaries.\n\n\nBy default, word vocabularies are limited to 50,000. You can change this value with the \n-src_vocab_size\n and \n-tgt_vocab_size\n. Alternatively, you can prune the vocabulary size by setting the minimum frequency of words with the \n-src_words_min_frequency\n and \n-tgt_words_min_frequency\n options.\n\n\n\n\nNote\n\n\nWhen pruning vocabularies to 50,000, the preprocessing will actually report a vocabulary size of 50,004 because of 4 special tokens that are automatically added.\n\n\n\n\nThe preprocessing script will generate \n*.dict\n files containing the vocabularies: source and target token vocabularies are named \nPREFIX.src.dict\n and \nPREFIX.tgt.dict\n, while features' vocabulary files are named \nPREFIX.{source,target}_feature_N.dict\n.\n\n\nThese files are optional for the rest of the workflow. However, it is common to reuse vocabularies across dataset using the \n-src_vocab\n and \n-tgt_vocab\n options. This is particularly needed when retraining a model on new data: the vocabulary has to be the same.\n\n\n\n\nTip\n\n\nVocabularies can be generated beforehand with the \ntools/build_vocab.lua\n script.\n\n\n\n\nEach line of dictionary files is space-separated fields:\n\n\n\n\ntoken\n the vocab entry.\n\n\nID\n its index used internally to map tokens to integer as an entry of lookup tables.\n\n\n(optional) the vocab frequency in the corpus it was extracted form. This field is generated.\n\n\nother fields are ignored\n\n\n\n\n\n\nNote\n\n\nif you provide your own vocabulary - be sure to integrate the 4 special tokens: \nblank\n \nunk\n \ns\n \n/s\n. A good practice is to keep them at the beginning of the file with the respective index 1, 2, 3, 4\n\n\n\n\nShuffling and sorting\n\n\nBy default, OpenNMT both shuffles and sorts the data before the training. This process comes from 2 constraints of batch training:\n\n\n\n\nshuffling\n: sentences within a batch should come from different parts of the corpus\n\n\nsorting\n: sentences within a batch should have the same source length (i.e. without padding to maximize efficiency)\n\n\n\n\n\n\nNote\n\n\nDuring the training, batches are also randomly selected unless the \n-curriculum\n option is used.\n\n\n\n\nSentence length\n\n\nDuring preprocessing, too long sentences (with source longer than \n-src_seq_length\n or target longer than \n-tgt_seq_length\n) are discarded from the corpus. You can have an idea of the distribution of sentence length in your training corpus by looking at the preprocess log where a table gives percent of sentences with length 1-10, 11-20, 21-30, ..., 90+:\n\n\n[04/14/17 00:40:10 INFO]  * Source Sentence Length (range of 10): [ 7% ; 35% ; 32% ; 16% ; 7% ; 0% ; 0% ; 0% ; 0% ; 0% ]\n[04/14/17 00:40:10 INFO]  * Target Sentence Length (range of 10): [ 9% ; 38% ; 30% ; 15% ; 5% ; 0% ; 0% ; 0% ; 0% ; 0% ]\n\n\n\n\n\n\n\nNote\n\n\nLimiting maximal sentence length is a key parameter to reduce the GPU memory footprint used during training: indeed the memory grows linearly with maximal sentence length.", 
            "title": "Preparation"
        }, 
        {
            "location": "/data/preparation/#data-type", 
            "text": "By default, the data type is  bitext  which are aligned source and target files. Alignment is by default done at the line level, but can also be done through aligned index (see  Index files ).  For training language models, data type is  monotext  which is only one language file.  Finally, you can also manipulate the  feattext  data type (see  Input vectors ) which allows to code sequences of vectors (e.g. sequence of features generated by a device).   Note  Input vectors can only be used for the source.", 
            "title": "Data type"
        }, 
        {
            "location": "/data/preparation/#delimiters", 
            "text": "Training data (for  bitext  and  monotext  data types) are expected to follow the following format:   sentences are newline-separated  tokens are space-separated", 
            "title": "Delimiters"
        }, 
        {
            "location": "/data/preparation/#index-files", 
            "text": "Index files are aligning different files by index and not by line. For instance the following files are aligned by index:  line1 First line\nline2 Second line  line2 Deuxi\u00e8me ligne\nline1 Premi\u00e8re ligne  where the first token of each line is an index which must have an equivalent (at any position) in aligned files.  The option  -idx_files  is used (in  preprocess.lua  or  translate.lua ) to enable this feature.", 
            "title": "Index files"
        }, 
        {
            "location": "/data/preparation/#input-vectors", 
            "text": "OpenNMT supports the use of vector sequence instead of word sequence on the source side.  The data type is  feattext  and is using the  Kaldi  text format ( .ark  files). For instance the following entry, indexed by  KEY  is representing a sequence\nof  m  vectors of  n  values:  KEY [\nFEAT1.1 FEAT1.2 FEAT1.3 ... FEAT1.n\n...\nFEATm.1 FEATm.2 FEATm.3 ... FEATm.n ]   Warning  Note that you need to use index files for representing input vectors.", 
            "title": "Input vectors"
        }, 
        {
            "location": "/data/preparation/#vocabularies", 
            "text": "The main goal of the preprocessing is to build the word and features vocabularies and assign each word to an index within these dictionaries.  By default, word vocabularies are limited to 50,000. You can change this value with the  -src_vocab_size  and  -tgt_vocab_size . Alternatively, you can prune the vocabulary size by setting the minimum frequency of words with the  -src_words_min_frequency  and  -tgt_words_min_frequency  options.   Note  When pruning vocabularies to 50,000, the preprocessing will actually report a vocabulary size of 50,004 because of 4 special tokens that are automatically added.   The preprocessing script will generate  *.dict  files containing the vocabularies: source and target token vocabularies are named  PREFIX.src.dict  and  PREFIX.tgt.dict , while features' vocabulary files are named  PREFIX.{source,target}_feature_N.dict .  These files are optional for the rest of the workflow. However, it is common to reuse vocabularies across dataset using the  -src_vocab  and  -tgt_vocab  options. This is particularly needed when retraining a model on new data: the vocabulary has to be the same.   Tip  Vocabularies can be generated beforehand with the  tools/build_vocab.lua  script.   Each line of dictionary files is space-separated fields:   token  the vocab entry.  ID  its index used internally to map tokens to integer as an entry of lookup tables.  (optional) the vocab frequency in the corpus it was extracted form. This field is generated.  other fields are ignored    Note  if you provide your own vocabulary - be sure to integrate the 4 special tokens:  blank   unk   s   /s . A good practice is to keep them at the beginning of the file with the respective index 1, 2, 3, 4", 
            "title": "Vocabularies"
        }, 
        {
            "location": "/data/preparation/#shuffling-and-sorting", 
            "text": "By default, OpenNMT both shuffles and sorts the data before the training. This process comes from 2 constraints of batch training:   shuffling : sentences within a batch should come from different parts of the corpus  sorting : sentences within a batch should have the same source length (i.e. without padding to maximize efficiency)    Note  During the training, batches are also randomly selected unless the  -curriculum  option is used.", 
            "title": "Shuffling and sorting"
        }, 
        {
            "location": "/data/preparation/#sentence-length", 
            "text": "During preprocessing, too long sentences (with source longer than  -src_seq_length  or target longer than  -tgt_seq_length ) are discarded from the corpus. You can have an idea of the distribution of sentence length in your training corpus by looking at the preprocess log where a table gives percent of sentences with length 1-10, 11-20, 21-30, ..., 90+:  [04/14/17 00:40:10 INFO]  * Source Sentence Length (range of 10): [ 7% ; 35% ; 32% ; 16% ; 7% ; 0% ; 0% ; 0% ; 0% ; 0% ]\n[04/14/17 00:40:10 INFO]  * Target Sentence Length (range of 10): [ 9% ; 38% ; 30% ; 15% ; 5% ; 0% ; 0% ; 0% ; 0% ; 0% ]   Note  Limiting maximal sentence length is a key parameter to reduce the GPU memory footprint used during training: indeed the memory grows linearly with maximal sentence length.", 
            "title": "Sentence length"
        }, 
        {
            "location": "/data/word_features/", 
            "text": "OpenNMT supports additional features on source and target words in the form of \ndiscrete labels\n.\n\n\n\n\nOn the source side, these features act as \nadditional information\n to the encoder. An\nembedding will be optimized for each label and then fed as additional source input\nalongside the word it annotates.\n\n\nOn the target side, these features will be \npredicted\n by the network. The\ndecoder is then able to decode a sentence and annotate each word.\n\n\n\n\nTo use additional features, directly modify your data by appending labels to each word with\nthe special character \n\uffe8\n (unicode character FFE8). There can be an \narbitrary number\n of additional\nfeatures in the form \nword\uffe8feat1\uffe8feat2\uffe8...\uffe8featN\n but each word must have the same number of\nfeatures and in the same order. Source and target data can have a different number of additional features.\n\n\nAs an example, see \ndata/src-train-case.txt\n which uses a separate feature\nto represent the case of each word. Using case as a feature is a way to optimize the word\ndictionary (no duplicated words like \"the\" and \"The\") and gives the system an additional\ninformation that can be useful to optimize its objective function.\n\n\nit\uffe8C is\uffe8l not\uffe8l acceptable\uffe8l that\uffe8l ,\uffe8n with\uffe8l the\uffe8l help\uffe8l of\uffe8l the\uffe8l national\uffe8l bureaucracies\uffe8l ,\uffe8n parliament\uffe8C \napos;s\uffe8l legislative\uffe8l prerogative\uffe8l should\uffe8l be\uffe8l made\uffe8l null\uffe8l and\uffe8l void\uffe8l by\uffe8l means\uffe8l of\uffe8l implementing\uffe8l provisions\uffe8l whose\uffe8l content\uffe8l ,\uffe8n purpose\uffe8l and\uffe8l extent\uffe8l are\uffe8l not\uffe8l laid\uffe8l down\uffe8l in\uffe8l advance\uffe8l .\uffe8n\n\n\n\n\n\nYou can generate this case feature with OpenNMT's tokenization script and the \n-case_feature\n flag.\n\n\nTime-shifting\n\n\nBy default, word features on the target side are automatically shifted compared to the words so that their prediction directly depends on the word they annotate. This way, the decoder architecture is similar to a RNN-based sequence tagger with the output of a timestep being the tag of the input.\n\n\nMore precisely at timestep \nt\n:\n\n\n\n\nthe inputs are \nwords^{(t)}\n and \nfeatures^{(t-1)}\n\n\n\n\nthe outputs are \nwords^{(t+1)}\n and \nfeatures^{(t)}\n\n\n\n\n\n\nTo reuse available vocabulary, \nfeatures^{(-1)}\n is set to the end of sentence token.\n\n\nVocabularies\n\n\nBy default, features vocabulary size is unlimited. Depending on the type of features you are using, you may want to limit their vocabulary during the preprocessing with the \n-src_vocab_size\n and \n-tgt_vocab_size\n options in the format \nword_vocab_size[ feat1_vocab_size[ feat2_vocab_size[ ...]]]\n. For example:\n\n\n# unlimited source features vocabulary size\n\n-src_vocab_size \n50000\n\n\n\n# first feature vocabulary is limited to 60, others are unlimited\n\n-src_vocab_size \n50000\n \n60\n\n\n\n# second feature vocabulary is limited to 100, others are unlimited\n\n-src_vocab_size \n50000\n \n0\n \n100\n\n\n\n# limit vocabulary size of the first and second feature\n\n-src_vocab_size \n50000\n \n60\n \n100\n\n\n\n\n\n\nYou can similarly use \n-src_words_min_frequency\n and \n-tgt_words_min_frequency\n to limit vocabulary by frequency instead of absolute size.\n\n\nLike words, word features vocabularies can be reused across datasets with the \n-features_vocabs_prefix\n. For example, if the processing generates theses features dictionaries:\n\n\n\n\ndata/demo.source_feature_1.dict\n\n\ndata/demo.source_feature_2.dict\n\n\ndata/demo.source_feature_3.dict\n\n\n\n\nyou have to set \n-features_vocabs_prefix data/demo\n as command line option.\n\n\nEmbeddings\n\n\nThe feature embedding size is automatically computed based on the number of values the feature takes. This default size reduction works well for features with few values like the case or POS.\n\n\nFor other features, you may want to manually choose the embedding size with the \n-src_word_vec_size\n and \n-tgt_word_vec_size\n options. They behave similarly to \n-src_vocab_size\n with a list of embedding size: \nword_vec_size[ feat1_vec_size[ feat2_vec_size[ ...]]]\n.\n\n\nThen, each feature embedding is concatenated to each other by default. You can instead choose to sum them by setting \n-feat_merge sum\n. Finally, the resulting merged embedding is concatenated to the word embedding.\n\n\n\n\nWarning\n\n\nIn the \nsum\n case, each feature embedding must have the same dimension. You can set the common embedding size with \n-feat_vec_size\n.\n\n\n\n\nBeam search\n\n\nDuring decoding, the beam search is only applied on the target words space and not on the word features. When the beam path is complete, the associated features are selected along this path.", 
            "title": "Word features"
        }, 
        {
            "location": "/data/word_features/#time-shifting", 
            "text": "By default, word features on the target side are automatically shifted compared to the words so that their prediction directly depends on the word they annotate. This way, the decoder architecture is similar to a RNN-based sequence tagger with the output of a timestep being the tag of the input.  More precisely at timestep  t :   the inputs are  words^{(t)}  and  features^{(t-1)}   the outputs are  words^{(t+1)}  and  features^{(t)}    To reuse available vocabulary,  features^{(-1)}  is set to the end of sentence token.", 
            "title": "Time-shifting"
        }, 
        {
            "location": "/data/word_features/#vocabularies", 
            "text": "By default, features vocabulary size is unlimited. Depending on the type of features you are using, you may want to limit their vocabulary during the preprocessing with the  -src_vocab_size  and  -tgt_vocab_size  options in the format  word_vocab_size[ feat1_vocab_size[ feat2_vocab_size[ ...]]] . For example:  # unlimited source features vocabulary size \n-src_vocab_size  50000  # first feature vocabulary is limited to 60, others are unlimited \n-src_vocab_size  50000   60  # second feature vocabulary is limited to 100, others are unlimited \n-src_vocab_size  50000   0   100  # limit vocabulary size of the first and second feature \n-src_vocab_size  50000   60   100   You can similarly use  -src_words_min_frequency  and  -tgt_words_min_frequency  to limit vocabulary by frequency instead of absolute size.  Like words, word features vocabularies can be reused across datasets with the  -features_vocabs_prefix . For example, if the processing generates theses features dictionaries:   data/demo.source_feature_1.dict  data/demo.source_feature_2.dict  data/demo.source_feature_3.dict   you have to set  -features_vocabs_prefix data/demo  as command line option.", 
            "title": "Vocabularies"
        }, 
        {
            "location": "/data/word_features/#embeddings", 
            "text": "The feature embedding size is automatically computed based on the number of values the feature takes. This default size reduction works well for features with few values like the case or POS.  For other features, you may want to manually choose the embedding size with the  -src_word_vec_size  and  -tgt_word_vec_size  options. They behave similarly to  -src_vocab_size  with a list of embedding size:  word_vec_size[ feat1_vec_size[ feat2_vec_size[ ...]]] .  Then, each feature embedding is concatenated to each other by default. You can instead choose to sum them by setting  -feat_merge sum . Finally, the resulting merged embedding is concatenated to the word embedding.   Warning  In the  sum  case, each feature embedding must have the same dimension. You can set the common embedding size with  -feat_vec_size .", 
            "title": "Embeddings"
        }, 
        {
            "location": "/data/word_features/#beam-search", 
            "text": "During decoding, the beam search is only applied on the target words space and not on the word features. When the beam path is complete, the associated features are selected along this path.", 
            "title": "Beam search"
        }, 
        {
            "location": "/training/models/", 
            "text": "In addition to standard dimension settings like the number of layers, the hidden dimension size, etc., OpenNMT also provides various model architecture.\n\n\nEncoders\n\n\nDefault encoder\n\n\nThe default encoder is a simple recurrent neural network (LSTM or GRU).\n\n\nBidirectional encoder\n\n\nThe bidirectional encoder (\n-brnn\n) consists of two independent encoders: one encoding the normal sequence and the other the reversed sequence. The output and final states are concatenated or summed depending on the \n-brnn_merge\n option.\n\n\n\n\nPyramidal deep bidirectional encoder\n\n\nThe pyramidal deep bidirectional encoder (\n-pdbrnn\n) is an alternative bidirectional encoder that reduces the time dimension after \neach\n layer based on the \n-pdbrnn_reduction\n factor and using \n-pdbrnn_merge\n as the reduction action (sum or concatenation).\n\n\n\n\nDeep bidirectional encoder\n\n\nThe deep bidirectional encoder (\n-dbrnn\n) is an alternative bidirectional encoder where the outputs of every layers are summed (or concatenated) prior feeding to the next layer. It a special case of a pyramidal deep bidirectional encoder without time reduction (i.e. \n-pdbrnn_reduction = 1\n).\n\n\n\n\nDecoders\n\n\nDefault decoder\n\n\nThe default decoder applies attention over the source sequence and implements input feeding by default.\n\n\nInput feeding is an approach to feed attentional vectors \"\nas inputs to the next time steps to inform the model about past alignment decisions\n\" (\nLuong et al. (2015)\n). This can be disabled by setting \n-input_feed 0\n.\n\n\n\n\nResidual connections\n\n\nWith residual connections the input of a layer is element-wise added to the output before feeding to the next layer. This approach proved to be useful for the gradient flow with deep RNN stacks (more than 4 layers).\n\n\n\n\nThe following components support residual connections with the \n-residual\n flag:\n\n\n\n\ndefault encoder\n\n\nbidirectional encoder\n\n\ndefault decoder\n\n\n\n\nBridges\n\n\nA bridge is an additional layer between the encoder and the decoder that defines how to pass the encoder states to the decoder. It can be one of the following:\n\n\n\n\n-bridge copy\n (default): the encoder states are copied\n\n\n-bridge dense\n: the encoder states are forwaded through a dense layer\n\n\n-bridge dense_nonlinear\n: the encoder states are forwaded through a dense layer followed by a non-linearity, here \ntanh\n\n\n\n\n-bridge none\n: the encoder states are not passed and the decoder initial states are set to zero\n\n\n\n\nWith the \ncopy\n bridge, encoder and decoder should have the same structure (number of layers, final hidden size, etc.).\n\n\nAttention Model\n\n\nDifferent models are available from \nLuong (2015)\n \"Global Attention Model\".\n\n\n\n\nwhere:\n\n\n\n\na_t(s) = \\frac{\\mathrm{exp}(\\mathrm{score}(h_t,\\bar{h}_s))}{\\sum_{s'} \\mathrm{exp}(\\mathrm{score}(h_t,\\bar{h}_s)}\n\n\n\n\nand the score function is one of these:\n\n\n\n\ndot\n: \n\\mathrm{score}(h_t,\\bar{h}_s)=h_t^T\\bar{h}_s\n\n\n\n\ngeneral\n: \n\\mathrm{score}(h_t,\\bar{h}_s)=h_t^TW_a\\bar{h}_s\n\n\n\n\nconcat\n: \n\\mathrm{score}(h_t,\\bar{h}_s)=\\nu_a^T.\\mathrm{tanh}(W_a[h_t;\\bar{h}_s])\n\n\n\n\n\n\nThe model is selected using \n-global_attention\n option or can be disabled with \n-attention none\n option. The default attention model is \ngeneral\n.", 
            "title": "Models"
        }, 
        {
            "location": "/training/models/#encoders", 
            "text": "", 
            "title": "Encoders"
        }, 
        {
            "location": "/training/models/#default-encoder", 
            "text": "The default encoder is a simple recurrent neural network (LSTM or GRU).", 
            "title": "Default encoder"
        }, 
        {
            "location": "/training/models/#bidirectional-encoder", 
            "text": "The bidirectional encoder ( -brnn ) consists of two independent encoders: one encoding the normal sequence and the other the reversed sequence. The output and final states are concatenated or summed depending on the  -brnn_merge  option.", 
            "title": "Bidirectional encoder"
        }, 
        {
            "location": "/training/models/#pyramidal-deep-bidirectional-encoder", 
            "text": "The pyramidal deep bidirectional encoder ( -pdbrnn ) is an alternative bidirectional encoder that reduces the time dimension after  each  layer based on the  -pdbrnn_reduction  factor and using  -pdbrnn_merge  as the reduction action (sum or concatenation).", 
            "title": "Pyramidal deep bidirectional encoder"
        }, 
        {
            "location": "/training/models/#deep-bidirectional-encoder", 
            "text": "The deep bidirectional encoder ( -dbrnn ) is an alternative bidirectional encoder where the outputs of every layers are summed (or concatenated) prior feeding to the next layer. It a special case of a pyramidal deep bidirectional encoder without time reduction (i.e.  -pdbrnn_reduction = 1 ).", 
            "title": "Deep bidirectional encoder"
        }, 
        {
            "location": "/training/models/#decoders", 
            "text": "", 
            "title": "Decoders"
        }, 
        {
            "location": "/training/models/#default-decoder", 
            "text": "The default decoder applies attention over the source sequence and implements input feeding by default.  Input feeding is an approach to feed attentional vectors \" as inputs to the next time steps to inform the model about past alignment decisions \" ( Luong et al. (2015) ). This can be disabled by setting  -input_feed 0 .", 
            "title": "Default decoder"
        }, 
        {
            "location": "/training/models/#residual-connections", 
            "text": "With residual connections the input of a layer is element-wise added to the output before feeding to the next layer. This approach proved to be useful for the gradient flow with deep RNN stacks (more than 4 layers).   The following components support residual connections with the  -residual  flag:   default encoder  bidirectional encoder  default decoder", 
            "title": "Residual connections"
        }, 
        {
            "location": "/training/models/#bridges", 
            "text": "A bridge is an additional layer between the encoder and the decoder that defines how to pass the encoder states to the decoder. It can be one of the following:   -bridge copy  (default): the encoder states are copied  -bridge dense : the encoder states are forwaded through a dense layer  -bridge dense_nonlinear : the encoder states are forwaded through a dense layer followed by a non-linearity, here  tanh   -bridge none : the encoder states are not passed and the decoder initial states are set to zero   With the  copy  bridge, encoder and decoder should have the same structure (number of layers, final hidden size, etc.).", 
            "title": "Bridges"
        }, 
        {
            "location": "/training/models/#attention-model", 
            "text": "Different models are available from  Luong (2015)  \"Global Attention Model\".   where:   a_t(s) = \\frac{\\mathrm{exp}(\\mathrm{score}(h_t,\\bar{h}_s))}{\\sum_{s'} \\mathrm{exp}(\\mathrm{score}(h_t,\\bar{h}_s)}   and the score function is one of these:   dot :  \\mathrm{score}(h_t,\\bar{h}_s)=h_t^T\\bar{h}_s   general :  \\mathrm{score}(h_t,\\bar{h}_s)=h_t^TW_a\\bar{h}_s   concat :  \\mathrm{score}(h_t,\\bar{h}_s)=\\nu_a^T.\\mathrm{tanh}(W_a[h_t;\\bar{h}_s])    The model is selected using  -global_attention  option or can be disabled with  -attention none  option. The default attention model is  general .", 
            "title": "Attention Model"
        }, 
        {
            "location": "/training/embeddings/", 
            "text": "Word embeddings are learned using a lookup table. Each word is assigned to a random vector within this table that is simply updated with the gradients coming from the network.\n\n\nPretrained\n\n\nWhen training with small amounts of data, performance can be improved by starting with pretrained embeddings. The arguments \n-pre_word_vecs_dec\n and \n-pre_word_vecs_enc\n can be used to specify these files.\n\n\nThe pretrained embeddings must be manually constructed Torch serialized tensors that correspond to the source and target dictionary files. For example:\n\n\nlocal\n \nvocab_size\n \n=\n \n50004\n\n\nlocal\n \nembedding_size\n \n=\n \n500\n\n\n\nlocal\n \nembeddings\n \n=\n \ntorch\n.\nTensor\n(\nvocab_size\n,\n \nembedding_size\n):\nuniform\n()\n\n\n\ntorch\n.\nsave\n(\nenc_embeddings.t7\n,\n \nembeddings\n)\n\n\n\n\n\n\nwhere \nembeddings[i]\n is the embedding of the \ni\n-th word in the vocabulary.\n\n\nTo automate this process, OpenNMT provides a script \ntools/embeddings.lua\n than can download pretrained embeddings from \nPolyglot\n or convert trained embeddings from \nword2vec\n, \nGloVe\n or \nFastText\n with regard to the word vocabularies generated by \npreprocess.lua\n. Supported format are:\n\n\n\n\nword2vec-bin\n (default): binary format generated by word2vec.\n\n\nword2vec-txt\n: textual word2vec format - starts with header line containing number of words and embedding size, and is then followed by one line per embedding: the first token is the word, and following fields are the embeddings values.\n\n\nglove\n: text format - same format than \nword2vec-txt\n but without header line.\n\n\n\n\n\n\nNote\n\n\nThe script requires the \nlua-zlib\n package.\n\n\n\n\nFor example, to generate pretrained English words embeddings:\n\n\nth tools/embeddings.lua -lang en -dict_file data/demo.src.dict -save_data data/demo-src-emb\n\n\n\n\n\n\n\nNote\n\n\nLanguages codes are Polygot's \nWikipedia Language Codes\n.\n\n\n\n\nOr to map pretrained \nword2vec\n vectors to the built vocabulary:\n\n\nth tools/embeddings.lua -embed_type word2vec -embed_file data/GoogleNews-vectors-negative300.bin -dict_file data/demo.src.dict\n\\\n\n                        -save_data data/demo-src-emb\n\n\n\n\n\n\n\nTip\n\n\nIf vocabs as-is are not found in the embeddings file, you can use \n-approximate\n option to also look for uppercase variants and variants without possible joiner marks. You can dump the non found vocabs by setting \n-save_unknown_dict\n parameter.\n\n\n\n\nFixed\n\n\nBy default these embeddings will be updated during training, but they can be held fixed using \n-fix_word_vecs_enc\n and \n-fix_word_vecs_dec\n options. These options can be enabled or disabled during a retraining.\n\n\n\n\nTip\n\n\nWhen using pretrained word embeddings, if you declare a larger \n-word_vec_size\n then the difference is uniformally initalized and you can use \n-fix_word_vecs_enc pretrained\n (or \n-fix_word_vecs_dec pretrained\n) to fix the pretrained part and optimize the remaining part.\n\n\n\n\nExtraction\n\n\nThe \ntools/extract_embeddings.lua\n script can be used to extract the model word embeddings into text files. They can then be easily transformed into another format for visualization or processing.", 
            "title": "Embeddings"
        }, 
        {
            "location": "/training/embeddings/#pretrained", 
            "text": "When training with small amounts of data, performance can be improved by starting with pretrained embeddings. The arguments  -pre_word_vecs_dec  and  -pre_word_vecs_enc  can be used to specify these files.  The pretrained embeddings must be manually constructed Torch serialized tensors that correspond to the source and target dictionary files. For example:  local   vocab_size   =   50004  local   embedding_size   =   500  local   embeddings   =   torch . Tensor ( vocab_size ,   embedding_size ): uniform ()  torch . save ( enc_embeddings.t7 ,   embeddings )   where  embeddings[i]  is the embedding of the  i -th word in the vocabulary.  To automate this process, OpenNMT provides a script  tools/embeddings.lua  than can download pretrained embeddings from  Polyglot  or convert trained embeddings from  word2vec ,  GloVe  or  FastText  with regard to the word vocabularies generated by  preprocess.lua . Supported format are:   word2vec-bin  (default): binary format generated by word2vec.  word2vec-txt : textual word2vec format - starts with header line containing number of words and embedding size, and is then followed by one line per embedding: the first token is the word, and following fields are the embeddings values.  glove : text format - same format than  word2vec-txt  but without header line.    Note  The script requires the  lua-zlib  package.   For example, to generate pretrained English words embeddings:  th tools/embeddings.lua -lang en -dict_file data/demo.src.dict -save_data data/demo-src-emb   Note  Languages codes are Polygot's  Wikipedia Language Codes .   Or to map pretrained  word2vec  vectors to the built vocabulary:  th tools/embeddings.lua -embed_type word2vec -embed_file data/GoogleNews-vectors-negative300.bin -dict_file data/demo.src.dict \\ \n                        -save_data data/demo-src-emb   Tip  If vocabs as-is are not found in the embeddings file, you can use  -approximate  option to also look for uppercase variants and variants without possible joiner marks. You can dump the non found vocabs by setting  -save_unknown_dict  parameter.", 
            "title": "Pretrained"
        }, 
        {
            "location": "/training/embeddings/#fixed", 
            "text": "By default these embeddings will be updated during training, but they can be held fixed using  -fix_word_vecs_enc  and  -fix_word_vecs_dec  options. These options can be enabled or disabled during a retraining.   Tip  When using pretrained word embeddings, if you declare a larger  -word_vec_size  then the difference is uniformally initalized and you can use  -fix_word_vecs_enc pretrained  (or  -fix_word_vecs_dec pretrained ) to fix the pretrained part and optimize the remaining part.", 
            "title": "Fixed"
        }, 
        {
            "location": "/training/embeddings/#extraction", 
            "text": "The  tools/extract_embeddings.lua  script can be used to extract the model word embeddings into text files. They can then be easily transformed into another format for visualization or processing.", 
            "title": "Extraction"
        }, 
        {
            "location": "/training/logs/", 
            "text": "During the training, some information are displayed every \n-report_every\n iterations. These logs are usually needed to evaluate the training progress, efficiency and convergence.\n\n\n\n\nNote\n\n\nMeasurements are reported as an average since the previous print.\n\n\n\n\nPerplexity\n\n\nA key information is the \ntraining perplexity\n defined by:\n\n\n\n\nppl(X,Y)=\\exp(\\frac{-\\sum_{i=1}^{|Y|}\\log P(y_i|y_{i-1},\\dotsc,y_{1},X)}{|Y|})\n\n\n\n\nwith \nX\n being the source sequence, \nY\n the true target sequence and \ny_i\n the \ni\n-th target word. The numerator is the negative log likelihood and the loss function value.\n\n\nYou want the perplexity to go down and be low in which case it means your model fits well the training data. At the end of an epoch, the logs report the \nvalidation perplexity\n with the same formula but applied on the validation data. It shows how well your model fits unseen data.\n\n\n\n\nNote\n\n\nDuring evaluation on the validation dataset, dropout is turned off.\n\n\n\n\nLogs management\n\n\nSome advanced options are available to manage your logs like using a file (\n-log_file\n), or disabling them entirely (\n-disable_logs\n). See the options of the script to learn about them.", 
            "title": "Logs"
        }, 
        {
            "location": "/training/logs/#perplexity", 
            "text": "A key information is the  training perplexity  defined by:   ppl(X,Y)=\\exp(\\frac{-\\sum_{i=1}^{|Y|}\\log P(y_i|y_{i-1},\\dotsc,y_{1},X)}{|Y|})   with  X  being the source sequence,  Y  the true target sequence and  y_i  the  i -th target word. The numerator is the negative log likelihood and the loss function value.  You want the perplexity to go down and be low in which case it means your model fits well the training data. At the end of an epoch, the logs report the  validation perplexity  with the same formula but applied on the validation data. It shows how well your model fits unseen data.   Note  During evaluation on the validation dataset, dropout is turned off.", 
            "title": "Perplexity"
        }, 
        {
            "location": "/training/logs/#logs-management", 
            "text": "Some advanced options are available to manage your logs like using a file ( -log_file ), or disabling them entirely ( -disable_logs ). See the options of the script to learn about them.", 
            "title": "Logs management"
        }, 
        {
            "location": "/training/multi_gpu/", 
            "text": "OpenNMT can make use of multiple GPU during the training by implementing \ndata parallelism\n. This technique trains batches in parallel on different network replicas. To use data parallelism, assign a list of GPU identifiers to the \n-gpuid\n option. For example:\n\n\nth train.lua -data data/demo-train.t7 -save_model demo -gpuid \n1\n \n2\n \n4\n\n\n\n\n\n\nwill use the first, the second and the fourth GPU of the machine as returned by the CUDA API.\n\n\n\n\nNote\n\n\nnvidia-smi\n enumerates devices based on the driver API which can be in a different order than the CUDA API.\n\n\n\n\nSynchronous\n\n\nIn this default mode, each replica processes in parallel a different batch at each iteration. The gradients from each replica are accumulated, and parameters updated and synchronized.\n\n\n\n\nWarning\n\n\nWhen using \nN\n GPU(s), the actual batch size is \nN \\times\n\n\n-max_batch_size\n.\n\n\n\n\nAsynchronous\n\n\n(Also known as asynchronous SGD or downpour SGD.)\n\n\nIn this mode enabled with the \n-async_parallel\n flag, the different replicas are independently\ncalculating their own gradients, updating a master copy of the parameters and getting updated values\nof the parameters. To enable convergence at the beginning of the training, only one replica is working for the first \n-async_parallel_minbatch\n iterations to prepare a better initialization for the asynchronous part.\n\n\n\n\nWarning\n\n\nA GPU core is dedicated to store the master copy of the parameters and is not used for training.\n\n\n\n\n\n\nNote\n\n\nAs training logs and saving require synchronization, consider using higher \n-report_every\n and \n-save_every\n values.", 
            "title": "Multi GPU"
        }, 
        {
            "location": "/training/multi_gpu/#synchronous", 
            "text": "In this default mode, each replica processes in parallel a different batch at each iteration. The gradients from each replica are accumulated, and parameters updated and synchronized.   Warning  When using  N  GPU(s), the actual batch size is  N \\times  -max_batch_size .", 
            "title": "Synchronous"
        }, 
        {
            "location": "/training/multi_gpu/#asynchronous", 
            "text": "(Also known as asynchronous SGD or downpour SGD.)  In this mode enabled with the  -async_parallel  flag, the different replicas are independently\ncalculating their own gradients, updating a master copy of the parameters and getting updated values\nof the parameters. To enable convergence at the beginning of the training, only one replica is working for the first  -async_parallel_minbatch  iterations to prepare a better initialization for the asynchronous part.   Warning  A GPU core is dedicated to store the master copy of the parameters and is not used for training.    Note  As training logs and saving require synchronization, consider using higher  -report_every  and  -save_every  values.", 
            "title": "Asynchronous"
        }, 
        {
            "location": "/training/retraining/", 
            "text": "By default, OpenNMT saves a checkpoint every 5000 iterations and at the end of each epoch. For more frequent or infrequent saves, you can use the \n-save_every\n and \n-save_every_epochs\n options which define the number of iterations and epochs after which the training saves a checkpoint.\n\n\nThere are several reasons one may want to train from a saved model with the \n-train_from\n option:\n\n\n\n\ncontinuing a stopped training\n\n\ncontinuing the training with a smaller batch size\n\n\ntraining a model on new data (incremental adaptation)\n\n\nstarting a training from pre-trained parameters\n\n\netc.\n\n\n\n\nConsiderations\n\n\nWhen training from an existing model, some settings can not be changed:\n\n\n\n\nthe model topology (layers, hidden size, etc.)\n\n\nthe vocabularies\n\n\n\n\n\n\nExceptions\n\n\n-dropout\n, \n-fix_word_vecs_enc\n and \n-fix_word_vecs_dec\n are model options that can be changed for a retraining.\n\n\n\n\nResuming a stopped training\n\n\nIt is common that a training stops: crash, server reboot, user action, etc. In this case, you may want to continue the training for more epochs by using using the \n-continue\n flag. For example:\n\n\n# start the initial training\n\nth train.lua -gpuid \n1\n -data data/demo-train.t7 -save_model demo -save_every \n50\n\n\n\n# train for several epochs...\n\n\n\n# need to reboot the server!\n\n\n\n# continue the training from the last checkpoint\n\nth train.lua -gpuid \n1\n -data data/demo-train.t7 -save_model demo -save_every \n50\n -train_from demo_checkpoint.t7 -continue\n\n\n\n\n\nThe \n-continue\n flag ensures that the training continues with the same configuration and optimization states. In particular, the following options are set to their last known value:\n\n\n\n\n-curriculum\n\n\n-decay\n\n\n-learning_rate_decay\n\n\n-learning_rate\n\n\n-max_grad_norm\n\n\n-min_learning_rate\n\n\n-optim\n\n\n-start_decay_at\n\n\n-start_decay_ppl_delta\n\n\n-start_epoch\n\n\n-start_iteration\n\n\n\n\n\n\nNote\n\n\nThe \n-end_epoch\n value is not automatically set as the user may want to continue its training for more epochs past the end.\n\n\n\n\nAdditionally, the \n-continue\n flag retrieves from the previous training:\n\n\n\n\nthe non-SGD optimizers states\n\n\nthe random generator states\n\n\nthe batch order (when continuing from an intermediate checkpoint)\n\n\n\n\nTraining from pre-trained parameters\n\n\nAnother use case it to use a base model and train it further with new training options (in particular the optimization method and the learning rate). Using \n-train_from\n without \n-continue\n will start a new training with parameters initialized from a pre-trained model.", 
            "title": "Retraining"
        }, 
        {
            "location": "/training/retraining/#considerations", 
            "text": "When training from an existing model, some settings can not be changed:   the model topology (layers, hidden size, etc.)  the vocabularies    Exceptions  -dropout ,  -fix_word_vecs_enc  and  -fix_word_vecs_dec  are model options that can be changed for a retraining.", 
            "title": "Considerations"
        }, 
        {
            "location": "/training/retraining/#resuming-a-stopped-training", 
            "text": "It is common that a training stops: crash, server reboot, user action, etc. In this case, you may want to continue the training for more epochs by using using the  -continue  flag. For example:  # start the initial training \nth train.lua -gpuid  1  -data data/demo-train.t7 -save_model demo -save_every  50  # train for several epochs...  # need to reboot the server!  # continue the training from the last checkpoint \nth train.lua -gpuid  1  -data data/demo-train.t7 -save_model demo -save_every  50  -train_from demo_checkpoint.t7 -continue  The  -continue  flag ensures that the training continues with the same configuration and optimization states. In particular, the following options are set to their last known value:   -curriculum  -decay  -learning_rate_decay  -learning_rate  -max_grad_norm  -min_learning_rate  -optim  -start_decay_at  -start_decay_ppl_delta  -start_epoch  -start_iteration    Note  The  -end_epoch  value is not automatically set as the user may want to continue its training for more epochs past the end.   Additionally, the  -continue  flag retrieves from the previous training:   the non-SGD optimizers states  the random generator states  the batch order (when continuing from an intermediate checkpoint)", 
            "title": "Resuming a stopped training"
        }, 
        {
            "location": "/training/retraining/#training-from-pre-trained-parameters", 
            "text": "Another use case it to use a base model and train it further with new training options (in particular the optimization method and the learning rate). Using  -train_from  without  -continue  will start a new training with parameters initialized from a pre-trained model.", 
            "title": "Training from pre-trained parameters"
        }, 
        {
            "location": "/training/decay/", 
            "text": "OpenNMT's training implements empirical learning rate decay strategies. Experiences showed that using stochastic gradient descent (SGD) and a decay strategy yield better performance than optimization methods with adaptive learning rates.\n\n\nLearning rate updates are always computed at the end of an epoch. When a decay condition is met, the following update rule is applied:\n\n\n\n\nlr^{(t+1)} = lr^{(t)} \\times decay\n\n\n\n\nwhere \nlr^{(0)}=\n\n\n-learning_rate\n and \ndecay=\n\n\n-learning_rate_decay\n.\n\n\nIf an epoch is a too large unit for your particular use case, consider using \ndata sampling\n. Additionally, it may be useful to set a minimum learning rate with \n-min_learning_rate\n to stop the training earlier when the learning rate is too small to make a difference.\n\n\nDefault\n\n\nBy default, the decay is applied when one of the following conditions is met:\n\n\n\n\nThe validation perplexity is not improving more than \n-start_decay_ppl_delta\n.\n\n\nThe current epoch is past \n-start_decay_at\n.\n\n\n\n\nOnce one of the conditions is met, the learning rate is decayed after \neach\n remaining epoch.\n\n\nEpoch-based\n\n\nWith the \n-decay epoch_only\n option, the learning rate is only decayed when the condition is met on the epoch:\n\n\n\n\nThe current epoch is past \n-start_decay_at\n.\n\n\n\n\nPerplexity-based\n\n\nWith the \n-decay perplexity_only\n option, the learning rate is only decayed when the condition is met on the validation perplexity:\n\n\n\n\nThe validation perplexity is not improving more than \n-start_decay_ppl_delta\n.", 
            "title": "Decay strategies"
        }, 
        {
            "location": "/training/decay/#default", 
            "text": "By default, the decay is applied when one of the following conditions is met:   The validation perplexity is not improving more than  -start_decay_ppl_delta .  The current epoch is past  -start_decay_at .   Once one of the conditions is met, the learning rate is decayed after  each  remaining epoch.", 
            "title": "Default"
        }, 
        {
            "location": "/training/decay/#epoch-based", 
            "text": "With the  -decay epoch_only  option, the learning rate is only decayed when the condition is met on the epoch:   The current epoch is past  -start_decay_at .", 
            "title": "Epoch-based"
        }, 
        {
            "location": "/training/decay/#perplexity-based", 
            "text": "With the  -decay perplexity_only  option, the learning rate is only decayed when the condition is met on the validation perplexity:   The validation perplexity is not improving more than  -start_decay_ppl_delta .", 
            "title": "Perplexity-based"
        }, 
        {
            "location": "/training/sampling/", 
            "text": "Data sampling is a technique to select a subset of the training set at each epoch. This could be a way to make the epoch unit smaller or select relevant training sequences at each epoch. There are different types of sampling that are selected using \n-sample_type\n option as defined below.\n\n\nWhen sampling, with the option \n-sample_tgt_vocab\n it is also possible to restrict the target vocabulary to the current sample which gives an approximate of the full generator as defined here \nJean et al, 2015\n through a so-called \"Importance Sampling\" approach.\n\n\n\n\nTip\n\n\nImportance Sampling is particularly useful when training systems with very large target vocabulary\n\n\n\n\nUniform\n\n\nThe simplest data sampling is to uniformly select a subset of the training data. Using the \n-sample N\n option, the training will randomly choose \nN\n training sequences at each epoch.\n\n\nA typical use case is to reduce the length of the epochs for more frequent learning rate updates and validation perplexity computation.\n\n\nPerplexity-based\n\n\nThis approach is an attempt to feed relevant training data at each epoch. When using the flag \n-sample_type perplexity\n, the perplexity of each sequence is used to generate a multinomial probability distribution over the training sequences. The higher the perplexity, the more likely the sequence is selected.\n\n\nAlternatively, perplexity-based sampling can be enabled when an average training perplexity is met with the \n-sample_perplexity_init\n option.\n\n\n\n\nWarning\n\n\nThis perplexity-based approach is experimental and effects are to be experimented. This also results in a ~10% slowdown as the perplexity of \neach\n sequence has to be independently computed.\n\n\n\n\nPartition\n\n\nWhen using the flag \n-sample_type partition\n, samples are drawn without random, uniformally and incrementally from the corpus training. Use this mode for making sure all training sequences will be sent the same number of time.", 
            "title": "Data sampling"
        }, 
        {
            "location": "/training/sampling/#uniform", 
            "text": "The simplest data sampling is to uniformly select a subset of the training data. Using the  -sample N  option, the training will randomly choose  N  training sequences at each epoch.  A typical use case is to reduce the length of the epochs for more frequent learning rate updates and validation perplexity computation.", 
            "title": "Uniform"
        }, 
        {
            "location": "/training/sampling/#perplexity-based", 
            "text": "This approach is an attempt to feed relevant training data at each epoch. When using the flag  -sample_type perplexity , the perplexity of each sequence is used to generate a multinomial probability distribution over the training sequences. The higher the perplexity, the more likely the sequence is selected.  Alternatively, perplexity-based sampling can be enabled when an average training perplexity is met with the  -sample_perplexity_init  option.   Warning  This perplexity-based approach is experimental and effects are to be experimented. This also results in a ~10% slowdown as the perplexity of  each  sequence has to be independently computed.", 
            "title": "Perplexity-based"
        }, 
        {
            "location": "/training/sampling/#partition", 
            "text": "When using the flag  -sample_type partition , samples are drawn without random, uniformally and incrementally from the corpus training. Use this mode for making sure all training sequences will be sent the same number of time.", 
            "title": "Partition"
        }, 
        {
            "location": "/translation/inference/", 
            "text": "Release models\n\n\nAfter training a model, you may want to release it for inference only by using the \ntools/release_model.lua\n script. A released model takes less space on disk and is compatible with both CPU and GPU translation.\n\n\nth tools/release_model.lua -model model.t7 -gpuid \n1\n\n\n\n\n\n\nBy default, it will create a \nmodel_release.t7\n file. See \nth tools/release_model.lua -h\n for advanced options.\n\n\n\n\nWarning\n\n\nA GPU is required to load non released models and released models can no longer be used for training.\n\n\n\n\nInference engine\n\n\nCTranslate is a C++ implementation of \ntranslate.lua\n for integration in existing products. Take a look at the \nGitHub project\n for more information.", 
            "title": "Inference"
        }, 
        {
            "location": "/translation/inference/#release-models", 
            "text": "After training a model, you may want to release it for inference only by using the  tools/release_model.lua  script. A released model takes less space on disk and is compatible with both CPU and GPU translation.  th tools/release_model.lua -model model.t7 -gpuid  1   By default, it will create a  model_release.t7  file. See  th tools/release_model.lua -h  for advanced options.   Warning  A GPU is required to load non released models and released models can no longer be used for training.", 
            "title": "Release models"
        }, 
        {
            "location": "/translation/inference/#inference-engine", 
            "text": "CTranslate is a C++ implementation of  translate.lua  for integration in existing products. Take a look at the  GitHub project  for more information.", 
            "title": "Inference engine"
        }, 
        {
            "location": "/translation/beam_search/", 
            "text": "By default, translation is done using beam search. The \n-beam_size\n option can be used to trade-off translation time and search accuracy, with \n-beam_size 1\n giving greedy search. The small default beam size is often enough in practice.\n\n\nBeam search can also be used to provide an approximate n-best list of translations by setting \n-n_best\n greater than 1. For analysis, the translation command also takes an oracle/gold \n-tgt\n file and will output a comparison of scores.\n\n\nHypotheses filtering\n\n\nThe beam search provides a built-in filter based on unknown words: \n-max_num_unks\n. Hypotheses with more unknown words than this value are dropped.\n\n\n\n\nNote\n\n\nAs dropped hypotheses temporarily reduce the beam size, the \n-pre_filter_factor\n is a way to increase the number of considered hypotheses before applying filters.\n\n\n\n\nNormalization\n\n\nThe beam search also supports various normalization techniques that are disabled by default and can be used to biased the scores generated by the model:\n\n\n\n\ns(Y,X)=\\frac{\\log P(Y|X)}{lp(Y)}+cp(X,Y)\n\n\n\n\nwhere \nX\n is the source, \nY\n is the current target, and the functions as defined below. An additional penalty on end of sentence tokens can also be added to prioritize longer sentences.\n\n\nLength normalization\n\n\nScores are normalized by the following formula as defined in \nWu et al. (2016)\n:\n\n\n\n\nlp(Y) = \\frac{(5+|Y|)^\\alpha}{(5+1)^\\alpha}\n\n\n\n\nwhere \n|Y|\n is the current target length and \n\\alpha\n is the length normalization coefficient \n-length_norm\n.\n\n\nCoverage normalization\n\n\nScores are penalized by the following formula as defined in \nWu et al. (2016)\n:\n\n\n\n\ncp(X,Y) = \\beta\\sum_{i=1}^{|X|}\\log(\\min(\\sum_{j=1}^{|Y|}p_{i,j},1.0))\n\n\n\n\nwhere \np_{i,j}\n is the attention probability of the \nj\n-th target word \ny_j\n on the \ni\n-th source word \nx_i\n, \n|X|\n is the source length, \n|Y|\n is the current target length and \n\\beta\n is the coverage normalization coefficient \n-coverage_norm\n.\n\n\nEnd of sentence normalization\n\n\nThe score of the end of sentence token is penalized by the following formula:\n\n\n\n\nep(X,Y)=\\gamma\\frac{|X|}{|Y|}\n\n\n\n\nwhere \n|X|\n is the source length, \n|Y|\n is the current target length and \n\\gamma\n is the coverage normalization coefficient \n-eos_norm\n.", 
            "title": "Beam search"
        }, 
        {
            "location": "/translation/beam_search/#hypotheses-filtering", 
            "text": "The beam search provides a built-in filter based on unknown words:  -max_num_unks . Hypotheses with more unknown words than this value are dropped.   Note  As dropped hypotheses temporarily reduce the beam size, the  -pre_filter_factor  is a way to increase the number of considered hypotheses before applying filters.", 
            "title": "Hypotheses filtering"
        }, 
        {
            "location": "/translation/beam_search/#normalization", 
            "text": "The beam search also supports various normalization techniques that are disabled by default and can be used to biased the scores generated by the model:   s(Y,X)=\\frac{\\log P(Y|X)}{lp(Y)}+cp(X,Y)   where  X  is the source,  Y  is the current target, and the functions as defined below. An additional penalty on end of sentence tokens can also be added to prioritize longer sentences.", 
            "title": "Normalization"
        }, 
        {
            "location": "/translation/beam_search/#length-normalization", 
            "text": "Scores are normalized by the following formula as defined in  Wu et al. (2016) :   lp(Y) = \\frac{(5+|Y|)^\\alpha}{(5+1)^\\alpha}   where  |Y|  is the current target length and  \\alpha  is the length normalization coefficient  -length_norm .", 
            "title": "Length normalization"
        }, 
        {
            "location": "/translation/beam_search/#coverage-normalization", 
            "text": "Scores are penalized by the following formula as defined in  Wu et al. (2016) :   cp(X,Y) = \\beta\\sum_{i=1}^{|X|}\\log(\\min(\\sum_{j=1}^{|Y|}p_{i,j},1.0))   where  p_{i,j}  is the attention probability of the  j -th target word  y_j  on the  i -th source word  x_i ,  |X|  is the source length,  |Y|  is the current target length and  \\beta  is the coverage normalization coefficient  -coverage_norm .", 
            "title": "Coverage normalization"
        }, 
        {
            "location": "/translation/beam_search/#end-of-sentence-normalization", 
            "text": "The score of the end of sentence token is penalized by the following formula:   ep(X,Y)=\\gamma\\frac{|X|}{|Y|}   where  |X|  is the source length,  |Y|  is the current target length and  \\gamma  is the coverage normalization coefficient  -eos_norm .", 
            "title": "End of sentence normalization"
        }, 
        {
            "location": "/translation/unknowns/", 
            "text": "The default translation mode allows the model to produce the \nunk\n symbol when it is not sure of the specific target word.\n\n\nOften times \nunk\n symbols will correspond to proper names that can be directly transposed between languages. The \n-replace_unk\n option will substitute \nunk\n with source words that have the highest attention weight.\n\n\nPhrase table\n\n\nAlternatively, advanced users may prefer to provide a pre-constructed phrase table from an external aligner (such as \nfast_align\n) using the \n-phrase_table\n option to allow for non-identity replacement.\n\n\nInstead of copying the source token with the highest attention, it will lookup in the phrase table for a possible translation. If a valid replacement is not found only then the source token will be copied.\n\n\nThe phrase table is a file with one translation per line in the format:\n\n\nsource|||target\n\n\n\n\n\nWhere \nsource\n and \ntarget\n are \ncase sensitive\n and \nsingle\n tokens.\n\n\nWorkarounds\n\n\nSeveral techniques exist to minimize the out-of-vocabulary issue:\n\n\n\n\nsub-tokenization like BPE or \"wordpiece\" to simulate \nopen\n vocabularies\n\n\nmixed word/characters model as described in \nWu et al. (2016)", 
            "title": "Unknown words"
        }, 
        {
            "location": "/translation/unknowns/#phrase-table", 
            "text": "Alternatively, advanced users may prefer to provide a pre-constructed phrase table from an external aligner (such as  fast_align ) using the  -phrase_table  option to allow for non-identity replacement.  Instead of copying the source token with the highest attention, it will lookup in the phrase table for a possible translation. If a valid replacement is not found only then the source token will be copied.  The phrase table is a file with one translation per line in the format:  source|||target  Where  source  and  target  are  case sensitive  and  single  tokens.", 
            "title": "Phrase table"
        }, 
        {
            "location": "/translation/unknowns/#workarounds", 
            "text": "Several techniques exist to minimize the out-of-vocabulary issue:   sub-tokenization like BPE or \"wordpiece\" to simulate  open  vocabularies  mixed word/characters model as described in  Wu et al. (2016)", 
            "title": "Workarounds"
        }, 
        {
            "location": "/tools/tokenization/", 
            "text": "OpenNMT provides generic tokenization utilities to quickly process new training data.\n\n\n\n\nNote\n\n\nFor LuaJIT users, tokenization tools require the \nbit32\n package.\n\n\n\n\nTokenization\n\n\nTo tokenize a corpus:\n\n\nth tools/tokenize.lua OPTIONS \n file \n file.tok\n\n\n\n\n\nDetokenization\n\n\nIf you activate \n-joiner_annotate\n marker, the tokenization is reversible. Just use:\n\n\nth tools/detokenize.lua OPTIONS \n file.tok \n file.detok\n\n\n\n\n\nSpecial characters\n\n\n\n\n\uffe8\n is the feature separator symbol. If such character is used in source text, it is replaced by its non presentation form \n\u2502\n.\n\n\n\uffed\n is the default joiner marker (generated in \n-joiner_annotate marker\n mode). If such character is used in source text, it is replaced by its non presentation form \n\u25a0\n\n\n\n\nMixed casing words\n\n\n-segment_case\n feature enables tokenizer to segment words into subwords with one of 3 casing types (truecase ('House'), uppercase ('HOUSE') or lowercase ('house')), which helps  restore right casing during  detokenization. This feature is especially useful for texts with a signficant number of words with mixed casing ('WiFi' -\n 'Wi' and 'Fi').\n\n\nWiFi --\n wi\uffe8C fi\uffe8C\nTVs --\n tv\uffe8U s\uffe8L\n\n\n\n\n\nBPE\n\n\nOpenNMT's BPE module fully supports the \noriginal BPE\n as default mode:\n\n\ntools/learn_bpe.lua -size \n30000\n -save_bpe codes \n input\ntools/tokenize.lua -bpe_model codes \n input\n\n\n\n\n\nwith two additional features:\n\n\n1. Add support for different modes of handling prefixes and/or suffixes: \n-bpe_mode\n\n\n\n\nsuffix\n: BPE merge operations are learnt to distinguish sub-tokens like \"ent\" in the middle of a word and \"ent\n\\w\n\" at the end of a word. \"\n\\w\n\" is an artificial marker appended to the end of each token input and treated as a single unit before doing statistics on bigrams. This is the default mode which is useful for most of the languages.\n\n\nprefix\n: BPE merge operations are learnt to distinguish sub-tokens like \"ent\" in the middle of a word and \"\nw>ent\" at the beginning of a word. \"\nw>\" is an artificial marker appended to the beginning of each token input and treated as a single unit before doing statistics on bigrams.\n\n\nboth\n: \nsuffix\n + \nprefix\n\n\nnone\n: No artificial marker is appended to input tokens, a sub-token is treated equally whether it is in the middle or at the beginning or at the end of a token.\n\n\n\n\n2. Add support for BPE in addition to the case feature: \n-bpe_case_insensitive\n\n\nOpenNMT's tokenization flow first applies BPE then add the case feature for each input token. With the standard BPE, \"Constitution\" and \"constitution\" may result in the different sequences of sub-tokens:\n\n\nConstitution --\n con\uffe8C sti\uffe8l tu\uffe8l tion\uffe8l\nconstitution --\n consti\uffe8l tu\uffe8l tion\uffe8l\n\n\n\n\n\nIf you want a \ncaseless\n split so that you can take the best from using case feature, and you can achieve that with the following command lines:\n\n\n# We don\nt need BPE to care about case\n\ntools/learn_bpe.lua -size \n30000\n -save_bpe codes_lc \n input_lowercased\n\n\n# The case information is preserved in the true case input\n\ntools/tokenize.lua -bpe_model codes_lc -bpe_case_insensitive \n input\n\n\n\n\n\nThe output of the previous example would be:\n\n\nConstitution --\n con\uffe8C sti\uffe8l tu\uffe8l tion\uffe8l\nconstitution --\n con\uffe8l sti\uffe8l tu\uffe8l tion\uffe8l\n\n\n\n\n\n\n\nNote\n\n\nUse Lua 5.2 if you encounter any memory issue while using \nlearn_bpe.lua\n (e.g. \n-size\n is too big). Otherwise, stay with Lua 5.1 for better efficiency.", 
            "title": "Tokenization"
        }, 
        {
            "location": "/tools/tokenization/#tokenization", 
            "text": "To tokenize a corpus:  th tools/tokenize.lua OPTIONS   file   file.tok", 
            "title": "Tokenization"
        }, 
        {
            "location": "/tools/tokenization/#detokenization", 
            "text": "If you activate  -joiner_annotate  marker, the tokenization is reversible. Just use:  th tools/detokenize.lua OPTIONS   file.tok   file.detok", 
            "title": "Detokenization"
        }, 
        {
            "location": "/tools/tokenization/#special-characters", 
            "text": "\uffe8  is the feature separator symbol. If such character is used in source text, it is replaced by its non presentation form  \u2502 .  \uffed  is the default joiner marker (generated in  -joiner_annotate marker  mode). If such character is used in source text, it is replaced by its non presentation form  \u25a0", 
            "title": "Special characters"
        }, 
        {
            "location": "/tools/tokenization/#mixed-casing-words", 
            "text": "-segment_case  feature enables tokenizer to segment words into subwords with one of 3 casing types (truecase ('House'), uppercase ('HOUSE') or lowercase ('house')), which helps  restore right casing during  detokenization. This feature is especially useful for texts with a signficant number of words with mixed casing ('WiFi' -  'Wi' and 'Fi').  WiFi --  wi\uffe8C fi\uffe8C\nTVs --  tv\uffe8U s\uffe8L", 
            "title": "Mixed casing words"
        }, 
        {
            "location": "/tools/tokenization/#bpe", 
            "text": "OpenNMT's BPE module fully supports the  original BPE  as default mode:  tools/learn_bpe.lua -size  30000  -save_bpe codes   input\ntools/tokenize.lua -bpe_model codes   input  with two additional features:  1. Add support for different modes of handling prefixes and/or suffixes:  -bpe_mode   suffix : BPE merge operations are learnt to distinguish sub-tokens like \"ent\" in the middle of a word and \"ent \\w \" at the end of a word. \" \\w \" is an artificial marker appended to the end of each token input and treated as a single unit before doing statistics on bigrams. This is the default mode which is useful for most of the languages.  prefix : BPE merge operations are learnt to distinguish sub-tokens like \"ent\" in the middle of a word and \" w>ent\" at the beginning of a word. \" w>\" is an artificial marker appended to the beginning of each token input and treated as a single unit before doing statistics on bigrams.  both :  suffix  +  prefix  none : No artificial marker is appended to input tokens, a sub-token is treated equally whether it is in the middle or at the beginning or at the end of a token.   2. Add support for BPE in addition to the case feature:  -bpe_case_insensitive  OpenNMT's tokenization flow first applies BPE then add the case feature for each input token. With the standard BPE, \"Constitution\" and \"constitution\" may result in the different sequences of sub-tokens:  Constitution --  con\uffe8C sti\uffe8l tu\uffe8l tion\uffe8l\nconstitution --  consti\uffe8l tu\uffe8l tion\uffe8l  If you want a  caseless  split so that you can take the best from using case feature, and you can achieve that with the following command lines:  # We don t need BPE to care about case \ntools/learn_bpe.lua -size  30000  -save_bpe codes_lc   input_lowercased # The case information is preserved in the true case input \ntools/tokenize.lua -bpe_model codes_lc -bpe_case_insensitive   input  The output of the previous example would be:  Constitution --  con\uffe8C sti\uffe8l tu\uffe8l tion\uffe8l\nconstitution --  con\uffe8l sti\uffe8l tu\uffe8l tion\uffe8l   Note  Use Lua 5.2 if you encounter any memory issue while using  learn_bpe.lua  (e.g.  -size  is too big). Otherwise, stay with Lua 5.1 for better efficiency.", 
            "title": "BPE"
        }, 
        {
            "location": "/tools/servers/", 
            "text": "OpenNMT provides simple translation servers to easily showcase your results remotely.\n\n\nREST\n\n\nYou can use an easy REST syntax to simply send plain text. Sentences will be tokenized, translated and then detokenized using OpenNMT tools.\n\n\nThe server uses the \nrestserver-xavante\n dependency, you need to install it by running:\n\n\nluarocks install restserver-xavante\n\n\n\n\n\nThe translation server can be run using any of the arguments from \ntokenize.lua\n or \ntranslate.lua\n.\n\n\nth tools/rest_translation_server.lua -model ../Recipes/baseline-1M-enfr/exp/model-baseline-1M-enfr_epoch13_3.44.t7 -gpuid \n1\n -host ... -port -case_feature -bpe_model ...\n\n\n\n\n\n\n\nNote\n\n\nThe default host is set to \n127.0.0.1\n and default port to \n7784\n.\n\n\n\n\nYou can test it with a \ncurl\n command locally or from any other client:\n\n\ncurl -v -H \nContent-Type: application/json\n -X POST -d \n[{ \nsrc\n : \nHello World\n }]\n http://IP_address:7784/translator/translate\n\n\n\n\n\nAnswer will be embedded in a JSON format, translated sentence in the \ntgt\n section. Additionally you can get the attention matrix with the \n-withAttn\n option in the server command line.\n\n\nZeroMQ\n\n\nThe server uses the 0MQ for RPC. You can install 0MQ and the Lua bindings on Ubuntu by running:\n\n\nsudo apt-get install libzmq-dev\nluarocks install dkjson\nluarocks install lua-zmq \nZEROMQ_LIBDIR\n=\n/usr/lib/x86_64-linux-gnu/ \nZEROMQ_INCDIR\n=\n/usr/include\n\n\n\n\n\nThe translation server can be run using any of the arguments from \ntranslate.lua\n.\n\n\nth tools/translation_server.lua -host ... -port ... -model ...\n\n\n\n\n\n\n\nNote\n\n\nThe default host is set to \n127.0.0.1\n which only allows local access. If you want to support remote access, use \n0.0.0.0\n instead.\n\n\n\n\nIt runs as a message queue that takes in a JSON batch of src sentences. For example the following 5 lines of Python\ncode can be used to send a single sentence for translation.\n\n\nimport\n \nzmq\n,\n \nsys\n,\n \njson\n\n\nsock\n \n=\n \nzmq\n.\nContext\n()\n.\nsocket\n(\nzmq\n.\nREQ\n)\n\n\nsock\n.\nconnect\n(\ntcp://127.0.0.1:5556\n)\n\n\nsock\n.\nsend\n(\njson\n.\ndumps\n([{\nsrc\n:\n \n \n.\njoin\n(\nsys\n.\nargv\n[\n1\n:])}]))\n\n\nprint\n \nsock\n.\nrecv\n()\n\n\n\n\n\n\nFor a longer example, see our \nPython/Flask server\n in development.", 
            "title": "Servers"
        }, 
        {
            "location": "/tools/servers/#rest", 
            "text": "You can use an easy REST syntax to simply send plain text. Sentences will be tokenized, translated and then detokenized using OpenNMT tools.  The server uses the  restserver-xavante  dependency, you need to install it by running:  luarocks install restserver-xavante  The translation server can be run using any of the arguments from  tokenize.lua  or  translate.lua .  th tools/rest_translation_server.lua -model ../Recipes/baseline-1M-enfr/exp/model-baseline-1M-enfr_epoch13_3.44.t7 -gpuid  1  -host ... -port -case_feature -bpe_model ...   Note  The default host is set to  127.0.0.1  and default port to  7784 .   You can test it with a  curl  command locally or from any other client:  curl -v -H  Content-Type: application/json  -X POST -d  [{  src  :  Hello World  }]  http://IP_address:7784/translator/translate  Answer will be embedded in a JSON format, translated sentence in the  tgt  section. Additionally you can get the attention matrix with the  -withAttn  option in the server command line.", 
            "title": "REST"
        }, 
        {
            "location": "/tools/servers/#zeromq", 
            "text": "The server uses the 0MQ for RPC. You can install 0MQ and the Lua bindings on Ubuntu by running:  sudo apt-get install libzmq-dev\nluarocks install dkjson\nluarocks install lua-zmq  ZEROMQ_LIBDIR = /usr/lib/x86_64-linux-gnu/  ZEROMQ_INCDIR = /usr/include  The translation server can be run using any of the arguments from  translate.lua .  th tools/translation_server.lua -host ... -port ... -model ...   Note  The default host is set to  127.0.0.1  which only allows local access. If you want to support remote access, use  0.0.0.0  instead.   It runs as a message queue that takes in a JSON batch of src sentences. For example the following 5 lines of Python\ncode can be used to send a single sentence for translation.  import   zmq ,   sys ,   json  sock   =   zmq . Context () . socket ( zmq . REQ )  sock . connect ( tcp://127.0.0.1:5556 )  sock . send ( json . dumps ([{ src :     . join ( sys . argv [ 1 :])}]))  print   sock . recv ()   For a longer example, see our  Python/Flask server  in development.", 
            "title": "ZeroMQ"
        }, 
        {
            "location": "/options/usage/", 
            "text": "By default, OpenNMT's scripts can only be called from the root of OpenNMT's directory. If calling the scripts from any directory is more convenient to you, you need to extend the \nLUA_PATH\n:\n\n\nexport\n \nLUA_PATH\n=\n$LUA_PATH\n;/path/to/OpenNMT/?.lua\n\n\n\n\n\n\nConfiguration files\n\n\nYou can pass options using a configuration file. The file has a simple key-value syntax with one \noption = value\n per line. Here is an example:\n\n\n$ cat generic.txt\nrnn_size = 600\nlayers = 4\nbrnn = true\nsave_model = generic\n\n\n\n\n\nIt handles empty lines and ignores lines prefixed with \n#\n.\n\n\nYou can then pass this file along other options on the command line:\n\n\nth train.lua -config generic.txt -data data/demo-train.t7 -gpuid \n1\n\n\n\n\n\n\nIf an option appears both in the file and on the command line, the file takes priority.\n\n\nBoolean options\n\n\nBoolean options can be used without argument. In this case, their presence on the command line negates their default value. For example the option \n-brnn\n enables bidirectional encoder when added to the command line.\n\n\nThey optionally accept an argument to make it more practical in scripts:\n\n\n\n\n0\n or \nfalse\n\n\n1\n or \ntrue\n\n\n\n\nMultiple arguments\n\n\nSome options can take multiple arguments (\ntable\n argument type in the option listings). You can either space-separate (\nvalue1 value2 value3\n) or comma-separate (\nvalue1,value2,value3\n) the values.", 
            "title": "Scripts usage"
        }, 
        {
            "location": "/options/usage/#configuration-files", 
            "text": "You can pass options using a configuration file. The file has a simple key-value syntax with one  option = value  per line. Here is an example:  $ cat generic.txt\nrnn_size = 600\nlayers = 4\nbrnn = true\nsave_model = generic  It handles empty lines and ignores lines prefixed with  # .  You can then pass this file along other options on the command line:  th train.lua -config generic.txt -data data/demo-train.t7 -gpuid  1   If an option appears both in the file and on the command line, the file takes priority.", 
            "title": "Configuration files"
        }, 
        {
            "location": "/options/usage/#boolean-options", 
            "text": "Boolean options can be used without argument. In this case, their presence on the command line negates their default value. For example the option  -brnn  enables bidirectional encoder when added to the command line.  They optionally accept an argument to make it more practical in scripts:   0  or  false  1  or  true", 
            "title": "Boolean options"
        }, 
        {
            "location": "/options/usage/#multiple-arguments", 
            "text": "Some options can take multiple arguments ( table  argument type in the option listings). You can either space-separate ( value1 value2 value3 ) or comma-separate ( value1,value2,value3 ) the values.", 
            "title": "Multiple arguments"
        }, 
        {
            "location": "/options/preprocess/", 
            "text": "preprocess.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nPreprocess options\n\n\n\n\n-data_type \nstring\n (accepted: \nbitext\n, \nmonotext\n, \nfeattext\n; default: \nbitext\n)\nType of data to preprocess. Use 'monotext' for monolingual data. This option impacts all options choices.\n\n\n-save_data \nstring\n (required)\nOutput file for the prepared data.\n\n\n\n\nData options\n\n\n\n\n-train_src \nstring\n (required)\nPath to the training source data.\n\n\n-train_tgt \nstring\n (required)\nPath to the training target data.\n\n\n-valid_src \nstring\n (required)\nPath to the validation source data.\n\n\n-valid_tgt \nstring\n (required)\nPath to the validation target data.\n\n\n-src_vocab \nstring\n (default: \n''\n)\nPath to an existing source vocabulary.\n\n\n-src_vocab_size \ntable\n (default: \n50000\n)\nList of source vocabularies size: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are not pruned.\n\n\n-src_words_min_frequency \ntable\n (default: \n0\n)\nList of source words min frequency: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are pruned by size.\n\n\n-tgt_vocab \nstring\n (default: \n''\n)\nPath to an existing target vocabulary.\n\n\n-tgt_vocab_size \ntable\n (default: \n50000\n)\nList of target vocabularies size: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are not pruned.\n\n\n-tgt_words_min_frequency \ntable\n (default: \n0\n)\nList of target words min frequency: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are pruned by size.\n\n\n-src_seq_length \nnumber\n (default: \n50\n)\nMaximum source sequence length.\n\n\n-tgt_seq_length \nnumber\n (default: \n50\n)\nMaximum target sequence length.\n\n\n-features_vocabs_prefix \nstring\n (default: \n''\n)\nPath prefix to existing features vocabularies.\n\n\n-time_shift_feature [\nboolean\n]\n (default: \ntrue\n)\nTime shift features on the decoder side.\n\n\n-keep_frequency [\nboolean\n]\n (default: \nfalse\n)\nKeep frequency of words in dictionary.\n\n\n-sort [\nboolean\n]\n (default: \ntrue\n)\nIf set, sort the sequences by size to build batches without source padding.\n\n\n-shuffle [\nboolean\n]\n (default: \ntrue\n)\nIf set, shuffle the data (prior sorting).\n\n\n-idx_files [\nboolean\n]\n (default: \nfalse\n)\nIf set, source and target files are 'key value' with key match between source and target.\n\n\n-report_every \nnumber\n (default: \n100000\n)\nReport status every this many sentences.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nOther options\n\n\n\n\n-seed \nnumber\n (default: \n3425\n)\nRandom seed.", 
            "title": "preprocess.lua"
        }, 
        {
            "location": "/options/preprocess/#preprocess-options", 
            "text": "-data_type  string  (accepted:  bitext ,  monotext ,  feattext ; default:  bitext ) Type of data to preprocess. Use 'monotext' for monolingual data. This option impacts all options choices.  -save_data  string  (required) Output file for the prepared data.", 
            "title": "Preprocess options"
        }, 
        {
            "location": "/options/preprocess/#data-options", 
            "text": "-train_src  string  (required) Path to the training source data.  -train_tgt  string  (required) Path to the training target data.  -valid_src  string  (required) Path to the validation source data.  -valid_tgt  string  (required) Path to the validation target data.  -src_vocab  string  (default:  '' ) Path to an existing source vocabulary.  -src_vocab_size  table  (default:  50000 ) List of source vocabularies size:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are not pruned.  -src_words_min_frequency  table  (default:  0 ) List of source words min frequency:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are pruned by size.  -tgt_vocab  string  (default:  '' ) Path to an existing target vocabulary.  -tgt_vocab_size  table  (default:  50000 ) List of target vocabularies size:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are not pruned.  -tgt_words_min_frequency  table  (default:  0 ) List of target words min frequency:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are pruned by size.  -src_seq_length  number  (default:  50 ) Maximum source sequence length.  -tgt_seq_length  number  (default:  50 ) Maximum target sequence length.  -features_vocabs_prefix  string  (default:  '' ) Path prefix to existing features vocabularies.  -time_shift_feature [ boolean ]  (default:  true ) Time shift features on the decoder side.  -keep_frequency [ boolean ]  (default:  false ) Keep frequency of words in dictionary.  -sort [ boolean ]  (default:  true ) If set, sort the sequences by size to build batches without source padding.  -shuffle [ boolean ]  (default:  true ) If set, shuffle the data (prior sorting).  -idx_files [ boolean ]  (default:  false ) If set, source and target files are 'key value' with key match between source and target.  -report_every  number  (default:  100000 ) Report status every this many sentences.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/preprocess/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/preprocess/#other-options", 
            "text": "-seed  number  (default:  3425 ) Random seed.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/train/", 
            "text": "train.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nData options\n\n\n\n\n-data \nstring\n (required)\nPath to the data package \n*-train.t7\n generated by the preprocessing step.\n\n\n\n\nSampled dataset options\n\n\n\n\n-sample \nnumber\n (default: \n0\n)\nNumber of instances to sample from train data in each epoch.\n\n\n-sample_type \nstring\n (accepted: \nuniform\n, \nperplexity\n, \npartition\n; default: \nuniform\n)\nDefine the partition type. \nuniform\n draws randomly the sample, \nperplexity\n uses perplexity as a probability distribution when sampling (with \n-sample_perplexity_init\n and \n-sample_perplexity_max\n options), \npartition\n draws different subsets at each epoch.\n\n\n-sample_perplexity_init \nnumber\n (default: \n15\n)\nStart perplexity-based sampling when average train perplexity per batch falls below this value.\n\n\n-sample_perplexity_max \nnumber\n (default: \n-1.5\n)\nWhen greater than 0, instances with perplexity above this value will be considered as noise and ignored; when less than 0, mode + \n-sample_perplexity_max\n * stdev will be used as threshold.\n\n\n-sample_tgt_vocab [\nboolean\n]\n (default: \nfalse\n)\nUse importance sampling approach as approximation of full softmax: target vocabulary is built using sample.\n\n\n\n\nModel options\n\n\n\n\n-model_type \nstring\n (accepted: \nlm\n, \nseq2seq\n, \nseqtagger\n; default: \nseq2seq\n)\nType of model to train. This option impacts all options choices.\n\n\n-param_init \nnumber\n (default: \n0.1\n)\nParameters are initialized over uniform distribution with support (-\nparam_init\n, \nparam_init\n).\n\n\n\n\nSequence to Sequence with Attention options\n\n\n\n\n-enc_layers \nnumber\n (default: \n0\n)\nIf \n 0, number of layers of the encoder. This overrides the global \n-layers\n option.\n\n\n-dec_layers \nnumber\n (default: \n0\n)\nIf \n 0, number of layers of the decoder. This overrides the global \n-layers\n option.\n\n\n-word_vec_size \nnumber\n (default: \n0\n)\nShared word embedding size. If set, this overrides \n-src_word_vec_size\n and \n-tgt_word_vec_size\n.\n\n\n-src_word_vec_size \ntable\n (default: \n500\n)\nList of source embedding sizes: \nword[ feat1[ feat2[ ...] ] ]\n.\n\n\n-tgt_word_vec_size \ntable\n (default: \n500\n)\nList of target embedding sizes: \nword[ feat1[ feat2[ ...] ] ]\n.\n\n\n-pre_word_vecs_enc \nstring\n (default: \n''\n)\nPath to pretrained word embeddings on the encoder side serialized as a Torch tensor.\n\n\n-pre_word_vecs_dec \nstring\n (default: \n''\n)\nPath to pretrained word embeddings on the decoder side serialized as a Torch tensor.\n\n\n-fix_word_vecs_enc [\nboolean\n/\nstring\n]\n (accepted: \nfalse\n, \ntrue\n, \npretrained\n; default: \nfalse\n)\nFix word embeddings on the encoder side.\n\n\n-fix_word_vecs_dec [\nboolean\n/\nstring\n]\n (accepted: \nfalse\n, \ntrue\n, \npretrained\n; default: \nfalse\n)\nFix word embeddings on the decoder side.\n\n\n-feat_merge \nstring\n (accepted: \nconcat\n, \nsum\n; default: \nconcat\n)\nMerge action for the features embeddings.\n\n\n-feat_vec_exponent \nnumber\n (default: \n0.7\n)\nWhen features embedding sizes are not set and using \n-feat_merge concat\n, their dimension will be set to \nN^feat_vec_exponent\n where \nN\n is the number of values the feature takes.\n\n\n-feat_vec_size \nnumber\n (default: \n20\n)\nWhen features embedding sizes are not set and using \n-feat_merge sum\n, this is the common embedding size of the features\n\n\n-layers \nnumber\n (default: \n2\n)\nNumber of recurrent layers of the encoder and decoder. See also \n-enc_layers\n, \n-dec_layers\n and \n-bridge\n to assign different layers to the encoder and decoder.\n\n\n-rnn_size \nnumber\n (default: \n500\n)\nHidden size of the recurrent unit.\n\n\n-rnn_type \nstring\n (accepted: \nLSTM\n, \nGRU\n; default: \nLSTM\n)\nType of recurrent cell.\n\n\n-dropout \nnumber\n (default: \n0.3\n)\nDropout probability applied between recurrent layers.\n\n\n-dropout_input [\nboolean\n]\n (default: \nfalse\n)\nAlso apply dropout to the input of the recurrent module.\n\n\n-residual [\nboolean\n]\n (default: \nfalse\n)\nAdd residual connections between recurrent layers.\n\n\n-bridge \nstring\n (accepted: \ncopy\n, \ndense\n, \ndense_nonlinear\n, \nnone\n; default: \ncopy\n)\nDefine how to pass encoder states to the decoder. With \ncopy\n, the encoder and decoder must have the same number of layers.\n\n\n-input_feed [\nboolean\n]\n (default: \ntrue\n)\nFeed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.\n\n\n-brnn [\nboolean\n]\n (default: \nfalse\n)\nUse a bidirectional encoder.\n\n\n-dbrnn [\nboolean\n]\n (default: \nfalse\n)\nUse a deep bidirectional encoder.\n\n\n-pdbrnn [\nboolean\n]\n (default: \nfalse\n)\nUse a pyramidal deep bidirectional encoder.\n\n\n-attention \nstring\n (accepted: \nnone\n, \nglobal\n; default: \nglobal\n)\nAttention model.\n\n\n-brnn_merge \nstring\n (accepted: \nconcat\n, \nsum\n; default: \nsum\n)\nMerge action for the bidirectional states.\n\n\n-pdbrnn_reduction \nnumber\n (default: \n2\n)\nTime-reduction factor at each layer.\n\n\n-pdbrnn_merge \nstring\n (accepted: \nconcat\n, \nsum\n; default: \nconcat\n)\nMerge action when reducing time.\n\n\n\n\nGlobal Attention Model options\n\n\n\n\n-global_attention \nstring\n (accepted: \ngeneral\n, \ndot\n, \nconcat\n; default: \ngeneral\n)\nGlobal attention model type.\n\n\n\n\nTrainer options\n\n\n\n\n-save_every \nnumber\n (default: \n5000\n)\nSave intermediate models every this many iterations within an epoch. If = 0, will not save intermediate models.\n\n\n-save_every_epochs \nnumber\n (default: \n1\n)\nSave a model every this many epochs. If = 0, will not save a model at each epoch.\n\n\n-report_every \nnumber\n (default: \n50\n)\nReport progress every this many iterations within an epoch.\n\n\n-async_parallel [\nboolean\n]\n (default: \nfalse\n)\nWhen training on multiple GPUs, update parameters asynchronously.\n\n\n-async_parallel_minbatch \nnumber\n (default: \n1000\n)\nIn asynchronous training, minimal number of sequential batches before being parallel.\n\n\n-start_iteration \nnumber\n (default: \n1\n)\nIf loading from a checkpoint, the iteration from which to start.\n\n\n-start_epoch \nnumber\n (default: \n1\n)\nIf loading from a checkpoint, the epoch from which to start.\n\n\n-end_epoch \nnumber\n (default: \n13\n)\nThe final epoch of the training. If = 0, train forever unless another stopping condition is met (e.g. \n-min_learning_rate\n is reached).\n\n\n-curriculum \nnumber\n (default: \n0\n)\nFor this many epochs, order the minibatches based on source length (from smaller to longer). Sometimes setting this to 1 will increase convergence speed.\n\n\n\n\nOptimization options\n\n\n\n\n-max_batch_size \nnumber\n (default: \n64\n)\nMaximum batch size.\n\n\n-uneven_batches [\nboolean\n]\n (default: \nfalse\n)\nIf set, batches are filled up to \n-max_batch_size\n even if the source lengths are different. Slower but needed for some tasks.\n\n\n-optim \nstring\n (accepted: \nsgd\n, \nadagrad\n, \nadadelta\n, \nadam\n; default: \nsgd\n)\nOptimization method.\n\n\n-learning_rate \nnumber\n (default: \n1\n)\nInitial learning rate. If \nadagrad\n or \nadam\n is used, then this is the global learning rate. Recommended settings are: \nsgd\n = 1, \nadagrad\n = 0.1, \nadam\n = 0.0002.\n\n\n-min_learning_rate \nnumber\n (default: \n0\n)\nDo not continue the training past this learning rate value.\n\n\n-max_grad_norm \nnumber\n (default: \n5\n)\nClip the gradients norm to this value.\n\n\n-learning_rate_decay \nnumber\n (default: \n0.7\n)\nLearning rate decay factor: \nlearning_rate = learning_rate * learning_rate_decay\n.\n\n\n-start_decay_at \nnumber\n (default: \n9\n)\nIn \"default\" decay mode, start decay after this epoch.\n\n\n-start_decay_ppl_delta \nnumber\n (default: \n0\n)\nStart decay when validation perplexity improvement is lower than this value.\n\n\n-decay \nstring\n (accepted: \ndefault\n, \nepoch_only\n, \nperplexity_only\n; default: \ndefault\n)\nWhen to apply learning rate decay. \ndefault\n: decay after each epoch past \n-start_decay_at\n or as soon as the validation perplexity is not improving more than \n-start_decay_ppl_delta\n, \nepoch_only\n: only decay after each epoch past \n-start_decay_at\n, \nperplexity_only\n: only decay when validation perplexity is not improving more than \n-start_decay_ppl_delta\n.\n\n\n\n\nSaver options\n\n\n\n\n-save_model \nstring\n (required)\nModel filename (the model will be saved as \nsave_model\n_epochN_PPL.t7\n where \nPPL\n is the validation perplexity.\n\n\n-train_from \nstring\n (default: \n''\n)\nPath to a checkpoint.\n\n\n-continue [\nboolean\n]\n (default: \nfalse\n)\nIf set, continue the training where it left off.\n\n\n\n\nCrayon options\n\n\n\n\n-exp_host \nstring\n (default: \n127.0.0.1\n)\nCrayon server IP.\n\n\n-exp_port \nstring\n (default: \n8889\n)\nCrayon server port.\n\n\n-exp \nstring\n (default: \n''\n)\nCrayon experiment name.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nOther options\n\n\n\n\n-disable_mem_optimization [\nboolean\n]\n (default: \nfalse\n)\nDisable sharing of internal buffers between clones for visualization or development.\n\n\n-profiler [\nboolean\n]\n (default: \nfalse\n)\nGenerate profiling logs.\n\n\n-seed \nnumber\n (default: \n3435\n)\nRandom seed.", 
            "title": "train.lua"
        }, 
        {
            "location": "/options/train/#data-options", 
            "text": "-data  string  (required) Path to the data package  *-train.t7  generated by the preprocessing step.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/train/#sampled-dataset-options", 
            "text": "-sample  number  (default:  0 ) Number of instances to sample from train data in each epoch.  -sample_type  string  (accepted:  uniform ,  perplexity ,  partition ; default:  uniform ) Define the partition type.  uniform  draws randomly the sample,  perplexity  uses perplexity as a probability distribution when sampling (with  -sample_perplexity_init  and  -sample_perplexity_max  options),  partition  draws different subsets at each epoch.  -sample_perplexity_init  number  (default:  15 ) Start perplexity-based sampling when average train perplexity per batch falls below this value.  -sample_perplexity_max  number  (default:  -1.5 ) When greater than 0, instances with perplexity above this value will be considered as noise and ignored; when less than 0, mode +  -sample_perplexity_max  * stdev will be used as threshold.  -sample_tgt_vocab [ boolean ]  (default:  false ) Use importance sampling approach as approximation of full softmax: target vocabulary is built using sample.", 
            "title": "Sampled dataset options"
        }, 
        {
            "location": "/options/train/#model-options", 
            "text": "-model_type  string  (accepted:  lm ,  seq2seq ,  seqtagger ; default:  seq2seq ) Type of model to train. This option impacts all options choices.  -param_init  number  (default:  0.1 ) Parameters are initialized over uniform distribution with support (- param_init ,  param_init ).", 
            "title": "Model options"
        }, 
        {
            "location": "/options/train/#sequence-to-sequence-with-attention-options", 
            "text": "-enc_layers  number  (default:  0 ) If   0, number of layers of the encoder. This overrides the global  -layers  option.  -dec_layers  number  (default:  0 ) If   0, number of layers of the decoder. This overrides the global  -layers  option.  -word_vec_size  number  (default:  0 ) Shared word embedding size. If set, this overrides  -src_word_vec_size  and  -tgt_word_vec_size .  -src_word_vec_size  table  (default:  500 ) List of source embedding sizes:  word[ feat1[ feat2[ ...] ] ] .  -tgt_word_vec_size  table  (default:  500 ) List of target embedding sizes:  word[ feat1[ feat2[ ...] ] ] .  -pre_word_vecs_enc  string  (default:  '' ) Path to pretrained word embeddings on the encoder side serialized as a Torch tensor.  -pre_word_vecs_dec  string  (default:  '' ) Path to pretrained word embeddings on the decoder side serialized as a Torch tensor.  -fix_word_vecs_enc [ boolean / string ]  (accepted:  false ,  true ,  pretrained ; default:  false ) Fix word embeddings on the encoder side.  -fix_word_vecs_dec [ boolean / string ]  (accepted:  false ,  true ,  pretrained ; default:  false ) Fix word embeddings on the decoder side.  -feat_merge  string  (accepted:  concat ,  sum ; default:  concat ) Merge action for the features embeddings.  -feat_vec_exponent  number  (default:  0.7 ) When features embedding sizes are not set and using  -feat_merge concat , their dimension will be set to  N^feat_vec_exponent  where  N  is the number of values the feature takes.  -feat_vec_size  number  (default:  20 ) When features embedding sizes are not set and using  -feat_merge sum , this is the common embedding size of the features  -layers  number  (default:  2 ) Number of recurrent layers of the encoder and decoder. See also  -enc_layers ,  -dec_layers  and  -bridge  to assign different layers to the encoder and decoder.  -rnn_size  number  (default:  500 ) Hidden size of the recurrent unit.  -rnn_type  string  (accepted:  LSTM ,  GRU ; default:  LSTM ) Type of recurrent cell.  -dropout  number  (default:  0.3 ) Dropout probability applied between recurrent layers.  -dropout_input [ boolean ]  (default:  false ) Also apply dropout to the input of the recurrent module.  -residual [ boolean ]  (default:  false ) Add residual connections between recurrent layers.  -bridge  string  (accepted:  copy ,  dense ,  dense_nonlinear ,  none ; default:  copy ) Define how to pass encoder states to the decoder. With  copy , the encoder and decoder must have the same number of layers.  -input_feed [ boolean ]  (default:  true ) Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.  -brnn [ boolean ]  (default:  false ) Use a bidirectional encoder.  -dbrnn [ boolean ]  (default:  false ) Use a deep bidirectional encoder.  -pdbrnn [ boolean ]  (default:  false ) Use a pyramidal deep bidirectional encoder.  -attention  string  (accepted:  none ,  global ; default:  global ) Attention model.  -brnn_merge  string  (accepted:  concat ,  sum ; default:  sum ) Merge action for the bidirectional states.  -pdbrnn_reduction  number  (default:  2 ) Time-reduction factor at each layer.  -pdbrnn_merge  string  (accepted:  concat ,  sum ; default:  concat ) Merge action when reducing time.", 
            "title": "Sequence to Sequence with Attention options"
        }, 
        {
            "location": "/options/train/#global-attention-model-options", 
            "text": "-global_attention  string  (accepted:  general ,  dot ,  concat ; default:  general ) Global attention model type.", 
            "title": "Global Attention Model options"
        }, 
        {
            "location": "/options/train/#trainer-options", 
            "text": "-save_every  number  (default:  5000 ) Save intermediate models every this many iterations within an epoch. If = 0, will not save intermediate models.  -save_every_epochs  number  (default:  1 ) Save a model every this many epochs. If = 0, will not save a model at each epoch.  -report_every  number  (default:  50 ) Report progress every this many iterations within an epoch.  -async_parallel [ boolean ]  (default:  false ) When training on multiple GPUs, update parameters asynchronously.  -async_parallel_minbatch  number  (default:  1000 ) In asynchronous training, minimal number of sequential batches before being parallel.  -start_iteration  number  (default:  1 ) If loading from a checkpoint, the iteration from which to start.  -start_epoch  number  (default:  1 ) If loading from a checkpoint, the epoch from which to start.  -end_epoch  number  (default:  13 ) The final epoch of the training. If = 0, train forever unless another stopping condition is met (e.g.  -min_learning_rate  is reached).  -curriculum  number  (default:  0 ) For this many epochs, order the minibatches based on source length (from smaller to longer). Sometimes setting this to 1 will increase convergence speed.", 
            "title": "Trainer options"
        }, 
        {
            "location": "/options/train/#optimization-options", 
            "text": "-max_batch_size  number  (default:  64 ) Maximum batch size.  -uneven_batches [ boolean ]  (default:  false ) If set, batches are filled up to  -max_batch_size  even if the source lengths are different. Slower but needed for some tasks.  -optim  string  (accepted:  sgd ,  adagrad ,  adadelta ,  adam ; default:  sgd ) Optimization method.  -learning_rate  number  (default:  1 ) Initial learning rate. If  adagrad  or  adam  is used, then this is the global learning rate. Recommended settings are:  sgd  = 1,  adagrad  = 0.1,  adam  = 0.0002.  -min_learning_rate  number  (default:  0 ) Do not continue the training past this learning rate value.  -max_grad_norm  number  (default:  5 ) Clip the gradients norm to this value.  -learning_rate_decay  number  (default:  0.7 ) Learning rate decay factor:  learning_rate = learning_rate * learning_rate_decay .  -start_decay_at  number  (default:  9 ) In \"default\" decay mode, start decay after this epoch.  -start_decay_ppl_delta  number  (default:  0 ) Start decay when validation perplexity improvement is lower than this value.  -decay  string  (accepted:  default ,  epoch_only ,  perplexity_only ; default:  default ) When to apply learning rate decay.  default : decay after each epoch past  -start_decay_at  or as soon as the validation perplexity is not improving more than  -start_decay_ppl_delta ,  epoch_only : only decay after each epoch past  -start_decay_at ,  perplexity_only : only decay when validation perplexity is not improving more than  -start_decay_ppl_delta .", 
            "title": "Optimization options"
        }, 
        {
            "location": "/options/train/#saver-options", 
            "text": "-save_model  string  (required) Model filename (the model will be saved as  save_model _epochN_PPL.t7  where  PPL  is the validation perplexity.  -train_from  string  (default:  '' ) Path to a checkpoint.  -continue [ boolean ]  (default:  false ) If set, continue the training where it left off.", 
            "title": "Saver options"
        }, 
        {
            "location": "/options/train/#crayon-options", 
            "text": "-exp_host  string  (default:  127.0.0.1 ) Crayon server IP.  -exp_port  string  (default:  8889 ) Crayon server port.  -exp  string  (default:  '' ) Crayon experiment name.", 
            "title": "Crayon options"
        }, 
        {
            "location": "/options/train/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/train/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/train/#other-options", 
            "text": "-disable_mem_optimization [ boolean ]  (default:  false ) Disable sharing of internal buffers between clones for visualization or development.  -profiler [ boolean ]  (default:  false ) Generate profiling logs.  -seed  number  (default:  3435 ) Random seed.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/translate/", 
            "text": "translate.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nData options\n\n\n\n\n-src \nstring\n (required)\nSource sequences to translate.\n\n\n-tgt \nstring\n (default: \n''\n)\nOptional true target sequences.\n\n\n-output \nstring\n (default: \npred.txt\n)\nOutput file.\n\n\n-idx_files [\nboolean\n]\n (default: \nfalse\n)\nIf set, source and target files are 'key value' with key match between source and target.\n\n\n\n\nTranslator options\n\n\n\n\n-model \nstring\n (required)\nPath to the serialized model file.\n\n\n-beam_size \nnumber\n (default: \n5\n)\nBeam size.\n\n\n-batch_size \nnumber\n (default: \n30\n)\nBatch size.\n\n\n-max_sent_length \nnumber\n (default: \n250\n)\nMaximum output sentence length.\n\n\n-replace_unk [\nboolean\n]\n (default: \nfalse\n)\nReplace the generated \n tokens with the source token that has the highest attention weight. If \n-phrase_table\n is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token\n\n\n-phrase_table \nstring\n (default: \n''\n)\nPath to source-target dictionary to replace \nunk\n tokens.\n\n\n-n_best \nnumber\n (default: \n1\n)\nIf \n 1, it will also output an n-best list of decoded sentences.\n\n\n-max_num_unks \nnumber\n (default: \ninf\n)\nAll sequences with more \nunk\ns than this will be ignored during beam search.\n\n\n-target_subdict \nstring\n (default: \n''\n)\nPath to target words dictionary corresponding to the source.\n\n\n-pre_filter_factor \nnumber\n (default: \n1\n)\nOptional, set this only if filter is being used. Before applying filters, hypotheses with top \nbeam_size * pre_filter_factor\n scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.\n\n\n-length_norm \nnumber\n (default: \n0\n)\nLength normalization coefficient (alpha). If set to 0, no length normalization.\n\n\n-coverage_norm \nnumber\n (default: \n0\n)\nCoverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.\n\n\n-eos_norm \nnumber\n (default: \n0\n)\nEnd of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.\n\n\n-dump_input_encoding [\nboolean\n]\n (default: \nfalse\n)\nInstead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nOther options\n\n\n\n\n-time [\nboolean\n]\n (default: \nfalse\n)\nMeasure average translation time.", 
            "title": "translate.lua"
        }, 
        {
            "location": "/options/translate/#data-options", 
            "text": "-src  string  (required) Source sequences to translate.  -tgt  string  (default:  '' ) Optional true target sequences.  -output  string  (default:  pred.txt ) Output file.  -idx_files [ boolean ]  (default:  false ) If set, source and target files are 'key value' with key match between source and target.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/translate/#translator-options", 
            "text": "-model  string  (required) Path to the serialized model file.  -beam_size  number  (default:  5 ) Beam size.  -batch_size  number  (default:  30 ) Batch size.  -max_sent_length  number  (default:  250 ) Maximum output sentence length.  -replace_unk [ boolean ]  (default:  false ) Replace the generated   tokens with the source token that has the highest attention weight. If  -phrase_table  is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token  -phrase_table  string  (default:  '' ) Path to source-target dictionary to replace  unk  tokens.  -n_best  number  (default:  1 ) If   1, it will also output an n-best list of decoded sentences.  -max_num_unks  number  (default:  inf ) All sequences with more  unk s than this will be ignored during beam search.  -target_subdict  string  (default:  '' ) Path to target words dictionary corresponding to the source.  -pre_filter_factor  number  (default:  1 ) Optional, set this only if filter is being used. Before applying filters, hypotheses with top  beam_size * pre_filter_factor  scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.  -length_norm  number  (default:  0 ) Length normalization coefficient (alpha). If set to 0, no length normalization.  -coverage_norm  number  (default:  0 ) Coverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.  -eos_norm  number  (default:  0 ) End of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.  -dump_input_encoding [ boolean ]  (default:  false ) Instead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.", 
            "title": "Translator options"
        }, 
        {
            "location": "/options/translate/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/translate/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/translate/#other-options", 
            "text": "-time [ boolean ]  (default:  false ) Measure average translation time.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/tag/", 
            "text": "tag.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nData options\n\n\n\n\n-src \nstring\n (required)\nSource sequences to tag.\n\n\n-output \nstring\n (default: \npred.txt\n)\nOutput file.\n\n\n\n\nTagger options\n\n\n\n\n-model \nstring\n (required)\nPath to the serialized model file.\n\n\n-batch_size \nnumber\n (default: \n30\n)\nBatch size.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nOther options\n\n\n\n\n-time [\nboolean\n]\n (default: \nfalse\n)\nMeasure average translation time.", 
            "title": "tag.lua"
        }, 
        {
            "location": "/options/tag/#data-options", 
            "text": "-src  string  (required) Source sequences to tag.  -output  string  (default:  pred.txt ) Output file.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/tag/#tagger-options", 
            "text": "-model  string  (required) Path to the serialized model file.  -batch_size  number  (default:  30 ) Batch size.", 
            "title": "Tagger options"
        }, 
        {
            "location": "/options/tag/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/tag/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/tag/#other-options", 
            "text": "-time [ boolean ]  (default:  false ) Measure average translation time.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/build_vocab/", 
            "text": "build_vocab.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nVocabulary options\n\n\n\n\n-data \nstring\n (required)\nData file.\n\n\n-save_vocab \nstring\n (required)\nVocabulary dictionary prefix.\n\n\n-vocab_size \ntable\n (default: \n50000\n)\nList of source vocabularies size: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are not pruned.\n\n\n-words_min_frequency \ntable\n (default: \n0\n)\nList of source words min frequency: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are pruned by size.\n\n\n-keep_frequency [\nboolean\n]\n (default: \nfalse\n)\nKeep frequency of words in dictionary.\n\n\n-idx_files [\nboolean\n]\n (default: \nfalse\n)\nIf set, each line of the data file starts with a first field which is the index of the sentence.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/build_vocab.lua"
        }, 
        {
            "location": "/options/build_vocab/#vocabulary-options", 
            "text": "-data  string  (required) Data file.  -save_vocab  string  (required) Vocabulary dictionary prefix.  -vocab_size  table  (default:  50000 ) List of source vocabularies size:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are not pruned.  -words_min_frequency  table  (default:  0 ) List of source words min frequency:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are pruned by size.  -keep_frequency [ boolean ]  (default:  false ) Keep frequency of words in dictionary.  -idx_files [ boolean ]  (default:  false ) If set, each line of the data file starts with a first field which is the index of the sentence.", 
            "title": "Vocabulary options"
        }, 
        {
            "location": "/options/build_vocab/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/release_model/", 
            "text": "release_model.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nModel options\n\n\n\n\n-model \nstring\n (required)\nPath to the trained model to release.\n\n\n-output_model \nstring\n (default: \n''\n)\nPath the released model. If not set, the \nrelease\n suffix will be automatically added to the model filename.\n\n\n-force [\nboolean\n]\n (default: \nfalse\n)\nForce output model creation even if the target file exists.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/release_model.lua"
        }, 
        {
            "location": "/options/release_model/#model-options", 
            "text": "-model  string  (required) Path to the trained model to release.  -output_model  string  (default:  '' ) Path the released model. If not set, the  release  suffix will be automatically added to the model filename.  -force [ boolean ]  (default:  false ) Force output model creation even if the target file exists.", 
            "title": "Model options"
        }, 
        {
            "location": "/options/release_model/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/release_model/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/tokenize/", 
            "text": "tokenize.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nTokenizer options\n\n\n\n\n-mode \nstring\n (accepted: \nconservative\n, \naggressive\n; default: \nconservative\n)\nDefine how aggressive should the tokenization be. \naggressive\n only keeps sequences of letters/numbers, \nconservative\n allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc.\n\n\n-joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, \n-mode\n will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the end of token.\n\n\n-BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the beginning of token.\n\n\n-bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n\n\nOther options\n\n\n\n\n-nparallel \nnumber\n (default: \n1\n)\nNumber of parallel thread to run the tokenization\n\n\n-batchsize \nnumber\n (default: \n1000\n)\nSize of each parallel batch - you should not change except if low memory", 
            "title": "tools/tokenize.lua"
        }, 
        {
            "location": "/options/tokenize/#tokenizer-options", 
            "text": "-mode  string  (accepted:  conservative ,  aggressive ; default:  conservative ) Define how aggressive should the tokenization be.  aggressive  only keeps sequences of letters/numbers,  conservative  allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc.  -joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -joiner  string  (default:  \uffed ) Character used to annotate joiners.  -joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -case_feature [ boolean ]  (default:  false ) Generate case feature.  -segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used,  -mode  will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -EOT_marker  string  (default:  /w ) Marker used to mark the end of token.  -BOT_marker  string  (default:  w ) Marker used to mark the beginning of token.  -bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .", 
            "title": "Tokenizer options"
        }, 
        {
            "location": "/options/tokenize/#other-options", 
            "text": "-nparallel  number  (default:  1 ) Number of parallel thread to run the tokenization  -batchsize  number  (default:  1000 ) Size of each parallel batch - you should not change except if low memory", 
            "title": "Other options"
        }, 
        {
            "location": "/options/learn_bpe/", 
            "text": "learn_bpe.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nBPE options\n\n\n\n\n-size \nstring\n (default: \n30000\n)\nThe number of merge operations to learn.\n\n\n-t [\nboolean\n]\n (default: \nfalse\n)\nTokenize the input with tokenizer, the same options as tokenize.lua, but only \n-mode\n is taken into account for BPE training.\n\n\n-mode \nstring\n (accepted: \nconservative\n, \naggressive\n; default: \nconservative\n)\nDefine how aggressive should the tokenization be. \naggressive\n only keeps sequences of letters/numbers, \nconservative\n allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc.\n\n\n-segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-lc [\nboolean\n]\n (default: \nfalse\n)\nLowercase the output from the tokenizer before learning BPE.\n\n\n-bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. \nprefix\n: append \nw\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n/w\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n-save_bpe \nstring\n (default: \n''\n)\nPath to save the output model.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/learn_bpe.lua"
        }, 
        {
            "location": "/options/learn_bpe/#bpe-options", 
            "text": "-size  string  (default:  30000 ) The number of merge operations to learn.  -t [ boolean ]  (default:  false ) Tokenize the input with tokenizer, the same options as tokenize.lua, but only  -mode  is taken into account for BPE training.  -mode  string  (accepted:  conservative ,  aggressive ; default:  conservative ) Define how aggressive should the tokenization be.  aggressive  only keeps sequences of letters/numbers,  conservative  allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc.  -segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -lc [ boolean ]  (default:  false ) Lowercase the output from the tokenizer before learning BPE.  -bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode.  prefix : append  w  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  /w  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .  -save_bpe  string  (default:  '' ) Path to save the output model.", 
            "title": "BPE options"
        }, 
        {
            "location": "/options/learn_bpe/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/server/", 
            "text": "translation_server.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nServer options\n\n\n\n\n-host \nstring\n (default: \n127.0.0.1\n)\nHost to run the server on.\n\n\n-port \nstring\n (default: \n5556\n)\nPort to run the server on.\n\n\n\n\nTranslator options\n\n\n\n\n-model \nstring\n (required)\nPath to the serialized model file.\n\n\n-beam_size \nnumber\n (default: \n5\n)\nBeam size.\n\n\n-batch_size \nnumber\n (default: \n30\n)\nBatch size.\n\n\n-max_sent_length \nnumber\n (default: \n250\n)\nMaximum output sentence length.\n\n\n-replace_unk [\nboolean\n]\n (default: \nfalse\n)\nReplace the generated \n tokens with the source token that has the highest attention weight. If \n-phrase_table\n is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token\n\n\n-phrase_table \nstring\n (default: \n''\n)\nPath to source-target dictionary to replace \nunk\n tokens.\n\n\n-n_best \nnumber\n (default: \n1\n)\nIf \n 1, it will also output an n-best list of decoded sentences.\n\n\n-max_num_unks \nnumber\n (default: \ninf\n)\nAll sequences with more \nunk\ns than this will be ignored during beam search.\n\n\n-target_subdict \nstring\n (default: \n''\n)\nPath to target words dictionary corresponding to the source.\n\n\n-pre_filter_factor \nnumber\n (default: \n1\n)\nOptional, set this only if filter is being used. Before applying filters, hypotheses with top \nbeam_size * pre_filter_factor\n scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.\n\n\n-length_norm \nnumber\n (default: \n0\n)\nLength normalization coefficient (alpha). If set to 0, no length normalization.\n\n\n-coverage_norm \nnumber\n (default: \n0\n)\nCoverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.\n\n\n-eos_norm \nnumber\n (default: \n0\n)\nEnd of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.\n\n\n-dump_input_encoding [\nboolean\n]\n (default: \nfalse\n)\nInstead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/translation_server.lua"
        }, 
        {
            "location": "/options/server/#server-options", 
            "text": "-host  string  (default:  127.0.0.1 ) Host to run the server on.  -port  string  (default:  5556 ) Port to run the server on.", 
            "title": "Server options"
        }, 
        {
            "location": "/options/server/#translator-options", 
            "text": "-model  string  (required) Path to the serialized model file.  -beam_size  number  (default:  5 ) Beam size.  -batch_size  number  (default:  30 ) Batch size.  -max_sent_length  number  (default:  250 ) Maximum output sentence length.  -replace_unk [ boolean ]  (default:  false ) Replace the generated   tokens with the source token that has the highest attention weight. If  -phrase_table  is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token  -phrase_table  string  (default:  '' ) Path to source-target dictionary to replace  unk  tokens.  -n_best  number  (default:  1 ) If   1, it will also output an n-best list of decoded sentences.  -max_num_unks  number  (default:  inf ) All sequences with more  unk s than this will be ignored during beam search.  -target_subdict  string  (default:  '' ) Path to target words dictionary corresponding to the source.  -pre_filter_factor  number  (default:  1 ) Optional, set this only if filter is being used. Before applying filters, hypotheses with top  beam_size * pre_filter_factor  scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.  -length_norm  number  (default:  0 ) Length normalization coefficient (alpha). If set to 0, no length normalization.  -coverage_norm  number  (default:  0 ) Coverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.  -eos_norm  number  (default:  0 ) End of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.  -dump_input_encoding [ boolean ]  (default:  false ) Instead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.", 
            "title": "Translator options"
        }, 
        {
            "location": "/options/server/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/server/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/rest_server/", 
            "text": "rest_translation_server.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nServer options\n\n\n\n\n-port \nstring\n (default: \n7784\n)\nPort to run the server on.\n\n\n-withAttn [\nboolean\n]\n (default: \nfalse\n)\nIf set returns by default attn vector.\n\n\n\n\nTranslator options\n\n\n\n\n-model \nstring\n (required)\nPath to the serialized model file.\n\n\n-beam_size \nnumber\n (default: \n5\n)\nBeam size.\n\n\n-batch_size \nnumber\n (default: \n30\n)\nBatch size.\n\n\n-max_sent_length \nnumber\n (default: \n250\n)\nMaximum output sentence length.\n\n\n-replace_unk [\nboolean\n]\n (default: \nfalse\n)\nReplace the generated \n tokens with the source token that has the highest attention weight. If \n-phrase_table\n is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token\n\n\n-phrase_table \nstring\n (default: \n''\n)\nPath to source-target dictionary to replace \nunk\n tokens.\n\n\n-n_best \nnumber\n (default: \n1\n)\nIf \n 1, it will also output an n-best list of decoded sentences.\n\n\n-max_num_unks \nnumber\n (default: \ninf\n)\nAll sequences with more \nunk\ns than this will be ignored during beam search.\n\n\n-target_subdict \nstring\n (default: \n''\n)\nPath to target words dictionary corresponding to the source.\n\n\n-pre_filter_factor \nnumber\n (default: \n1\n)\nOptional, set this only if filter is being used. Before applying filters, hypotheses with top \nbeam_size * pre_filter_factor\n scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.\n\n\n-length_norm \nnumber\n (default: \n0\n)\nLength normalization coefficient (alpha). If set to 0, no length normalization.\n\n\n-coverage_norm \nnumber\n (default: \n0\n)\nCoverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.\n\n\n-eos_norm \nnumber\n (default: \n0\n)\nEnd of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.\n\n\n-dump_input_encoding [\nboolean\n]\n (default: \nfalse\n)\nInstead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nTokenizer options\n\n\n\n\n-mode \nstring\n (accepted: \nconservative\n, \naggressive\n; default: \nconservative\n)\nDefine how aggressive should the tokenization be. \naggressive\n only keeps sequences of letters/numbers, \nconservative\n allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc.\n\n\n-joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, \n-mode\n will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the end of token.\n\n\n-BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the beginning of token.\n\n\n-bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n\n\nOther options\n\n\n\n\n-batchsize \nnumber\n (default: \n1000\n)\nSize of each parallel batch - you should not change except if low memory.", 
            "title": "tools/rest_translation_server.lua"
        }, 
        {
            "location": "/options/rest_server/#server-options", 
            "text": "-port  string  (default:  7784 ) Port to run the server on.  -withAttn [ boolean ]  (default:  false ) If set returns by default attn vector.", 
            "title": "Server options"
        }, 
        {
            "location": "/options/rest_server/#translator-options", 
            "text": "-model  string  (required) Path to the serialized model file.  -beam_size  number  (default:  5 ) Beam size.  -batch_size  number  (default:  30 ) Batch size.  -max_sent_length  number  (default:  250 ) Maximum output sentence length.  -replace_unk [ boolean ]  (default:  false ) Replace the generated   tokens with the source token that has the highest attention weight. If  -phrase_table  is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token  -phrase_table  string  (default:  '' ) Path to source-target dictionary to replace  unk  tokens.  -n_best  number  (default:  1 ) If   1, it will also output an n-best list of decoded sentences.  -max_num_unks  number  (default:  inf ) All sequences with more  unk s than this will be ignored during beam search.  -target_subdict  string  (default:  '' ) Path to target words dictionary corresponding to the source.  -pre_filter_factor  number  (default:  1 ) Optional, set this only if filter is being used. Before applying filters, hypotheses with top  beam_size * pre_filter_factor  scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.  -length_norm  number  (default:  0 ) Length normalization coefficient (alpha). If set to 0, no length normalization.  -coverage_norm  number  (default:  0 ) Coverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.  -eos_norm  number  (default:  0 ) End of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.  -dump_input_encoding [ boolean ]  (default:  false ) Instead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.", 
            "title": "Translator options"
        }, 
        {
            "location": "/options/rest_server/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/rest_server/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/rest_server/#tokenizer-options", 
            "text": "-mode  string  (accepted:  conservative ,  aggressive ; default:  conservative ) Define how aggressive should the tokenization be.  aggressive  only keeps sequences of letters/numbers,  conservative  allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc.  -joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -joiner  string  (default:  \uffed ) Character used to annotate joiners.  -joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -case_feature [ boolean ]  (default:  false ) Generate case feature.  -segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used,  -mode  will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -EOT_marker  string  (default:  /w ) Marker used to mark the end of token.  -BOT_marker  string  (default:  w ) Marker used to mark the beginning of token.  -bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .", 
            "title": "Tokenizer options"
        }, 
        {
            "location": "/options/rest_server/#other-options", 
            "text": "-batchsize  number  (default:  1000 ) Size of each parallel batch - you should not change except if low memory.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/embeddings/", 
            "text": "embeddings.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nData options\n\n\n\n\n-dict_file \nstring\n (required)\nPath to outputted dict file from \npreprocess.lua\n.\n\n\n-embed_file \nstring\n (default: \n''\n)\nPath to the embedding file. Ignored if \n-lang\n is used.\n\n\n-save_data \nstring\n (required)\nOutput file path/label.\n\n\n-save_unknown_dict \nstring\n (default: \n''\n)\nPath to file for saving vocabs not found in embedding.\n\n\n\n\nEmbedding options\n\n\n\n\n-lang \nstring\n (default: \n''\n)\nWikipedia Language Code to autoload embeddings.\n\n\n-embed_type \nstring\n (accepted: \nword2vec-bin\n, \nword2vec-txt\n, \nglove\n; default: \nword2vec-bin\n)\nEmbeddings file origin. Ignored if \n-lang\n is used.\n\n\n-normalize [\nboolean\n]\n (default: \ntrue\n)\nBoolean to normalize the word vectors, or not.\n\n\n-approximate [\nboolean\n]\n (default: \nfalse\n)\nIf set, will also look for variants (case, joiner annotate) to match dictionary and word embedding.\n\n\n-report_every \nnumber\n (default: \n100000\n)\nPrint stats every this many lines read from embedding file.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/embeddings.lua"
        }, 
        {
            "location": "/options/embeddings/#data-options", 
            "text": "-dict_file  string  (required) Path to outputted dict file from  preprocess.lua .  -embed_file  string  (default:  '' ) Path to the embedding file. Ignored if  -lang  is used.  -save_data  string  (required) Output file path/label.  -save_unknown_dict  string  (default:  '' ) Path to file for saving vocabs not found in embedding.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/embeddings/#embedding-options", 
            "text": "-lang  string  (default:  '' ) Wikipedia Language Code to autoload embeddings.  -embed_type  string  (accepted:  word2vec-bin ,  word2vec-txt ,  glove ; default:  word2vec-bin ) Embeddings file origin. Ignored if  -lang  is used.  -normalize [ boolean ]  (default:  true ) Boolean to normalize the word vectors, or not.  -approximate [ boolean ]  (default:  false ) If set, will also look for variants (case, joiner annotate) to match dictionary and word embedding.  -report_every  number  (default:  100000 ) Print stats every this many lines read from embedding file.", 
            "title": "Embedding options"
        }, 
        {
            "location": "/options/embeddings/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/extensions/", 
            "text": "OpenNMT is explicitly separated out into a library and application section. All modeling and training code can be directly used within other Torch applications.\n\n\nImage-to-Text\n\n\nAs an example use case we have released an extension for translating from images-to-text. This model replaces the source-side word embeddings with a convolutional image network. The full model is\navailable at \nOpenNMT/im2text\n.", 
            "title": "Extensions"
        }, 
        {
            "location": "/extensions/#image-to-text", 
            "text": "As an example use case we have released an extension for translating from images-to-text. This model replaces the source-side word embeddings with a convolutional image network. The full model is\navailable at  OpenNMT/im2text .", 
            "title": "Image-to-Text"
        }, 
        {
            "location": "/references/", 
            "text": "This is the list of papers, OpenNMT has been inspired on:\n\n\n\n\nLuong, M. T., Pham, H., \n Manning, C. D. (2015). \nEffective approaches to attention-based neural machine translation\n. arXiv preprint arXiv:1508.04025.\n\n\nSennrich, R., \n Haddow, B. (2016). \nLinguistic input features improve neural machine translation\n. arXiv preprint arXiv:1606.02892.\n\n\nSennrich, R., Haddow, B., \n Birch, A. (2015). \nNeural machine translation of rare words with subword units\n. arXiv preprint arXiv:1508.07909.\n\n\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... \n Klingner, J. (2016). \nGoogle's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n. arXiv preprint arXiv:1609.08144.\n\n\nJean, S., Cho, K., Memisevic, R., Bengio, Y. (2015). \nOn Using Very Large Target Vocabulary for Neural Machine Translation\n. ACL 2015", 
            "title": "References"
        }, 
        {
            "location": "/issues/", 
            "text": "luajit: out of memory\n\n\nThis most likely happened when training a model with long sequences and the LuaJIT memory limit was reached. You will need to switch to Lua 5.2 instead.\n\n\nTHCudaCheck FAIL [...]: out of memory\n\n\nThis means your model was too large to fit on the available GPU memory.\n\n\nTo work around this error during training, follow these steps in order and stop when the training no more fails:\n\n\n\n\nPrefix your command line with \nTHC_CACHING_ALLOCATOR=0\n\n\nReduce the \n-max_batch_size\n value (64 by default)\n\n\nReduce the \n-src_seq_length\n and \n-tgt_seq_length\n values during the preprocessing\n\n\nReduce your model size (\n-layers\n, \n-rnn_size\n, etc.)\n\n\n\n\nunknown Torch class \ntorch.CudaTensor>\n\n\nThis means you wanted to load a GPU model but did not use the \n-gpuid\n option to define which GPU to use.", 
            "title": "Common issues"
        }, 
        {
            "location": "/issues/#luajit-out-of-memory", 
            "text": "This most likely happened when training a model with long sequences and the LuaJIT memory limit was reached. You will need to switch to Lua 5.2 instead.", 
            "title": "luajit: out of memory"
        }, 
        {
            "location": "/issues/#thcudacheck-fail-out-of-memory", 
            "text": "This means your model was too large to fit on the available GPU memory.  To work around this error during training, follow these steps in order and stop when the training no more fails:   Prefix your command line with  THC_CACHING_ALLOCATOR=0  Reduce the  -max_batch_size  value (64 by default)  Reduce the  -src_seq_length  and  -tgt_seq_length  values during the preprocessing  Reduce your model size ( -layers ,  -rnn_size , etc.)", 
            "title": "THCudaCheck FAIL [...]: out of memory"
        }, 
        {
            "location": "/issues/#unknown-torch-class-torchcudatensor62", 
            "text": "This means you wanted to load a GPU model but did not use the  -gpuid  option to define which GPU to use.", 
            "title": "unknown Torch class &lt;torch.CudaTensor>"
        }
    ]
}